[
  {
    "objectID": "1-scraping_linear_reg.html",
    "href": "1-scraping_linear_reg.html",
    "title": "Data Scraping and Linear Regression",
    "section": "",
    "text": "For topic 1, we will cover linear regression. But before diving into that topic, we will talk about how to scrape data from the web.\nLibraries:\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(polite)"
  },
  {
    "objectID": "1-scraping_linear_reg.html#ethics",
    "href": "1-scraping_linear_reg.html#ethics",
    "title": "Data Scraping and Linear Regression",
    "section": "Ethics",
    "text": "Ethics\nData scraping is the process of extracting data from websites. This can be done manually, but it is often done using a program. In this section, we will use the rvest package to scrape data from a website.\nWhen scraping car prices from sellers’ websites in Brunei, it’s important to consider legal and ethical aspects:\n\nLegal Considerations: If the data is public, non-personal, and factual, scraping is generally acceptable. However, laws vary by location. If the data is behind a login or used for commercial purposes, consulting a lawyer is advisable.\nTerms of Service: Many websites prohibit scraping in their terms of service. In some regions, like the US, these terms may not be binding unless you explicitly agree to them (e.g., by creating an account). In Europe, these terms are often enforceable even without explicit consent.\nPersonally Identifiable Information: Avoid scraping data that includes personal information (names, contact details, etc.) due to strict privacy laws like the GDPR in Europe. Ethical concerns arise even if the data is public.\nCopyright: Data like car prices is generally not protected by copyright, as it’s factual. However, if scraping includes original content (like descriptions or images), consider copyright laws and whether “fair use” applies."
  },
  {
    "objectID": "1-scraping_linear_reg.html#html-basics",
    "href": "1-scraping_linear_reg.html#html-basics",
    "title": "Data Scraping and Linear Regression",
    "section": "HTML basics",
    "text": "HTML basics\nHTML stands for “HyperText Markup Language” and looks like this:\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Page title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1 id='first'&gt;A heading&lt;/h1&gt;\n  &lt;p&gt;Some text &amp; &lt;b&gt;some bold text.&lt;/b&gt;&lt;/p&gt;\n  &lt;img src='myimg.png' width='100' height='100'&gt;\n&lt;/body&gt;\nHTML has a hierarchical structure formed by elements which consist of a start tag (e.g. &lt;tag&gt;), optional attributes (id='first'), an end tag1 (like &lt;/tag&gt;), and contents (everything in between the start and end tag).\nSince &lt; and &gt; are used for start and end tags, you can’t write them directly. Instead you have to use the HTML escapes &gt; (greater than) and &lt; (less than). And since those escapes use &, if you want a literal ampersand you have to escape it as &amp;. There are a wide range of possible HTML escapes but you don’t need to worry about them too much because rvest automatically handles them for you.\n\nElements\nAll up, there are over 100 HTML elements. Some of the most important are:\n\nEvery HTML page must be in an &lt;html&gt; element, and it must have two children: &lt;head&gt;, which contains document metadata like the page title, and &lt;body&gt;, which contains the content you see in the browser.\nBlock tags like &lt;h1&gt; (heading 1), &lt;p&gt; (paragraph), and &lt;ol&gt; (ordered list) form the overall structure of the page.\nInline tags like &lt;b&gt; (bold), &lt;i&gt; (italics), and &lt;a&gt; (links) formats text inside block tags.\n\nIf you encounter a tag that you’ve never seen before, you can find out what it does with a little googling. I recommend the MDN Web Docs which are produced by Mozilla, the company that makes the Firefox web browser.\n\n\nContents\nMost elements can have content in between their start and end tags. This content can either be text or more elements. For example, the following HTML contains paragraph of text, with one word in bold.\n&lt;p&gt;\n  Hi! My &lt;b&gt;name&lt;/b&gt; is Haziq.\n&lt;/p&gt;\nThis renders as\n\n  Hi! My name is Haziq.\n\nThe children of a node refers only to elements, so the &lt;p&gt; element above has one child, the &lt;b&gt; element. The &lt;b&gt; element has no children, but it does have contents (the text “name”).\nConceptually, this can be represented as follows:\n\n\n\n\n\ngraph TD;\n    P[\"&lt;p&gt; element\"]\n    P -- \"content\" --&gt; T1[\"'Hi! My '\"]\n    P --&gt; B[\"&lt;b&gt; element\"]\n    P -- \"content\" --&gt; T2[\"' is Haziq.'\"]\n    B -- \"content\" --&gt; T3[\"'name'\"]\n\n\n\n\n\n\nSome elements, like &lt;img&gt; can’t have children. These elements depend solely on attributes for their behavior.\n\n\nAttributes\nTags can have named attributes which look like name1='value1' name2='value2'. Two of the most important attributes are id and class, which are used in conjunction with CSS (Cascading Style Sheets) to control the visual appearance of the page. These are often useful when scraping data off a page."
  },
  {
    "objectID": "1-scraping_linear_reg.html#reading-html-with-rvest",
    "href": "1-scraping_linear_reg.html#reading-html-with-rvest",
    "title": "Data Scraping and Linear Regression",
    "section": "Reading HTML with rvest",
    "text": "Reading HTML with rvest\nYou’ll usually start the scraping process with read_html(). This returns an xml_document2 object which you’ll then manipulate using rvest functions:\n\nhtml &lt;- read_html(\"http://rvest.tidyverse.org/\")\nclass(html)\n\n[1] \"xml_document\" \"xml_node\"    \n\n\nFor examples and experimentation, rvest also includes a function that lets you create an xml_document from literal HTML:\n\nhtml &lt;- minimal_html(\"\n  &lt;p&gt;This is a paragraph&lt;p&gt;\n  &lt;ul&gt;\n    &lt;li&gt;This is a bulleted list&lt;/li&gt;\n  &lt;/ul&gt;\n\")\nhtml\n\n{html_document}\n&lt;html&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n&lt;p&gt;This is a paragraph&lt;/p&gt;\\n&lt;p&gt;\\n  &lt;/p&gt;\\n&lt;ul&gt;\\n&lt;li&gt;This is a bull ...\n\n\nRegardless of how you get the HTML, you’ll need some way to identify the elements that contain the data you care about. rvest provides two options: CSS selectors and XPath expressions. Here I’ll focus on CSS selectors because they’re simpler but still sufficiently powerful for most scraping tasks."
  },
  {
    "objectID": "1-scraping_linear_reg.html#css-selectors",
    "href": "1-scraping_linear_reg.html#css-selectors",
    "title": "Data Scraping and Linear Regression",
    "section": "CSS selectors",
    "text": "CSS selectors\nCSS is short for cascading style sheets, and is a tool for defining the visual styling of HTML documents. CSS includes a miniature language for selecting elements on a page called CSS selectors. CSS selectors define patterns for locating HTML elements, and are useful for scraping because they provide a concise way of describing which elements you want to extract.\nCSS selectors can be quite complex, but fortunately you only need the simplest for rvest, because you can also write R code for more complicated situations. The four most important selectors are:\n\np: selects all &lt;p&gt; elements.\n.title: selects all elements with class “title”.\np.special: selects all &lt;p&gt; elements with class “special”.\n#title: selects the element with the id attribute that equals “title”. Id attributes must be unique within a document, so this will only ever select a single element.\n\nIf you want to learn more CSS selectors I recommend starting with the fun CSS dinner tutorial and then referring to the MDN web docs.\nLets try out the most important selectors with a simple example:\n\nhtml &lt;- minimal_html(\"\n  &lt;h1&gt;This is a heading&lt;/h1&gt;\n  &lt;p id='first'&gt;This is a paragraph&lt;/p&gt;\n  &lt;p class='important'&gt;This is an important paragraph&lt;/p&gt;\n\")\n\nIn rvest you can extract a single element with html_element() or all matching elements with html_elements(). Both functions take a document3 and a css selector:\n\nhtml |&gt; html_element(\"h1\")\n\n{html_node}\n&lt;h1&gt;\n\nhtml |&gt; html_elements(\"p\")\n\n{xml_nodeset (2)}\n[1] &lt;p id=\"first\"&gt;This is a paragraph&lt;/p&gt;\n[2] &lt;p class=\"important\"&gt;This is an important paragraph&lt;/p&gt;\n\nhtml |&gt; html_elements(\".important\")\n\n{xml_nodeset (1)}\n[1] &lt;p class=\"important\"&gt;This is an important paragraph&lt;/p&gt;\n\nhtml |&gt; html_elements(\"#first\")\n\n{xml_nodeset (1)}\n[1] &lt;p id=\"first\"&gt;This is a paragraph&lt;/p&gt;\n\n\nSelectors can also be combined in various ways using combinators. For example,The most important combinator is ” “, the descendant combination, because p a selects all &lt;a&gt; elements that are a child of a &lt;p&gt; element.\nIf you don’t know exactly what selector you need, I highly recommend using SelectorGadget, which lets you automatically generate the selector you need by supplying positive and negative examples in the browser."
  },
  {
    "objectID": "1-scraping_linear_reg.html#extracting-data",
    "href": "1-scraping_linear_reg.html#extracting-data",
    "title": "Data Scraping and Linear Regression",
    "section": "Extracting data",
    "text": "Extracting data\nNow that you’ve got the elements you care about, you’ll need to get data out of them. You’ll usually get the data from either the text contents or an attribute. But, sometimes (if you’re lucky!), the data you need will be in an HTML table.\n\nText\nUse html_text2() to extract the plain text contents of an HTML element:\n\nhtml &lt;- minimal_html(\"\n  &lt;ol&gt;\n    &lt;li&gt;apple &amp; pear&lt;/li&gt;\n    &lt;li&gt;banana&lt;/li&gt;\n    &lt;li&gt;pineapple&lt;/li&gt;\n  &lt;/ol&gt;\n\")\nhtml |&gt; \n  html_elements(\"li\") |&gt; \n  html_text2()\n\n[1] \"apple & pear\" \"banana\"       \"pineapple\"   \n\n\nNote that the escaped ampersand is automatically converted to &; you’ll only ever see HTML escapes in the source HTML, not in the data returned by rvest.\nYou might wonder why I used html_text2(), since it seems to give the same result as html_text():\n\nhtml |&gt; \n  html_elements(\"li\") |&gt; \n  html_text()\n\n[1] \"apple & pear\" \"banana\"       \"pineapple\"   \n\n\nThe main difference is how the two functions handle white space. In HTML, white space is largely ignored, and it’s the structure of the elements that defines how text is laid out. html_text2() does its best to follow the same rules, giving you something similar to what you’d see in the browser. Take this example which contains a bunch of white space that HTML ignores.\n\nhtml &lt;- minimal_html(\"&lt;body&gt;\n  &lt;p&gt;\n  This is\n  a\n  paragraph.&lt;/p&gt;&lt;p&gt;This is another paragraph.\n  \n  It has two sentences.&lt;/p&gt;\n\")\n\nhtml_text2() gives you what you expect: two paragraphs of text separated by a blank line.\n\nhtml |&gt; \n  html_element(\"body\") |&gt; \n  html_text2() |&gt; \n  cat()\n\nThis is a paragraph.\n\nThis is another paragraph. It has two sentences.\n\n\nWhereas html_text() returns the garbled raw underlying text:\n\nhtml |&gt; \n  html_element(\"body\") |&gt; \n  html_text() |&gt; \n  cat()\n\n\n  \n  This is\n  a\n  paragraph.This is another paragraph.\n  \n  It has two sentences.\n\n\n\n\nAttributes\nAttributes are used to record the destination of links (the href attribute of &lt;a&gt; elements) and the source of images (the src attribute of the &lt;img&gt; element):\n\nhtml &lt;- minimal_html(\"\n  &lt;p&gt;&lt;a href='https://en.wikipedia.org/wiki/Cat'&gt;cats&lt;/a&gt;&lt;/p&gt;\n  &lt;img src='https://cataas.com/cat' width='100' height='200'&gt;\n\")\n\nThe value of an attribute can be retrieved with html_attr():\n\nhtml |&gt; \n  html_elements(\"a\") |&gt; \n  html_attr(\"href\")\n\n[1] \"https://en.wikipedia.org/wiki/Cat\"\n\nhtml |&gt; \n  html_elements(\"img\") |&gt; \n  html_attr(\"src\")\n\n[1] \"https://cataas.com/cat\"\n\n\nNote that html_attr() always returns a string, so you may need to post-process with as.integer()/readr::parse_integer() or similar.\n\nhtml |&gt; \n  html_elements(\"img\") |&gt; \n  html_attr(\"width\")\n\n[1] \"100\"\n\nhtml |&gt; \n  html_elements(\"img\") |&gt; \n  html_attr(\"width\") |&gt; \n  as.integer()\n\n[1] 100\n\n\n\n\nTables\nHTML tables are composed four main elements: &lt;table&gt;, &lt;tr&gt; (table row), &lt;th&gt; (table heading), and &lt;td&gt; (table data). Here’s a simple HTML table with two columns and three rows:\n\nhtml &lt;- minimal_html(\"\n  &lt;table&gt;\n    &lt;tr&gt;\n      &lt;th&gt;x&lt;/th&gt;\n      &lt;th&gt;y&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;1.5&lt;/td&gt;\n      &lt;td&gt;2.7&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;4.9&lt;/td&gt;\n      &lt;td&gt;1.3&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;7.2&lt;/td&gt;\n      &lt;td&gt;8.1&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/table&gt;\n  \")\n\nBecause tables are a common way to store data, rvest includes the handy html_table() which converts a table into a data frame:\n\nhtml |&gt; \n  html_node(\"table\") |&gt; \n  html_table()\n\n# A tibble: 3 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1   1.5   2.7\n2   4.9   1.3\n3   7.2   8.1"
  },
  {
    "objectID": "1-scraping_linear_reg.html#element-vs-elements",
    "href": "1-scraping_linear_reg.html#element-vs-elements",
    "title": "Data Scraping and Linear Regression",
    "section": "Element vs elements",
    "text": "Element vs elements\nWhen using rvest, your eventual goal is usually to build up a data frame, and you want each row to correspond some repeated unit on the HTML page. In this case, you should generally start by using html_elements() to select the elements that contain each observation then use html_element() to extract the variables from each observation. This guarantees that you’ll get the same number of values for each variable because html_element() always returns the same number of outputs as inputs.\nTo illustrate this problem take a look at this simple example I constructed using a few entries from dplyr::starwars:\n\nhtml &lt;- minimal_html(\"\n  &lt;ul&gt;\n    &lt;li&gt;&lt;b&gt;C-3PO&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt; that weighs &lt;span class='weight'&gt;167 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;R2-D2&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt; that weighs &lt;span class='weight'&gt;96 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;Yoda&lt;/b&gt; weighs &lt;span class='weight'&gt;66 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;R4-P17&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  \")\n\nIf you try to extract name, species, and weight directly, you end up with one vector of length four and two vectors of length three, and no way to align them:\n\nhtml |&gt; html_elements(\"b\") |&gt; html_text2()\n\n[1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\n\nhtml |&gt; html_elements(\"i\") |&gt; html_text2()\n\n[1] \"droid\" \"droid\" \"droid\"\n\nhtml |&gt; html_elements(\".weight\") |&gt; html_text2()\n\n[1] \"167 kg\" \"96 kg\"  \"66 kg\" \n\n\nInstead, use html_elements() to find a element that corresponds to each character, then use html_element() to extract each variable for all observations:\n\ncharacters &lt;- html |&gt; html_elements(\"li\")\n\ncharacters |&gt; html_element(\"b\") |&gt; html_text2()\n\n[1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\n\ncharacters |&gt; html_element(\"i\") |&gt; html_text2()\n\n[1] \"droid\" \"droid\" NA      \"droid\"\n\ncharacters |&gt; html_element(\".weight\") |&gt; html_text2()\n\n[1] \"167 kg\" \"96 kg\"  \"66 kg\"  NA      \n\n\nhtml_element() automatically fills in NA when no elements match, keeping all of the variables aligned and making it easy to create a data frame:\n\ndata.frame(\n  name = characters |&gt; html_element(\"b\") |&gt; html_text2(),\n  species = characters |&gt; html_element(\"i\") |&gt; html_text2(),\n  weight = characters |&gt; html_element(\".weight\") |&gt; html_text2()\n)\n\n    name species weight\n1  C-3PO   droid 167 kg\n2  R2-D2   droid  96 kg\n3   Yoda    &lt;NA&gt;  66 kg\n4 R4-P17   droid   &lt;NA&gt;"
  },
  {
    "objectID": "1-scraping_linear_reg.html#scraping-house-prices",
    "href": "1-scraping_linear_reg.html#scraping-house-prices",
    "title": "Data Scraping and Linear Regression",
    "section": "Scraping house prices",
    "text": "Scraping house prices\n(LIVE DEMO)\n\n# This is how you get read the HTML into R\nurl &lt;- \"https://www.bruhome.com/v3/buy.asp?p_=buy&id=&district=&propose=&property=&price=&location=&mylist=128&sort=&display=&offset=&bedrooms=&bathrooms=&carpark=\"\nhtml &lt;- read_html(url)\n\n# Extract the house prices\nprices &lt;-\n  html |&gt;\n  html_elements(\".property-price\") |&gt;\n  html_text2()\n\n# Clean up\nprices &lt;- \n  str_remove_all(prices, \"[^0-9]\") |&gt;  # Remove non-numeric characters\n  as.integer()\n\n# Do same thing for number of beds, baths, location, and other remarks\nbeds &lt;-\n  html |&gt;\n  html_elements(\".property-bed\") |&gt;\n  html_text2() |&gt;\n  as.integer()\n\nbaths &lt;-\n  html |&gt;\n  html_elements(\".property-bath\") |&gt;\n  html_text2() |&gt;\n  as.integer()\n\nlocation &lt;-\n  html |&gt;\n  html_elements(\".property-address\") |&gt;\n  html_text2()\n\nremarks &lt;- \n  html |&gt;\n  html_elements(\"div p .mt-3\") |&gt;\n  html_text2()\n\nremarks &lt;- tail(remarks, length(prices))\n\n# Put it all in a data frame\nhsp_df &lt;- tibble(\n  price = prices,\n  beds = beds,\n  baths = baths,\n  location = location,\n  remarks = remarks\n)\n\nSome pages require you to click a “load more” button to see all the data.\n\n# Extract the links\nproperties &lt;-\n  html |&gt;\n  html_elements(\".property-link\") |&gt;\n  html_attr(\"href\")\n\n# Suppose I have a function that can extract the info I want from a single page\nextract_info &lt;- function(i) {\n  link &lt;- paste0(\"https://www.bruhome.com/v3/\", properties[i])\n  html &lt;- read_html(link)\n  out &lt;-\n    html |&gt;\n    html_elements(\"p\") |&gt;\n    html_text2()\n  out[1]\n}\n\n# Now what I could do is the following:\n# res &lt;- c()\n# for (i in seq_along(properties)) {\n#   res[i] &lt;- extract_info(i)\n# }\n\n# A better way:\nres &lt;- map(\n  .x = seq_along(properties),\n  .f = extract_info,\n  .progress = TRUE\n)"
  },
  {
    "objectID": "1-scraping_linear_reg.html#cleaning-using-llm",
    "href": "1-scraping_linear_reg.html#cleaning-using-llm",
    "title": "Data Scraping and Linear Regression",
    "section": "Cleaning using LLM",
    "text": "Cleaning using LLM\nADVANCED TOPIC!!!\n\n# remotes::install_github(\"AlbertRapp/tidychatmodels\")\nlibrary(tidychatmodels)\n\nchat &lt;-\n  create_chat(\"ollama\") |&gt;\n  add_model(\"llama3.1\") |&gt;\n  add_message('What is love? IN 10 WORDS.') |&gt; \n  perform_chat() \n\nextract_chat(chat)\n\n# Try to prime it to clean the data set\nclean_desc &lt;- function(i) {\n  create_chat(\"ollama\") |&gt;\n  add_model(\"llama3.1\") |&gt;\n  add_message(glue::glue(\"\n    The following is a description of house for sale in Brunei obtained from the web site of the real estate company Bruhome. I would like you to extract the following information:\n    \n    1. Built up area (usually in square feet) [NUMERIC]\n    2. Type of house (whether it is detached, semi-detached, terrace, apartment, or other) [CHARACTER]\n  \n  Please return semicolon separated values like this:\n  2500; detached\n  3000; semi-detached\n  2000; terrace\n  etc.\n  NUMBERS SHOULD NOT CONTAIN comma (,) for thousands separator\n  \n  Please only return these two values and nothing else. Do not return any other information. I only want these two values in your chat response.\n  \n  NOTE: Some of these listings may be related to LAND or COMMERCIAL properties. In this case, please return NA for built up area, and 'commercial' or 'land' for type.\n  \n  IF YOU DO NOT SEE ANY DESCRIPTION it may mean that the description is missing. In this case, return NA only.\n  \n  IF YOU SEE MULTIPLE DESCRIPTIONS, please return the first one only.\n  \n  ----------\n  \n  {res[[i]]}\n  \")) |&gt; \n  perform_chat() |&gt;\n  extract_chat(silent = TRUE) |&gt;\n  filter(role == \"assistant\") |&gt;\n  pull(message)\n}\n\n# Now map it over the descriptions!\ncleaned_descriptions &lt;-\n  map(\n    .x = seq_along(res),\n    .f = clean_desc,\n    .progress = TRUE\n  )\n\n# Now add to the hsp_df\nhsp_df$desc &lt;- unlist(cleaned_descriptions)\nhsp_df &lt;-\n  hsp_df |&gt;\n  mutate(\n    desc = unlist(res),\n    cleaned_desc = unlist(cleaned_descriptions)\n  ) |&gt;\n  separate(cleaned_desc, into = c(\"sqft\", \"type\"), sep = \";\") |&gt;\n  mutate(\n    sqft = as.integer(sqft),\n    type = case_when(\n      grepl(\"detached\", type, ignore.case = TRUE) ~ \"detached\",\n      grepl(\"semi-detached\", type, ignore.case = TRUE) ~ \"semi-detached\",\n      grepl(\"terrace\", type, ignore.case = TRUE) ~ \"terrace\",\n      grepl(\"apartment\", type, ignore.case = TRUE) ~ \"apartment\",\n      grepl(\"land\", type, ignore.case = TRUE) ~ \"land\",\n      grepl(\"commercial\", type, ignore.case = TRUE) ~ \"commercial\",\n      TRUE ~ NA\n    )\n  )\n# save(hsp_df, file = \"data/hsp_df.RData\")"
  },
  {
    "objectID": "1-scraping_linear_reg.html#linear-regression",
    "href": "1-scraping_linear_reg.html#linear-regression",
    "title": "Data Scraping and Linear Regression",
    "section": "Linear regression",
    "text": "Linear regression\nIn statistical modelling, the aim is to describe the relationship between one or more predictor variables (usually denoted \\(x\\)) and a response variable (usually denoted \\(y\\)). Mathematically, we can say \\[\ny = f(x) + \\epsilon.\n\\] Here \\(f\\) is some regression function that we want to estimate, and \\(\\epsilon\\) is an error term that captures the difference between the true relationship and our estimate.\nThe simplest type of modelling is called linear regression, where we assume that the relationship between \\(x\\) and \\(y\\) is linear. That is, \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_px_p + \\epsilon.\n\\] When we ask software to estimate the \\(\\beta\\) coefficients, it will find the values that optimise a certain criterion (typically, one that yields the smallest error values). In R, you need to supply two things:\n\nA formula that describes the relationship between the variables.\nThe data frame that contains the variables.\n\n\nModel fit\nHere’s an example:\n\nload(\"data/hsp_df.RData\")  # I saved this data set earlier and load it back\n\n# First clean the data a bit\nhtypes &lt;- c(\"detached\", \"semi-detached\", \"terrace\", \"apartment\")\nhsp_mod_df &lt;-\n  hsp_df |&gt;\n  filter(type %in% htypes) |&gt;\n  mutate(\n    type = factor(type, levels = htypes),\n    priceK = price / 1000,  # Price in thousands\n    sqftK = sqft / 1000  # Square feet in thousands\n  ) |&gt;\n  select(priceK, beds, baths, sqftK, type) |&gt;\n  drop_na()\n\nfit &lt;- lm(priceK ~ beds + baths + sqftK + type, data = hsp_mod_df)\nsummary(fit)\n\n\nCall:\nlm(formula = priceK ~ beds + baths + sqftK + type, data = hsp_mod_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.637  -38.483   -2.603   32.274  165.839 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -178.288     47.360  -3.765 0.000387 ***\nbeds            75.147     16.837   4.463 3.70e-05 ***\nbaths           44.832     10.470   4.282 6.92e-05 ***\nsqftK           -1.654      1.894  -0.873 0.386067    \ntypeterrace    -41.525     20.878  -1.989 0.051350 .  \ntypeapartment   62.151     34.485   1.802 0.076615 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56.59 on 59 degrees of freedom\nMultiple R-squared:  0.7965,    Adjusted R-squared:  0.7792 \nF-statistic: 46.18 on 5 and 59 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nInterpretation\nHere’s a breakdown of the key components to aid your understanding:\n\nResiduals: These represent the differences between the observed and predicted values of priceK. The five-number summary (Min, 1Q, Median, 3Q, Max) helps you understand the distribution of residuals. Ideally, the residuals should be symmetrically distributed around zero, indicating a good model fit.\nCoefficients: This section shows the estimated effect (Estimate) of each predictor on priceK:\n\n(Intercept): The expected priceK when all predictors are zero. Here, it’s -178.288, but this value often doesn’t have a real-world interpretation if the predictor values can’t actually be zero.\nbeds: Each additional bedroom increases the expected priceK by about 75.15 (thousand).\nbaths: Each additional bathroom increases the expected priceK by about 44.83 (thousand).\nsqftK: The effect of the square footage in thousands on priceK. Here, it’s not statistically significant (p-value = 0.386), meaning it doesn’t contribute much to predicting priceK.\ntype: This is a categorical variable with three levels. The coefficients for typeterrace and typeapartment are relative to the reference category (likely another property type not shown here, such as “detached house”). For example, typeterrace lowers the expected priceK by 41.53 (thousand) compared to the reference category.\n\nSignificance Codes: Indicators of statistical significance for each predictor:\n\n*** highly significant (p &lt; 0.001)\n** significant (p &lt; 0.01)\n* moderately significant (p &lt; 0.05)\n. marginally significant (p &lt; 0.1)\nNone of these symbols indicate non-significance.\n\nResidual Standard Error: This is the standard deviation of the residuals. A smaller value suggests a better fit, as it indicates that the observed values are closer to the fitted values.\nR-squared and Adjusted R-squared:\n\nR-squared (0.7965) indicates that about 79.65% of the variability in priceK is explained by the model.\nAdjusted R-squared (0.7792) is a modified version of R-squared that accounts for the number of predictors, providing a more accurate measure for models with multiple variables.\n\nF-statistic: This tests whether at least one predictor variable is significantly related to the dependent variable. A p-value of &lt; 2.2e-16 indicates the model is highly significant.\n\nKey Takeaway: The model shows that beds and baths significantly predict priceK, while sqftK does not have a significant effect. The type variable shows some variation, with typeterrace having a marginally significant negative effect on priceK. Overall, the model explains a large proportion of the variation in house prices.\n\n\nPredictions\nOne of the main uses of a linear regression model is to make predictions. That is, given a set of predictor values (typically unseen data), we can estimate the response variable. In the context of the house price data, this means we can estimate the price of a house given its number of bedrooms, bathrooms, square footage, and type.\nFirst, let’s set up the data for a new house:\n\nnew_house &lt;- tibble(\n  beds = 3,\n  baths = 2,\n  sqftK = 2.5,\n  type = factor(\"detached\", levels = htypes)\n)\n\nThen, to predict the price, we run the following command:\n\npredict(fit, newdata = new_house, interval = \"prediction\", level = 0.95)\n\n       fit      lwr      upr\n1 132.6803 14.74783 250.6128\n\n\nThis also gives the 95% prediction interval, which is a range of values within which we expect the true price to fall with 95% confidence. What we can see is that the model predicts the price of the new house to be around 133,000 Brunei dollars, with a 95% prediction interval of approximately [15,000, 251,000] Brunei dollars.\nYou might be wondering why the prediction interval is so wide. This is because the model is uncertain about the price of a new house, given the limited information we have. Generally, the more data you have, the narrower the prediction interval will be.\n\n\n\n\n\n\nNote\n\n\n\nYou can get model predictions for the original data set by using the predict() without newdata argument. Alternatively, fitted() works too."
  },
  {
    "objectID": "1-scraping_linear_reg.html#more-advanced-models",
    "href": "1-scraping_linear_reg.html#more-advanced-models",
    "title": "Data Scraping and Linear Regression",
    "section": "More advanced models",
    "text": "More advanced models\nLinear regression is a simple and powerful tool, but it has its limitations. If you were more interested in predictions, then you might want to consider more advanced machine learning (ML) models. Here are a couple of suggestions:\n\nRandom Forest: This is an ensemble learning method that builds multiple decision trees and merges them together to get a more accurate and stable prediction.\nGradient Boosting Machines (GBM): GBM is another ensemble learning method that builds multiple decision trees sequentially, with each tree correcting the errors of the previous one.\nNeural Networks: These are a set of algorithms that are designed to recognize patterns, with the ability to model complex relationships between inputs and outputs.\n\n\nRandom forests\nRandom forests are popular because they are easy to use and generally provide good results. Here’s how you can fit a random forest model to the house price data:\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nfit_rf &lt;- randomForest(priceK ~ beds + baths + sqftK + type, data = hsp_mod_df)\n\nWith random forests, you don’t really get “beta” coefficients. So there’s no point running summary(). Instead, it’s mainly used as a black box to obtain predicted values.\nLet’s compare the predictions from the random forest model to the linear regression model:\n\ntibble(\n  lm = predict(fit),\n  rf = predict(fit_rf)\n) |&gt;\n  ggplot(aes(lm, rf)) +\n  geom_point() +\n  geom_abline() +\n  labs(\n    x = \"Linear regression\",\n    y = \"Random forest\",\n    title = \"Comparison of linear regression and random forest predictions\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nTo see which model gives smaller errors, we can run the following code:\n\nresid_lm &lt;- hsp_mod_df$priceK - predict(fit)\nresid_rf &lt;- hsp_mod_df$priceK - predict(fit_rf)\n\n# Residual sum of squares\nsum(resid_lm ^ 2)\n\n[1] 188934.9\n\nsum(resid_rf ^ 2)\n\n[1] 204385.7\n\n\nIn this case, the linear regression model has a smaller residual sum of squares, indicating that it fits the data better.\nOut of curiosity, let’s see the predictions for the new house using the random forest model:\n\npredict(fit_rf, newdata = new_house, )\n\n       1 \n231.9267 \n\n\nWhich seems very different to the lm() predictions."
  },
  {
    "objectID": "1-scraping_linear_reg.html#footnotes",
    "href": "1-scraping_linear_reg.html#footnotes",
    "title": "Data Scraping and Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA number of tags (including &lt;p&gt; and &lt;li&gt;) don’t require end tags, but I think it’s best to include them because it makes seeing the structure of the HTML a little easier.↩︎\nThis class comes from the xml2 package. xml2 is a low-level package that rvest builds on top of.↩︎\nOr another element, more on that shortly.↩︎"
  },
  {
    "objectID": "2-gis_data.html",
    "href": "2-gis_data.html",
    "title": "Geographical Information System (GIS) data",
    "section": "",
    "text": "Libraries needed:\nlibrary(tidyverse)\n# remotes::install_github(\"propertypricebn/bruneimap\")\nlibrary(bruneimap)\nlibrary(ggrepel)\nlibrary(kernlab)\nlibrary(osrm)\nlibrary(osmdata)\nMore info:"
  },
  {
    "objectID": "3-quanteda.html",
    "href": "3-quanteda.html",
    "title": "Quantitative analysis of textual data",
    "section": "",
    "text": "https://tutorials.quanteda.io/introduction/"
  },
  {
    "objectID": "3-quanteda.html#introduction",
    "href": "3-quanteda.html#introduction",
    "title": "Quantitative analysis of textual data",
    "section": "Introduction",
    "text": "Introduction\nThere are several R packages used for quantitative text analysis, but we will focus specifically on the {quanteda} package. So, first install the package from CRAN:\n\ninstall.packages(\"quanteda\")\n\nSince the release of {quanteda} version 3.0, textstat_*, textmodel_* and textplot_* functions are available in separate packages. We will use several of these functions in the chapters below and strongly recommend installing these packages.\n\ninstall.packages(\"quanteda.textmodels\")\ninstall.packages(\"quanteda.textstats\")\ninstall.packages(\"quanteda.textplots\")\n\nWe will use the {readtext} package to read in different types of text data in these tutorials.\n\ninstall.packages(\"readtext\")"
  },
  {
    "objectID": "3-quanteda.html#quantitative-data",
    "href": "3-quanteda.html#quantitative-data",
    "title": "Quantitative analysis of textual data",
    "section": "Quantitative data",
    "text": "Quantitative data\nBefore beginning we need to load the libraries\n\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(readtext)\n\nAnd these ones lates for the modelling section:\n\nlibrary(seededlda)\n\nWarning: package 'seededlda' was built under R version 4.4.1\n\nlibrary(LSX)\nlibrary(lubridate)\nlibrary(ggdendro)\n\n\nPre-formatted files\nIf your text data is stored in a pre-formatted file where one column contains the text and additional columns might store document-level variables (e.g. year, author, or language), you can import this into R using read_csv().\n\npath_data &lt;- system.file(\"extdata/\", package = \"readtext\")\ndat_inaug &lt;- read_csv(paste0(path_data, \"/csv/inaugCorpus.csv\"))\n\nRows: 5 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): texts, President, FirstName\ndbl (1): Year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(dat_inaug)\n\nRows: 5\nColumns: 4\n$ texts     &lt;chr&gt; \"Fellow-Citizens of the Senate and of the House of Represent…\n$ Year      &lt;dbl&gt; 1789, 1793, 1797, 1801, 1805\n$ President &lt;chr&gt; \"Washington\", \"Washington\", \"Adams\", \"Jefferson\", \"Jefferson\"\n$ FirstName &lt;chr&gt; \"George\", \"George\", \"John\", \"Thomas\", \"Thomas\"\n\n\nThe data set is about the inaugural speeches of the US presidents. So as we can see the data set is arranged in tabular form, with 5 rows and 4 columns. The columns are texts, Year, President, and FirstName.\nAlternatively, you can use the {readtext} package to import character (comma- or tab-separated) values. {readtext} reads files containing text, along with any associated document-level variables. As an example, consider the following tsv file:\n\ntsv_file &lt;- paste0(path_data, \"/tsv/dailsample.tsv\")\ncat(readLines(tsv_file, n = 4), sep = \"\\n\")  # first 3 lines\n\nspeechID    memberID    partyID constID title   date    member_name party_name  const_name  speech\n1   977 22  158 1. CEANN COMHAIRLE I gCOIR AN LAE.  1919-01-21  Count George Noble, Count Plunkett  Sinn Féin   Roscommon North Molaimse don Dáil Cathal Brugha, an Teachta ó Dhéisibh Phortláirge do bheith mar Cheann Comhairle againn indiu.\n2   1603    22  103 1. CEANN COMHAIRLE I gCOIR AN LAE.  1919-01-21  Mr. Pádraic Ó Máille    Sinn Féin   Galway Connemara    Is bród mór damhsa cur leis an dtairgsin sin. Air sin do ghaibh CATHAL BRUGHA ceannus na Dála agus adubhairt:-\n3   116 22  178 1. CEANN COMHAIRLE I gCOIR AN LAE.  1919-01-21  Mr. Cathal Brugha   Sinn Féin   Waterford County    ' A cháirde, tá obair thábhachtach le déanamh annso indiu, an obair is tábhachtaighe do rinneadh in Éirinn ón lá tháinic na Gaedhil go hÉirinn, agus is naomhtha an obair í. Daoine go bhfuil dóchas aca as Dia iseadh sinn go léir, daoine a chuireann suim I ndlighthibh Dé, agus dá bhrigh sin budh chóir dúinn congnamh d'iarraidh ar Dhia I gcóir na hoibre atá againn le déanamh. Iarrfad anois ar an sagart is dúthrachtaighe dár mhair riamh I nÉirinn, an tAthair Micheál Ó Flannagáin, guidhe chum an Spiorad Naomh dúinn chum sinn do stiúradh ar ár leas ar an mbóthar atá againn le gabháil. ' ' Agus, a cháirde, pé cineál creidimh atá ag éinne annso, iarrfad ar gach n-aon paidir do chur suas chum Dé, ó íochtar a chroidhe chum cabhair do thabhairt dúinn indiu. Glaodhaim anois ar an Athair Micheál Ó Flannagáin. ' Do tháinig an tATHAIR MICHEÁL Ó FLANNAGÁIN I láthair na Dála agus do léigh an phaidir seo I n-ár ndiaidh: 'Tair, A Spioraid Naomh, ath-líon croidhthe t'fhíoraon, agus adhain ionnta teine do ghrádha. ' ' Cuir chugainn do Spioraid agus cruthóchfar iad agus athnuadhfaidh Tú aghaidh na talmhan.' ' Guidhmís: ' ' A Dhia, do theagaisc croidhthe sa bhfíoraon le lonnradh an Spioraid Naoimh, tabhair dúinn, san Spiorad cheudna, go mblaisfimíd an ceart agus go mbéidh síorgháirdeachais orainn de bhárr a shóláis sin. Tré Íosa Críost ár dTighearna. Ámén'\n\n\nThe document itself in raw format is arranged in tabular form, separated by tabs. Each row contains a “document” (in this case, a speech) and the columns contain document-level variables. The column that contains the actual speech is named speech. To import this using {readtext}, you can use the following code:\n\ndat_dail &lt;- readtext(tsv_file, text_field = \"speech\")\nglimpse(dat_dail)\n\nRows: 33\nColumns: 11\n$ doc_id      &lt;chr&gt; \"dailsample.tsv.1\", \"dailsample.tsv.2\", \"dailsample.tsv.3\"…\n$ text        &lt;chr&gt; \"Molaimse don Dáil Cathal Brugha, an Teachta ó Dhéisibh Ph…\n$ speechID    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ memberID    &lt;int&gt; 977, 1603, 116, 116, 116, 116, 496, 116, 116, 2095, 116, 1…\n$ partyID     &lt;int&gt; 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22…\n$ constID     &lt;int&gt; 158, 103, 178, 178, 178, 178, 46, 178, 178, 139, 178, 178,…\n$ title       &lt;chr&gt; \"1. CEANN COMHAIRLE I gCOIR AN LAE.\", \"1. CEANN COMHAIRLE …\n$ date        &lt;chr&gt; \"1919-01-21\", \"1919-01-21\", \"1919-01-21\", \"1919-01-21\", \"1…\n$ member_name &lt;chr&gt; \"Count George Noble, Count Plunkett\", \"Mr. Pádraic Ó Máill…\n$ party_name  &lt;chr&gt; \"Sinn Féin\", \"Sinn Féin\", \"Sinn Féin\", \"Sinn Féin\", \"Sinn …\n$ const_name  &lt;chr&gt; \"Roscommon North\", \"Galway Connemara\", \"Waterford County\",…\n\n\n\n\nMultiple text files\nA second option to import data is to load multiple text files at once that are stored in the same folder or subfolders. Again, path_data is the location of sample files on your computer. Unlike the pre-formatted files, individual text files usually do not contain document-level variables. However, you can create document-level variables using the {readtext} package.\nThe directory /txt/UDHR contains text files (“.txt”) of the Universal Declaration of Human Rights in 13 languages.\n\npath_udhr &lt;- paste0(path_data, \"/txt/UDHR\")\nlist.files(path_udhr)  # list the files in this folder\n\n [1] \"UDHR_chinese.txt\"    \"UDHR_czech.txt\"      \"UDHR_danish.txt\"    \n [4] \"UDHR_english.txt\"    \"UDHR_french.txt\"     \"UDHR_georgian.txt\"  \n [7] \"UDHR_greek.txt\"      \"UDHR_hungarian.txt\"  \"UDHR_icelandic.txt\" \n[10] \"UDHR_irish.txt\"      \"UDHR_japanese.txt\"   \"UDHR_russian.txt\"   \n[13] \"UDHR_vietnamese.txt\"\n\n\nEach one of these txt files contains the text of the UDHR in the specific language. For instance, to inspect what each one of these files contain, we do the following:\n\n# just first 5 lines\ncat(readLines(file.path(path_udhr, \"UDHR_chinese.txt\"), n = 5), sep = \"\\n\")  \n\n世界人权宣言\n联合国大会一九四八年十二月十日第217A(III)号决议通过并颁布 1948 年 12 月 10 日， 联 合 国 大 会 通 过 并 颁 布《 世 界 人 权 宣 言》。 这 一 具 有 历 史 意 义 的《 宣 言》 颁 布 后， 大 会 要 求 所 有 会 员 国 广 为 宣 传， 并 且“ 不 分 国 家 或 领 土 的 政 治 地 位 , 主 要 在 各 级 学 校 和 其 他 教 育 机 构 加 以 传 播、 展 示、 阅 读 和 阐 述。” 《 宣 言 》 全 文 如 下： 序言 鉴于对人类家庭所有成员的固有尊严及其平等的和不移的权利的 承 认, 乃 是 世 界 自 由、 正 义 与 和 平 的 基 础, 鉴 于 对 人 权 的 无 视 和 侮 蔑 已 发 展 为 野 蛮 暴 行, 这 些 暴 行 玷 污 了 人 类 的 良 心, 而 一 个 人 人 享 有 言 论 和 信 仰 自 由 并 免 予 恐 惧 和 匮 乏 的 世 界 的 来 临, 已 被 宣 布 为 普 通 人 民 的 最 高 愿 望, 鉴 于 为 使 人 类 不 致 迫 不 得 已 铤 而 走 险 对 暴 政 和 压 迫 进 行 反 叛, 有 必 要 使 人 权 受 法 治 的 保 护, 鉴 于 有 必 要 促 进 各 国 间 友 好 关 系 的 发 展, 鉴于各联合国国家的人民已在联合国宪章中重申他们对基本人 权、 人 格 尊 严 和 价 值 以 及 男 女 平 等 权 利 的 信 念, 并 决 心 促 成 较 大 自 由 中 的 社 会 进 步 和 生 活 水 平 的 改 善, 鉴于各会员国业已誓愿同联合国合作以促进对人权和基本自由的 普 遍 尊 重 和 遵 行, 鉴于对这些权利和自由的普遍了解对于这个誓愿的充分实现具有 很 大 的 重 要 性, 因 此 现 在, 大 会, 发 布 这 一 世 界 人 权 宣 言 , 作 为 所 有 人 民 和 所 有 国 家 努 力 实 现 的 共 同 标 准, 以 期 每 一 个 人 和 社 会 机 构 经 常 铭 念 本 宣 言, 努 力 通 过 教 诲 和 教 育 促 进 对 权 利 和 自 由 的 尊 重, 并 通 过 国 家 的 和 国 际 的 渐 进 措 施, 使 这 些 权 利 和 自 由 在 各 会 员 国 本 身 人 民 及 在 其 管 辖 下 领 土 的 人 民 中 得 到 普 遍 和 有 效 的 承 认 和 遵 行; 第一条\n\n\f人 人 生 而 自 由, 在 尊 严 和 权 利 上 一 律 平 等。 他 们 赋 有 理 性 和 良 心, 并 应 以 兄 弟 关 系 的 精 神 相 对 待。 第二条 人 人 有 资 格 享 有 本 宣 言 所 载 的 一 切 权 利 和 自 由, 不 分 种 族、 肤 色、 性 别、 语 言、 宗 教、 政 治 或 其 他 见 解、 国 籍 或 社 会 出 身、 财 产、 出 生 或 其 他 身 分 等 任 何 区 别。 并 且 不 得 因 一 人 所 属 的 国 家 或 领 土 的 政 治 的、 行 政 的 或 者 国 际 的 地 位 之 不 同 而 有 所 区 别, 无 论 该 领 土 是 独 立 领 土、 托 管 领 土、 非 自 治 领 土 或 者 处 于 其 他 任 何 主 权 受 限 制 的 情 况 之 下。 第三条 人 人 有 权 享 有 生 命、 自 由 和 人 身 安 全。 第四条 任 何 人 不 得 使 为 奴 隶 或 奴 役; 一 切 形 式 的 奴 隶 制 度 和 奴 隶 买 卖, 均 应 予 以 禁 止。 第五条 任 何 人 不 得 加 以 酷 刑, 或 施 以 残 忍 的、 不 人 道 的 或 侮 辱 性 的 待 遇 或 刑 罚。 第六条 人 人 在 任 何 地 方 有 权 被 承 认 在 法 律 前 的 人 格。 第七条\n\n\nTo import these files, you can use the following code:\n\ndat_udhr &lt;- readtext(path_udhr)\nglimpse(dat_udhr)\n\nRows: 13\nColumns: 2\n$ doc_id &lt;chr&gt; \"UDHR_chinese.txt\", \"UDHR_czech.txt\", \"UDHR_danish.txt\", \"UDHR_…\n$ text   &lt;chr&gt; \"世界人权宣言\\n联合国大会一九四八年十二月十日第217A(III)号决议…\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are using Windows, you need might need to specify the encoding of the file by adding encoding = \"utf-8\". In this case, imported texts might appear like &lt;U+4E16&gt;&lt;U+754C&gt;&lt;U+4EBA&gt;&lt;U+6743&gt; but they indicate that Unicode charactes are imported correctly.\n\n\nHere’s another example of multiple text files. The directory /txt/EU_manifestos contains text files (“.txt”) of the European Union manifestos in different languages.\n\npath_eu &lt;- paste0(path_data, \"/txt/EU_manifestos/\")\nlist.files(path_eu)  # list the files in this folder\n\n [1] \"EU_euro_2004_de_PSE.txt\" \"EU_euro_2004_de_V.txt\"  \n [3] \"EU_euro_2004_en_PSE.txt\" \"EU_euro_2004_en_V.txt\"  \n [5] \"EU_euro_2004_es_PSE.txt\" \"EU_euro_2004_es_V.txt\"  \n [7] \"EU_euro_2004_fi_V.txt\"   \"EU_euro_2004_fr_PSE.txt\"\n [9] \"EU_euro_2004_fr_V.txt\"   \"EU_euro_2004_gr_V.txt\"  \n[11] \"EU_euro_2004_hu_V.txt\"   \"EU_euro_2004_it_PSE.txt\"\n[13] \"EU_euro_2004_lv_V.txt\"   \"EU_euro_2004_nl_V.txt\"  \n[15] \"EU_euro_2004_pl_V.txt\"   \"EU_euro_2004_se_V.txt\"  \n[17] \"EU_euro_2004_si_V.txt\"  \n\n\nYou can generate document-level variables based on the file names using the docvarnames and docvarsfrom argument. dvsep = \"_\" specifies the value separator in the filenames. encoding = \"ISO-8859-1\" determines character encodings of the texts. Notice how the document variables are nicely generated from the file names.\n\ndat_eu &lt;- readtext(\n  file = path_eu,\n  docvarsfrom = \"filenames\", \n  docvarnames = c(\"unit\", \"context\", \"year\", \"language\", \"party\"),\n  dvsep = \"_\", \n  encoding = \"ISO-8859-1\"\n)\nglimpse(dat_eu)\n\nRows: 17\nColumns: 7\n$ doc_id   &lt;chr&gt; \"EU_euro_2004_de_PSE.txt\", \"EU_euro_2004_de_V.txt\", \"EU_euro_…\n$ text     &lt;chr&gt; \"PES · PSE · SPE European Parliament rue Wiertz B 1047 Brusse…\n$ unit     &lt;chr&gt; \"EU\", \"EU\", \"EU\", \"EU\", \"EU\", \"EU\", \"EU\", \"EU\", \"EU\", \"EU\", \"…\n$ context  &lt;chr&gt; \"euro\", \"euro\", \"euro\", \"euro\", \"euro\", \"euro\", \"euro\", \"euro…\n$ year     &lt;int&gt; 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2…\n$ language &lt;chr&gt; \"de\", \"de\", \"en\", \"en\", \"es\", \"es\", \"fi\", \"fr\", \"fr\", \"gr\", \"…\n$ party    &lt;chr&gt; \"PSE\", \"V\", \"PSE\", \"V\", \"PSE\", \"V\", \"V\", \"PSE\", \"V\", \"V\", \"V\"…\n\n\n\n\nJSON\nYou can also read JSON files (.json) downloaded from the Twititer stream API. twitter.json is located in data directory of this tutorial package.\nThe JSON file looks something like this\n{\"created_at\":\"Wed Jun 07 23:30:01 +0000 2017\",\"id\":872596537142116352,\"id_str\":\"872596537142116352\",\"text\":\"@EFC_Jayy UKIP\",\"display_text_range\":[10,14],\n\"source\":\"\\u003ca href=\\\"http:\\/\\/twitter.com\\/download\\/iphone\\\" rel=\\\"nofollow\\\"\\u003eTwitter for iPhone\\u003c\\/a\\u003e\",\"truncated\":false,\"in_reply_to_status_id\":872596176834572288,\n\"in_reply_to_status_id_str\":\"872596176834572288\",\"in_reply_to_user_id\":4556760676,\"in_reply_to_user_id_str\":\"4556760676\",\"in_reply_to_screen_name\":\"EFC_Jayy\",\"user\":{\"id\":863929468984995840,\"id_str\":\"863929468984995840\",\"name\":\"\\u30b8\\u30e7\\u30fc\\u30b8\",\"screen_name\":\"CoysJoji\",\"location\":\"Japan\",\"url\":null,\"description\":null,\"protected\":false,\n\"verified\":false,\"followers_count\":367,\"friends_count\":304,\"listed_count\":1,\"favourites_count\":1260,\"statuses_count\":2930,\"created_at\":\"Mon May 15 01:30:11 +0000 2017\",\"utc_offset\":null,\"time_zone\":null,\"geo_enabled\":false,\"lang\":\"en\",\"contributors_enabled\":false,\"is_translator\":false,\"profile_background_color\":\"F5F8FA\",\"profile_background_image_url\":\"\",\"profile_background_image_url_https\":\"\",\"profile_background_tile\":false,\n\"profile_link_color\":\"1DA1F2\",\"profile_sidebar_border_color\":\"C0DEED\",\"profile_sidebar_fill_color\":\"DDEEF6\",\"profile_text_color\":\"333333\",\"profile_use_background_image\":true,\"profile_image_url\":\"http:\\/\\/pbs.twimg.com\\/profile_images\\/870447188400365568\\/RiR1hbCe_normal.jpg\",\n\"profile_image_url_https\":\"https:\\/\\/pbs.twimg.com\\/profile_images\\/870447188400365568\\/RiR1hbCe_normal.jpg\",\"profile_banner_url\":\"https:\\/\\/pbs.twimg.com\\/profile_banners\\/863929468984995840\\/1494897624\",\"default_profile\":true,\"default_profile_image\":false,\"following\":null,\"follow_request_sent\":null,\"notifications\":null},\"geo\":null,\"coordinates\":null,\n\"place\":null,\"contributors\":null,\"is_quote_status\":false,\"retweet_count\":0,\"favorite_count\":0,\"entities\":{\"hashtags\":[],\"urls\":[],\"user_mentions\":[{\"screen_name\":\"EFC_Jayy\",\"name\":\"\\u274c\\u274c\\u274c\",\"id\":4556760676,\"id_str\":\"4556760676\",\"indices\":[0,9]}],\"symbols\":[]},\"favorited\":false,\"retweeted\":false,\"filter_level\":\"low\",\"lang\":\"en\",\"timestamp_ms\":\"1496878201171\"}\nIt’s a little hard to parse, but luckily we just leave it to the {readtext} package to do the job for us.\n\ndat_twitter &lt;- readtext(\"../data/twitter.json\", source = \"twitter\")\n\nThe file comes with several metadata for each tweet, such as the number of retweets and likes, the username, time and time zone.\n\nhead(names(dat_twitter))\n\n## [1] \"doc_id\"         \"text\"           \"retweet_count\"  \"favorite_count\"\n## [5] \"favorited\"      \"truncated\"\n\n\nPDF\nreadtext() can also convert and read PDF (“.pdf”) files. The directory /pdf/UDHR contains PDF files of the Universal Declaration of Human Rights in 13 languages. Each file looks like this:\n\n\ndat_udhr &lt;- readtext(\n  paste0(path_data, \"/pdf/UDHR/*.pdf\"), \n  docvarsfrom = \"filenames\", \n  docvarnames = c(\"document\", \"language\"),\n  sep = \"_\"\n)\nprint(dat_udhr)\n\nreadtext object consisting of 11 documents and 2 docvars.\n# A data frame: 11 × 4\n  doc_id           text                          document language\n  &lt;chr&gt;            &lt;chr&gt;                         &lt;chr&gt;    &lt;chr&gt;   \n1 UDHR_chinese.pdf \"\\\"世界人权宣言\\n\\n联合\\\"...\" UDHR     chinese \n2 UDHR_czech.pdf   \"\\\"VŠEOBECNÁ \\\"...\"           UDHR     czech   \n3 UDHR_danish.pdf  \"\\\"Den 10. de\\\"...\"           UDHR     danish  \n4 UDHR_english.pdf \"\\\"Universal \\\"...\"           UDHR     english \n5 UDHR_french.pdf  \"\\\"Déclaratio\\\"...\"           UDHR     french  \n6 UDHR_greek.pdf   \"\\\"ΟΙΚΟΥΜΕΝΙΚ\\\"...\"           UDHR     greek   \n# ℹ 5 more rows\n\n\n\n\nMicrosoft Word\nFinally, readtext() can import Microsoft Word (“.doc” and “.docx”) files.\n\ndat_word &lt;- readtext(paste0(path_data, \"/word/*.docx\"))\nprint(dat_udhr)\n\nreadtext object consisting of 11 documents and 2 docvars.\n# A data frame: 11 × 4\n  doc_id           text                          document language\n  &lt;chr&gt;            &lt;chr&gt;                         &lt;chr&gt;    &lt;chr&gt;   \n1 UDHR_chinese.pdf \"\\\"世界人权宣言\\n\\n联合\\\"...\" UDHR     chinese \n2 UDHR_czech.pdf   \"\\\"VŠEOBECNÁ \\\"...\"           UDHR     czech   \n3 UDHR_danish.pdf  \"\\\"Den 10. de\\\"...\"           UDHR     danish  \n4 UDHR_english.pdf \"\\\"Universal \\\"...\"           UDHR     english \n5 UDHR_french.pdf  \"\\\"Déclaratio\\\"...\"           UDHR     french  \n6 UDHR_greek.pdf   \"\\\"ΟΙΚΟΥΜΕΝΙΚ\\\"...\"           UDHR     greek   \n# ℹ 5 more rows"
  },
  {
    "objectID": "3-quanteda.html#workflow",
    "href": "3-quanteda.html#workflow",
    "title": "Quantitative analysis of textual data",
    "section": "Workflow",
    "text": "Workflow\n{quanteda} has three basic types of objects:\n\nCorpus\n\nSaves character strings and variables in a data frame\nCombines texts with document-level variables\n\nTokens\n\nStores tokens in a list of vectors\nMore efficient than character strings, but preserves positions of words\nPositional (string-of-words) analysis is performed using textstat_collocations(), tokens_ngrams() and tokens_select() or fcm() with window option\n\nDocument-feature matrix (DFM)\n\nRepresents frequencies of features in documents in a matrix\nThe most efficient structure, but it does not have information on positions of words\nNon-positional (bag-of-words) analysis are profrmed using many of the textstat_* and textmodel_* functions\n\n\nText analysis with {quanteda} goes through all those three types of objects either explicitly or implicitly.\n\n\n\n\n\n    graph TD\n    D[Text files]\n    V[Document-level variables]\n    C(Corpus)\n    T(Tokens)\n    AP[\"Positional analysis (string-of-words)\"]\n    AN[\"Non-positional analysis (bag-of-words)\"]\n    M(DFM)\n    style C stroke-width:4px\n    style T stroke-width:4px\n    style M stroke-width:4px\n    D --&gt; C\n    V --&gt; C \n    C --&gt; T \n    T --&gt; M\n    T -.-&gt; AP\n    M -.-&gt; AN\n\n\n\n\n\n\nFor example, if character vectors are given to dfm(), it internally constructs corpus and tokens objects before creating a DFM.\n\nCorpus\nYou can create a corpus from various available sources:\n\nA character vector consisting of one document per element\nA data frame consisting of a character vector for documents, and additional vectors for document-level variables\n\n\nCharacter vector\ndata_char_ukimmig2010 is a named character vector and consists of sections of British election manifestos on immigration and asylum.\n\nstr(data_char_ukimmig2010)\n\n Named chr [1:9] \"IMMIGRATION: AN UNPARALLELED CRISIS WHICH ONLY THE BNP CAN SOLVE. \\n\\n- At current immigration and birth rates,\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:9] \"BNP\" \"Coalition\" \"Conservative\" \"Greens\" ...\n\ncorp_immig &lt;- corpus(\n  data_char_ukimmig2010, \n  docvars = data.frame(party = names(data_char_ukimmig2010))\n)\nprint(corp_immig)\n\nCorpus consisting of 9 documents and 1 docvar.\nBNP :\n\"IMMIGRATION: AN UNPARALLELED CRISIS WHICH ONLY THE BNP CAN S...\"\n\nCoalition :\n\"IMMIGRATION.  The Government believes that immigration has e...\"\n\nConservative :\n\"Attract the brightest and best to our country. Immigration h...\"\n\nGreens :\n\"Immigration. Migration is a fact of life.  People have alway...\"\n\nLabour :\n\"Crime and immigration The challenge for Britain We will cont...\"\n\nLibDem :\n\"firm but fair immigration system Britain has always been an ...\"\n\n[ reached max_ndoc ... 3 more documents ]\n\nsummary(corp_immig)\n\nCorpus consisting of 9 documents, showing 9 documents:\n\n         Text Types Tokens Sentences        party\n          BNP  1125   3280        88          BNP\n    Coalition   142    260         4    Coalition\n Conservative   251    499        15 Conservative\n       Greens   322    679        21       Greens\n       Labour   298    683        29       Labour\n       LibDem   251    483        14       LibDem\n           PC    77    114         5           PC\n          SNP    88    134         4          SNP\n         UKIP   346    723        26         UKIP\n\n\n\n\nData frame\nUsing read_csv(), load an example file from path_data as a data frame called dat_inaug. Note that your file does not need to be formatted as .csv. You can build a {quanteda} corpus from any file format that R can import as a data frame (see, for instance, the rio package for importing various files as data frames into R).\n\n# set path\npath_data &lt;- system.file(\"extdata/\", package = \"readtext\")\n\n# import csv file\ndat_inaug &lt;- read.csv(paste0(path_data, \"/csv/inaugCorpus.csv\"))\nnames(dat_inaug)\n\n[1] \"texts\"     \"Year\"      \"President\" \"FirstName\"\n\n\nConstruct a corpus from the “texts” column in dat_inaug.\n\ncorp_inaug &lt;- corpus(dat_inaug, text_field = \"texts\")\nprint(corp_inaug)\n\nCorpus consisting of 5 documents and 3 docvars.\ntext1 :\n\"Fellow-Citizens of the Senate and of the House of Representa...\"\n\ntext2 :\n\"Fellow citizens, I am again called upon by the voice of my c...\"\n\ntext3 :\n\"When it was first perceived, in early times, that no middle ...\"\n\ntext4 :\n\"Friends and Fellow Citizens: Called upon to undertake the du...\"\n\ntext5 :\n\"Proceeding, fellow citizens, to that qualification which the...\"\n\n\n\n\nDocument-level variables\n{quanteda}’s objects keep information associated with documents. They are called “document-level variables”, or “docvars”, and are accessed using docvars().\n\ncorp &lt;- data_corpus_inaugural\nhead(docvars(corp))\n\n  Year  President FirstName                 Party\n1 1789 Washington    George                  none\n2 1793 Washington    George                  none\n3 1797      Adams      John            Federalist\n4 1801  Jefferson    Thomas Democratic-Republican\n5 1805  Jefferson    Thomas Democratic-Republican\n6 1809    Madison     James Democratic-Republican\n\n\nIf you want to extract individual elements of document variables, you can specify field. Or you could just subset it as you normally would a data.frame.\n\ndocvars(corp, field = \"Year\")\n\n [1] 1789 1793 1797 1801 1805 1809 1813 1817 1821 1825 1829 1833 1837 1841 1845\n[16] 1849 1853 1857 1861 1865 1869 1873 1877 1881 1885 1889 1893 1897 1901 1905\n[31] 1909 1913 1917 1921 1925 1929 1933 1937 1941 1945 1949 1953 1957 1961 1965\n[46] 1969 1973 1977 1981 1985 1989 1993 1997 2001 2005 2009 2013 2017 2021\n\ncorp$Year\n\n [1] 1789 1793 1797 1801 1805 1809 1813 1817 1821 1825 1829 1833 1837 1841 1845\n[16] 1849 1853 1857 1861 1865 1869 1873 1877 1881 1885 1889 1893 1897 1901 1905\n[31] 1909 1913 1917 1921 1925 1929 1933 1937 1941 1945 1949 1953 1957 1961 1965\n[46] 1969 1973 1977 1981 1985 1989 1993 1997 2001 2005 2009 2013 2017 2021\n\n\nSo that means assignments to change document-level variables will work as usual in R. For example, you can change the Year variable to a factor (if you wished). And since the output of a docvars() function is a data.frame, you could subset or filter as you would a data.frame.\n\ndocvars(corp) |&gt;\n  filter(Year &gt;= 1990)\n\n  Year President FirstName      Party\n1 1993   Clinton      Bill Democratic\n2 1997   Clinton      Bill Democratic\n3 2001      Bush George W. Republican\n4 2005      Bush George W. Republican\n5 2009     Obama    Barack Democratic\n6 2013     Obama    Barack Democratic\n7 2017     Trump Donald J. Republican\n8 2021     Biden Joseph R. Democratic\n\n# {quanteda} also provides corpus_subset() function, but since we learnt about\n# dplyr, we can use it here.\n\nAnother useful feature is the ability to change the unit of texts. For example, the UK Immigration 2010 data set is a corpus of 9 documents, where each document is a speech by the political party.\n\ncorp &lt;- corpus(data_char_ukimmig2010)\nprint(corp)\n\nCorpus consisting of 9 documents.\nBNP :\n\"IMMIGRATION: AN UNPARALLELED CRISIS WHICH ONLY THE BNP CAN S...\"\n\nCoalition :\n\"IMMIGRATION.  The Government believes that immigration has e...\"\n\nConservative :\n\"Attract the brightest and best to our country. Immigration h...\"\n\nGreens :\n\"Immigration. Migration is a fact of life.  People have alway...\"\n\nLabour :\n\"Crime and immigration The challenge for Britain We will cont...\"\n\nLibDem :\n\"firm but fair immigration system Britain has always been an ...\"\n\n[ reached max_ndoc ... 3 more documents ]\n\n\nWe can use corpus_reshape() to change the unit of texts. For example, we can change the unit of texts to sentences using the command below.\n\ncorp_sent &lt;- corpus_reshape(corp, to = \"sentences\")\nprint(corp_sent)\n\nCorpus consisting of 206 documents.\nBNP.1 :\n\"IMMIGRATION: AN UNPARALLELED CRISIS WHICH ONLY THE BNP CAN S...\"\n\nBNP.2 :\n\"The Scale of the Crisis  Britain's existence is in grave per...\"\n\nBNP.3 :\n\"In the absence of urgent action, we, the indigenous British ...\"\n\nBNP.4 :\n\"We, alone of all the political parties, have a decades-long ...\"\n\nBNP.5 :\n\"British People Set to be a Minority within 30 - 50 Years: Th...\"\n\nBNP.6 :\n\"Figures released by the ONS in January 2009 revealed that th...\"\n\n[ reached max_ndoc ... 200 more documents ]\n\n\nThe following code restores it back to the document level.\n\ncorp_doc &lt;- corpus_reshape(corp_sent, to = \"documents\")\nprint(corp_doc)\n\nCorpus consisting of 9 documents.\nBNP :\n\"IMMIGRATION: AN UNPARALLELED CRISIS WHICH ONLY THE BNP CAN S...\"\n\nCoalition :\n\"IMMIGRATION.  The Government believes that immigration has e...\"\n\nConservative :\n\"Attract the brightest and best to our country.  Immigration ...\"\n\nGreens :\n\"Immigration.  Migration is a fact of life.  People have alwa...\"\n\nLabour :\n\"Crime and immigration  The challenge for Britain  We will co...\"\n\nLibDem :\n\"firm but fair immigration system  Britain has always been an...\"\n\n[ reached max_ndoc ... 3 more documents ]\n\n\n\n\n\nTokens\ntokens() segments texts in a corpus into tokens (words or sentences) by word boundaries. By default, tokens() only removes separators (typically white spaces), but you can also remove punctuation and numbers.\n\ntoks &lt;- tokens(corp_immig)\nprint(toks)\n\nTokens consisting of 9 documents and 1 docvar.\nBNP :\n [1] \"IMMIGRATION\"  \":\"            \"AN\"           \"UNPARALLELED\" \"CRISIS\"      \n [6] \"WHICH\"        \"ONLY\"         \"THE\"          \"BNP\"          \"CAN\"         \n[11] \"SOLVE\"        \".\"           \n[ ... and 3,268 more ]\n\nCoalition :\n [1] \"IMMIGRATION\" \".\"           \"The\"         \"Government\"  \"believes\"   \n [6] \"that\"        \"immigration\" \"has\"         \"enriched\"    \"our\"        \n[11] \"culture\"     \"and\"        \n[ ... and 248 more ]\n\nConservative :\n [1] \"Attract\"     \"the\"         \"brightest\"   \"and\"         \"best\"       \n [6] \"to\"          \"our\"         \"country\"     \".\"           \"Immigration\"\n[11] \"has\"         \"enriched\"   \n[ ... and 487 more ]\n\nGreens :\n [1] \"Immigration\" \".\"           \"Migration\"   \"is\"          \"a\"          \n [6] \"fact\"        \"of\"          \"life\"        \".\"           \"People\"     \n[11] \"have\"        \"always\"     \n[ ... and 667 more ]\n\nLabour :\n [1] \"Crime\"       \"and\"         \"immigration\" \"The\"         \"challenge\"  \n [6] \"for\"         \"Britain\"     \"We\"          \"will\"        \"control\"    \n[11] \"immigration\" \"with\"       \n[ ... and 671 more ]\n\nLibDem :\n [1] \"firm\"        \"but\"         \"fair\"        \"immigration\" \"system\"     \n [6] \"Britain\"     \"has\"         \"always\"      \"been\"        \"an\"         \n[11] \"open\"        \",\"          \n[ ... and 471 more ]\n\n[ reached max_ndoc ... 3 more documents ]\n\ntoks_nopunct &lt;- tokens(data_char_ukimmig2010, remove_punct = TRUE)\nprint(toks_nopunct)\n\nTokens consisting of 9 documents.\nBNP :\n [1] \"IMMIGRATION\"  \"AN\"           \"UNPARALLELED\" \"CRISIS\"       \"WHICH\"       \n [6] \"ONLY\"         \"THE\"          \"BNP\"          \"CAN\"          \"SOLVE\"       \n[11] \"At\"           \"current\"     \n[ ... and 2,839 more ]\n\nCoalition :\n [1] \"IMMIGRATION\"  \"The\"          \"Government\"   \"believes\"     \"that\"        \n [6] \"immigration\"  \"has\"          \"enriched\"     \"our\"          \"culture\"     \n[11] \"and\"          \"strengthened\"\n[ ... and 219 more ]\n\nConservative :\n [1] \"Attract\"     \"the\"         \"brightest\"   \"and\"         \"best\"       \n [6] \"to\"          \"our\"         \"country\"     \"Immigration\" \"has\"        \n[11] \"enriched\"    \"our\"        \n[ ... and 440 more ]\n\nGreens :\n [1] \"Immigration\" \"Migration\"   \"is\"          \"a\"           \"fact\"       \n [6] \"of\"          \"life\"        \"People\"      \"have\"        \"always\"     \n[11] \"moved\"       \"from\"       \n[ ... and 598 more ]\n\nLabour :\n [1] \"Crime\"       \"and\"         \"immigration\" \"The\"         \"challenge\"  \n [6] \"for\"         \"Britain\"     \"We\"          \"will\"        \"control\"    \n[11] \"immigration\" \"with\"       \n[ ... and 608 more ]\n\nLibDem :\n [1] \"firm\"        \"but\"         \"fair\"        \"immigration\" \"system\"     \n [6] \"Britain\"     \"has\"         \"always\"      \"been\"        \"an\"         \n[11] \"open\"        \"welcoming\"  \n[ ... and 423 more ]\n\n[ reached max_ndoc ... 3 more documents ]\n\n\nYou can see how keywords are used in the actual contexts in a concordance view produced by kwic().\n\nkw &lt;- kwic(toks, pattern =  \"immig*\")\nhead(kw, 10)\n\nKeyword-in-context with 10 matches.                                                                 \n   [BNP, 1]                                       | IMMIGRATION |\n  [BNP, 16]                   SOLVE. - At current | immigration |\n  [BNP, 78]                 a halt to all further | immigration |\n  [BNP, 85]        the deportation of all illegal | immigrants  |\n [BNP, 169]          Britain, regardless of their | immigration |\n [BNP, 197] admission that they orchestrated mass | immigration |\n [BNP, 272]            grave peril, threatened by | immigration |\n [BNP, 374]                  ), legal Third World | immigrants  |\n [BNP, 531]        to second and third generation |  immigrant  |\n [BNP, 661]                     are added in, the |  immigrant  |\n                                          \n : AN UNPARALLELED CRISIS WHICH           \n and birth rates, indigenous              \n , the deportation of all                 \n , a halt to the                          \n status. - The BNP                        \n to change forcibly Britain's demographics\n and multiculturalism. In the             \n made up 14.7 percent (                   \n mothers. Figures released by             \n birth rate is estimated to               \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf you want to find multi-word expressions, separate words by white space and wrap the character vector by phrase(), as follows:\nkw_asylum &lt;- kwic(toks, pattern = phrase(\"asylum seeker*\"))\nTexts do not always appear nicely in your R console, so you can use View() to see the keywords-in-context in an interactive HTML table.\n\n\n\nYou can remove tokens that you are not interested in using tokens_select(). Usually we remove function words (grammatical words) that have little or no substantive meaning in pre-processing. stopwords() returns a pre-defined list of function words.\n\ntoks_nostop &lt;- tokens_select(\n  toks, \n  pattern = stopwords(\"en\"), \n  selection = \"remove\"  # keep or remove\n)\nprint(toks_nostop)\n\nTokens consisting of 9 documents and 1 docvar.\nBNP :\n [1] \"IMMIGRATION\"  \":\"            \"UNPARALLELED\" \"CRISIS\"       \"BNP\"         \n [6] \"CAN\"          \"SOLVE\"        \".\"            \"-\"            \"current\"     \n[11] \"immigration\"  \"birth\"       \n[ ... and 2,109 more ]\n\nCoalition :\n [1] \"IMMIGRATION\"  \".\"            \"Government\"   \"believes\"     \"immigration\" \n [6] \"enriched\"     \"culture\"      \"strengthened\" \"economy\"      \",\"           \n[11] \"must\"         \"controlled\"  \n[ ... and 146 more ]\n\nConservative :\n [1] \"Attract\"     \"brightest\"   \"best\"        \"country\"     \".\"          \n [6] \"Immigration\" \"enriched\"    \"nation\"      \"years\"       \"want\"       \n[11] \"attract\"     \"brightest\"  \n[ ... and 277 more ]\n\nGreens :\n [1] \"Immigration\" \".\"           \"Migration\"   \"fact\"        \"life\"       \n [6] \".\"           \"People\"      \"always\"      \"moved\"       \"one\"        \n[11] \"country\"     \"another\"    \n[ ... and 377 more ]\n\nLabour :\n [1] \"Crime\"            \"immigration\"      \"challenge\"        \"Britain\"         \n [5] \"control\"          \"immigration\"      \"new\"              \"Australian-style\"\n [9] \"points-based\"     \"system\"           \"-\"                \"unlike\"          \n[ ... and 391 more ]\n\nLibDem :\n [1] \"firm\"        \"fair\"        \"immigration\" \"system\"      \"Britain\"    \n [6] \"always\"      \"open\"        \",\"           \"welcoming\"   \"country\"    \n[11] \",\"           \"thousands\"  \n[ ... and 285 more ]\n\n[ reached max_ndoc ... 3 more documents ]\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe stopwords() function returns character vectors of stopwords for different languages, using the ISO-639-1 language codes. For Malay, use stopwords(\"ms\", source = \"stopwords-iso\"). For Bruneian specific context, you may need to amend the stopwords yourselves.\n\n\nYou can generate n-grams in any lengths from a tokens using tokens_ngrams(). N-grams are a contiguous sequence of n tokens from already tokenized text objects. So for example, in the phrase “natural language processing”:\n\nUnigram (1-gram): “natural”, “language”, “processing”\nBigram (2-gram): “natural language”, “language processing”\nTrigram (3-gram): “natural language processing”\n\n\n# tokens_ngrams() also supports skip to generate skip-grams.\ntoks_ngram &lt;- tokens_ngrams(toks_nopunct, n = 3, skip = 0)\nhead(toks_ngram[[1]], 20)  # the first political party's trigrams\n\n [1] \"IMMIGRATION_AN_UNPARALLELED\" \"AN_UNPARALLELED_CRISIS\"     \n [3] \"UNPARALLELED_CRISIS_WHICH\"   \"CRISIS_WHICH_ONLY\"          \n [5] \"WHICH_ONLY_THE\"              \"ONLY_THE_BNP\"               \n [7] \"THE_BNP_CAN\"                 \"BNP_CAN_SOLVE\"              \n [9] \"CAN_SOLVE_At\"                \"SOLVE_At_current\"           \n[11] \"At_current_immigration\"      \"current_immigration_and\"    \n[13] \"immigration_and_birth\"       \"and_birth_rates\"            \n[15] \"birth_rates_indigenous\"      \"rates_indigenous_British\"   \n[17] \"indigenous_British_people\"   \"British_people_are\"         \n[19] \"people_are_set\"              \"are_set_to\"                 \n\n\n\n\nDocument feature matrix\ndfm() constructs a document-feature matrix (DFM) from a tokens object.\n\ntoks_inaug &lt;- tokens(data_corpus_inaugural, remove_punct = TRUE)\ndfmat_inaug &lt;- dfm(toks_inaug)\nprint(dfmat_inaug)\n\nDocument-feature matrix of: 59 documents, 9,419 features (91.89% sparse) and 4 docvars.\n                 features\ndocs              fellow-citizens  of the senate and house representatives\n  1789-Washington               1  71 116      1  48     2               2\n  1793-Washington               0  11  13      0   2     0               0\n  1797-Adams                    3 140 163      1 130     0               2\n  1801-Jefferson                2 104 130      0  81     0               0\n  1805-Jefferson                0 101 143      0  93     0               0\n  1809-Madison                  1  69 104      0  43     0               0\n                 features\ndocs              among vicissitudes incident\n  1789-Washington     1            1        1\n  1793-Washington     0            0        0\n  1797-Adams          4            0        0\n  1801-Jefferson      1            0        0\n  1805-Jefferson      7            0        0\n  1809-Madison        0            0        0\n[ reached max_ndoc ... 53 more documents, reached max_nfeat ... 9,409 more features ]\n\n\nSome useful functions to operate on DFMs are:\n\nndoc(): returns the number of documents\nnfeat(): returns the number of features\ndocnames(): returns the document names\nfeatnames(): returns the feature (column) names\ntopfeatures(): returns the most frequent features\ndocvars(): returns the document-level variables\n\nDFMs sometimes behaves like normal matrices too, so you can use rowSums() and colSums() to calculate marginals.\nMost commonly perhaps, is you want to select some columns (i.e. features) from the DFM that satisfy a pattern. For instance,\n\ndfm_select(dfmat_inaug, pattern = \"freedom\")\n\nDocument-feature matrix of: 59 documents, 1 feature (38.98% sparse) and 4 docvars.\n                 features\ndocs              freedom\n  1789-Washington       0\n  1793-Washington       0\n  1797-Adams            0\n  1801-Jefferson        4\n  1805-Jefferson        2\n  1809-Madison          1\n[ reached max_ndoc ... 53 more documents ]\n\ndfm_keep(dfmat_inaug, min_nchar = 5)\n\nDocument-feature matrix of: 59 documents, 8,560 features (93.04% sparse) and 4 docvars.\n                 features\ndocs              fellow-citizens senate house representatives among\n  1789-Washington               1      1     2               2     1\n  1793-Washington               0      0     0               0     0\n  1797-Adams                    3      1     0               2     4\n  1801-Jefferson                2      0     0               0     1\n  1805-Jefferson                0      0     0               0     7\n  1809-Madison                  1      0     0               0     0\n                 features\ndocs              vicissitudes incident event could filled\n  1789-Washington            1        1     2     3      1\n  1793-Washington            0        0     0     0      0\n  1797-Adams                 0        0     0     1      0\n  1801-Jefferson             0        0     0     0      0\n  1805-Jefferson             0        0     0     2      0\n  1809-Madison               0        0     0     1      1\n[ reached max_ndoc ... 53 more documents, reached max_nfeat ... 8,550 more features ]\n\n\nThere is also dfm_trim() to remove features that are too frequent or too rare, based on frequencies.\n\n# Trim DFM containing features that occur less than 10 times in the corpus\ndfm_trim(dfmat_inaug, min_termfreq = 10)\n\nDocument-feature matrix of: 59 documents, 1,524 features (68.93% sparse) and 4 docvars.\n                 features\ndocs              fellow-citizens  of the senate and house representatives\n  1789-Washington               1  71 116      1  48     2               2\n  1793-Washington               0  11  13      0   2     0               0\n  1797-Adams                    3 140 163      1 130     0               2\n  1801-Jefferson                2 104 130      0  81     0               0\n  1805-Jefferson                0 101 143      0  93     0               0\n  1809-Madison                  1  69 104      0  43     0               0\n                 features\ndocs              among to life\n  1789-Washington     1 48    1\n  1793-Washington     0  5    0\n  1797-Adams          4 72    2\n  1801-Jefferson      1 61    1\n  1805-Jefferson      7 83    2\n  1809-Madison        0 61    1\n[ reached max_ndoc ... 53 more documents, reached max_nfeat ... 1,514 more features ]\n\n# Trim DFM containing features that occur in more than 10% of the documents\ndfm_trim(dfmat_inaug, max_docfreq = 0.1, docfreq_type = \"prop\")\n\nDocument-feature matrix of: 59 documents, 7,388 features (96.87% sparse) and 4 docvars.\n                 features\ndocs              vicissitudes filled anxieties notification transmitted 14th\n  1789-Washington            1      1         1            1           1    1\n  1793-Washington            0      0         0            0           0    0\n  1797-Adams                 0      0         0            0           0    0\n  1801-Jefferson             0      0         0            0           0    0\n  1805-Jefferson             0      0         0            0           0    0\n  1809-Madison               0      1         0            0           0    0\n                 features\ndocs              month summoned veneration fondest\n  1789-Washington     1        1          1       1\n  1793-Washington     0        0          0       0\n  1797-Adams          0        0          2       0\n  1801-Jefferson      0        0          0       0\n  1805-Jefferson      0        0          0       0\n  1809-Madison        0        0          0       0\n[ reached max_ndoc ... 53 more documents, reached max_nfeat ... 7,378 more features ]"
  },
  {
    "objectID": "3-quanteda.html#statistical-analysis",
    "href": "3-quanteda.html#statistical-analysis",
    "title": "Quantitative analysis of textual data",
    "section": "Statistical analysis",
    "text": "Statistical analysis\nNote: If you have not installed {quanteda.corpora}, do so by running\nremotes::install_github(\"quanteda/quanteda.corpora\")\n\nSimple frequency analysis\nUnlike topfeatures(), textstat_frequency() shows both term and document frequencies. You can also use the function to find the most frequent features within groups. Using the download() function from {quanteda.corpora}, you can retrieve a text corpus of tweets.\n\ncorp_tweets &lt;- quanteda.corpora::download(url = \"https://www.dropbox.com/s/846skn1i5elbnd2/data_corpus_sampletweets.rds?dl=1\")\n\nWe can analyse the most frequent hashtags by applying tokens_keep(pattern = \"#*\") before creating a DFM.\n\ntoks_tweets &lt;- \n  tokens(corp_tweets, remove_punct = TRUE) |&gt;\n  tokens_keep(pattern = \"#*\")\ndfmat_tweets &lt;- dfm(toks_tweets)\n\ntstat_freq &lt;- textstat_frequency(dfmat_tweets, n = 5, groups = lang)\nhead(tstat_freq, 20)\n\n             feature frequency rank docfreq     group\n1           #twitter         1    1       1    Basque\n2     #canviemeuropa         1    1       1    Basque\n3             #prest         1    1       1    Basque\n4           #psifizo         1    1       1    Basque\n5     #ekloges2014gr         1    1       1    Basque\n6            #ep2014         1    1       1 Bulgarian\n7         #yourvoice         1    1       1 Bulgarian\n8      #eudebate2014         1    1       1 Bulgarian\n9            #велико         1    1       1 Bulgarian\n10           #ep2014         1    1       1  Croatian\n11 #savedonbaspeople         1    1       1  Croatian\n12   #vitoriagasteiz         1    1       1  Croatian\n13           #ep14dk        32    1      32    Danish\n14            #dkpol        18    2      18    Danish\n15            #eupol         7    3       7    Danish\n16        #vindtilep         7    3       7    Danish\n17    #patentdomstol         4    5       4    Danish\n18           #ep2014        35    1      35     Dutch\n19              #vvd        11    2      11     Dutch\n20               #eu         9    3       7     Dutch\n\n\nYou can also plot the Twitter hashtag frequencies easily using ggplot().\n\ndfmat_tweets |&gt;\n  textstat_frequency(n = 15) |&gt;\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_point() +\n  coord_flip() +\n  labs(x = NULL, y = \"Frequency\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nAlternatively, you can create a word cloud of the 100 most common hashtags.\n\nset.seed(132)\ntextplot_wordcloud(dfmat_tweets, max_words = 100)\n\n\n\n\n\n\n\n\nFinally, it is possible to compare different groups within one Wordcloud. We must first create a dummy variable that indicates whether a tweet was posted in English or a different language. Afterwards, we can compare the most frequent hashtags of English and non-English tweets.\n\n# create document-level variable indicating whether tweet was in English or\n# other language\ncorp_tweets$dummy_english &lt;- \n  factor(ifelse(corp_tweets$lang == \"English\", \"English\", \"Not English\"))\n\n# tokenize texts\ntoks_tweets &lt;- tokens(corp_tweets)\n\n# create a grouped dfm and compare groups\ndfmat_corp_language &lt;- \n  dfm(toks_tweets) |&gt;\n  dfm_keep(pattern = \"#*\") |&gt;\n  dfm_group(groups = dummy_english)\n\n# create wordcloud\nset.seed(132) # set seed for reproducibility\ntextplot_wordcloud(dfmat_corp_language, comparison = TRUE, max_words = 200)\n\n\n\n\n\n\n\n\n\n\nLexical diversity\nLexical diversity is a measure of how varied the vocabulary in a text or speech is. It indicates the richness of language use by comparing the number of unique words (types) to the total number of words (tokens) in the text. It is useful, for instance, for analysing speakers’ or writers’ linguistic skills, or the complexity of ideas expressed in documents.\nA common metric for lexical diversity is the Type-Token Ratio (TTR), calculated as: \\[\nTTR = \\frac{N_{\\text{types}}}{N_{\\text{tokens}}}\n\\]\n\ntoks_inaug &lt;- tokens(data_corpus_inaugural)\ndfmat_inaug &lt;- \n  dfm(toks_inaug) |&gt;\n  dfm_remove(pattern = stopwords(\"en\"))  # similar to dfm_select()\n\ntstat_lexdiv &lt;- textstat_lexdiv(dfmat_inaug)\ntail(tstat_lexdiv, 5)\n\n     document       TTR\n55  2005-Bush 0.6176753\n56 2009-Obama 0.6828645\n57 2013-Obama 0.6605238\n58 2017-Trump 0.6409537\n59 2021-Biden 0.5572316\n\n\nWe can prepare a plot using ggplot() as follows:\n\nplot_df &lt;-\n  tstat_lexdiv |&gt;\n  mutate(id = row_number())\n\nggplot(plot_df, aes(id, TTR)) +\n  geom_line() +\n  scale_x_continuous(\n    breaks = plot_df$id,\n    labels = plot_df$document,\n    name = NULL\n  ) +\n  scale_y_continuous(labels = scales::percent) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nDocument/Feature similarity\nDocument/feature similarity is a measure of how alike two documents or sets of features are based on their content. It quantifies the degree to which documents share similar terms, topics, or characteristics.\ntextstat_dist() calculates similarities of documents or features for various measures. The output is compatible with R’s dist(), so hierarchical clustering can be performed without any transformation.\n\ntoks_inaug &lt;- tokens(data_corpus_inaugural)\ndfmat_inaug &lt;- \n  dfm(toks_inaug) |&gt;\n  dfm_remove(pattern = stopwords(\"en\"))  # similar to dfm_select()\n\n# Calculate document similarity\ndist_mat &lt;- textstat_dist(dfmat_inaug)  # using Euclidean distance\ndist_mat[1:3, 1:3]\n\n3 x 3 Matrix of class \"dspMatrix\"\n                1789-Washington 1793-Washington 1797-Adams\n1789-Washington         0.00000        76.13803   141.4072\n1793-Washington        76.13803         0.00000   206.6954\n1797-Adams            141.40721       206.69543     0.0000\n\n\nTo plot this using ggplot(), we rely on the {ggdendro} package.\n\nclust &lt;- hclust(as.dist(dist_mat))  # hierarchical clustering\n\nlibrary(ggdendro)\ndendr &lt;- dendro_data(clust)\nggdendrogram(dendr, rotate = TRUE) \n\n\n\n\n\n\n\n\n\n\nFeature co-occurence matrix\nA feature co-occurrence matrix (FCM) is a square matrix that counts the number of times two features co-occur in the same context, such as within the same document, sentence, or window of text. This is a special object in {quanteda}, but behaves similarly to a DFM. As an example, consider the following:\n\ntibble(\n  doc_id = 1:2,\n  text = c(\"I love Mathematics.\", \"Mathematics is awesome.\")\n) |&gt;\n  corpus() |&gt;\n  tokens(remove_punct = TRUE) |&gt;\n  fcm(context = \"document\")  # by default\n\nFeature co-occurrence matrix of: 5 by 5 features.\n             features\nfeatures      I love Mathematics is awesome\n  I           0    1           1  0       0\n  love        0    0           1  0       0\n  Mathematics 0    0           0  1       1\n  is          0    0           0  0       1\n  awesome     0    0           0  0       0\n\n\nLet’s download the data_corpus_guardian corpus from the {quanteda.corpora} package.\n\ncorp_news &lt;- quanteda.corpora::download(\"data_corpus_guardian\")\n\nWhen a corpus is large, you have to select features of a DFM before constructing a FCM. In the example below, we clean up as follows:\n\nRemove all stopwords and punctuation characters.\nRemove certain patterns that usually describe the publication time and date of articles.\nKeep only terms that occur at least 100 times in the document-feature matrix.\n\n\ntoks_news &lt;- tokens(corp_news, remove_punct = TRUE)\ndfmat_news &lt;- \n  dfm(toks_news) |&gt;\n  dfm_remove(pattern = c(stopwords(\"en\"), \"*-time\", \"updated-*\", \"gmt\", \"bst\", \"|\")) |&gt;\n  dfm_trim(min_termfreq = 100)\n\ntopfeatures(dfmat_news)\n\n      said     people        one        new       also         us        can \n     28413      11169       9884       8024       7901       7091       6972 \ngovernment       year       last \n      6821       6570       6335 \n\nnfeat(dfmat_news)\n\n[1] 4211\n\n\nTo construct an FCM from a DFM (or a tokens object), use fcm(). You can visualise the FCM using a textplot_network() graph as follows:\n\nfcmat_news &lt;- fcm(dfmat_news, context = \"document\")\nfeat &lt;- names(topfeatures(dfmat_news, 30))  # Top 30 features\nfcmat_news_select &lt;- fcm_select(fcmat_news, pattern = feat, selection = \"keep\")\ndim(fcmat_news_select)\n\n[1] 30 30\n\nset.seed(123)\nquanteda.textplots::textplot_network(fcmat_news_select)"
  },
  {
    "objectID": "3-quanteda.html#scaling-and-classification",
    "href": "3-quanteda.html#scaling-and-classification",
    "title": "Quantitative analysis of textual data",
    "section": "Scaling and classification",
    "text": "Scaling and classification\nIn this section we apply mainly unsupervised learning models to textual data. Scaling and classification aim to uncover hidden structures, relationships, and patterns within textual data by placing texts or words on latent scales (scaling) and grouping them into meaningful categories or themes (classification). This process transforms complex, high-dimensional text into more interpretable and actionable insights.\n\nWordfish\nWordfish is a Poisson scaling model of one-dimensional document positions (Slapin and Proksch 2008). This model is used primarily for scaling political texts to position documents (like speeches or manifestos) on a latent dimension, often reflecting ideological or policy positions. The main objective is to identify the relative positioning of texts on a scale (e.g., left-right political spectrum) based on word frequencies.\nLet \\(y_{ij}\\) be the count of word \\(j\\) in document \\(i\\). Then assume \\[\n\\begin{align}\ny_{ij} &\\sim \\operatorname{Poi}(\\lambda_{ij}) \\\\\n\\log (\\lambda_{ij}) &= \\psi_j +\\beta_j \\theta_i\n\\end{align}\n\\]\nIn this example, we will show how to apply Wordfish to the Irish budget speeches from 2010. First, we will create a document-feature matrix. Afterwards, we will run Wordfish.\n\ntoks_irish &lt;- tokens(data_corpus_irishbudget2010, remove_punct = TRUE)\ndfmat_irish &lt;- dfm(toks_irish)\n\n# Run Wordfish model\ntmod_wf &lt;- textmodel_wordfish(dfmat_irish, dir = c(6, 5))\nsummary(tmod_wf)\n\n\nCall:\ntextmodel_wordfish.dfm(x = dfmat_irish, dir = c(6, 5))\n\nEstimated Document Positions:\n                             theta      se\nLenihan, Brian (FF)        1.79403 0.02007\nBruton, Richard (FG)      -0.62160 0.02823\nBurton, Joan (LAB)        -1.13503 0.01568\nMorgan, Arthur (SF)       -0.07841 0.02896\nCowen, Brian (FF)          1.77846 0.02330\nKenny, Enda (FG)          -0.75350 0.02635\nODonnell, Kieran (FG)     -0.47615 0.04309\nGilmore, Eamon (LAB)      -0.58406 0.02994\nHiggins, Michael (LAB)    -1.00383 0.03964\nQuinn, Ruairi (LAB)       -0.92648 0.04183\nGormley, John (Green)      1.18361 0.07224\nRyan, Eamon (Green)        0.14738 0.06321\nCuffe, Ciaran (Green)      0.71541 0.07291\nOCaolain, Caoimhghin (SF) -0.03982 0.03878\n\nEstimated Feature Scores:\n        when      i presented    the supplementary  budget     to   this  house\nbeta -0.1594 0.3177    0.3603 0.1933         1.077 0.03537 0.3077 0.2473 0.1399\npsi   1.6241 2.7239   -1.7958 5.3308        -1.134 2.70993 4.5190 3.4603 1.0396\n       last   april    said     we   could   work    our   way through  period\nbeta 0.2419 -0.1565 -0.8339 0.4156 -0.6138 0.5221 0.6892 0.275  0.6115  0.4985\npsi  0.9853 -0.5725 -0.4514 3.5125  1.0858 1.1151 2.5278 1.419  1.1603 -0.1779\n         of severe economic distress   today   can  report    that\nbeta 0.2777  1.229   0.4237    1.799 0.09141 0.304  0.6196 0.01506\npsi  4.4656 -2.013   1.5714   -4.456 0.83875 1.564 -0.2466 3.83785\n     notwithstanding difficulties   past\nbeta           1.799        1.175 0.4746\npsi           -4.456       -1.357 0.9321\n\n\nThe R output shows the results of the Wordfish model applied to Irish political texts, estimating the ideological positions of various politicians. Each politician is assigned a “theta” value, representing their placement on a latent scale; positive values indicate one end of the spectrum, while negative values indicate the opposite.\nFor example, Brian Lenihan (FF) has a high positive theta, suggesting a strong position on one side, while Joan Burton (LAB) has a negative theta, placing her on the other side. The model also provides feature scores for words (beta values), indicating their importance in distinguishing between these positions. Words with higher absolute beta values, such as “supplementary,” are key in differentiating the ideological content of the texts, while psi values reflect word frequency variance, contributing to the model’s differentiation of document positions.\nWe can plot the results of a fitted scaling model using textplot_scale1d().\n\ntextplot_scale1d(tmod_wf)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe value of 0 for theta in the Wordfish model is not a true zero in an absolute sense. Instead, it serves as a relative reference point on the latent scale. In Wordfish, theta values are relative, meaning they indicate positions along a spectrum where the direction (positive or negative) is determined by the model’s scaling based on the data and specified parameters.\n\n\nThe function also allows you to plot scores by a grouping variable, in this case the party affiliation of the speakers.\n\ntextplot_scale1d(tmod_wf, groups = dfmat_irish$party)\n\n\n\n\n\n\n\n\nFinally, we can plot the estimated word positions and highlight certain features.\n\ntextplot_scale1d(\n  tmod_wf, \n  margin = \"features\", \n  highlighted = c(\"government\", \"global\", \"children\", \n                  \"bank\", \"economy\", \"the\", \"citizenship\",\n                  \"productivity\", \"deficit\")\n)\n\n\n\n\n\n\n\n\nBeta (x-axis) Reflects how strongly a word is associated with the latent dimension (e.g., ideological position). Words with high absolute beta values are more influential in distinguishing between different positions; positive beta values indicate words more associated with one end of the scale, while negative values indicate the opposite.\nPsi (y-axis) Represents the variance in word frequency. Higher psi values suggest that the word occurs with varying frequency across documents, while lower values indicate more consistent usage.\nTherefore, words in the upper right (high beta, high psi) are influential and variably used, indicating key terms that may strongly differentiate between document positions. Words in the lower left (low beta, low psi) are less influential and consistently used, likely serving as common or neutral terms.\nThe plot also helps identify which words are driving the distinctions in the latent scale and how their usage varies across documents.\n\n\nTopic models\nTopic models are statistical models used to identify the underlying themes or topics within a large collection of documents. They analyze word co-occurrences across documents to group words into topics, where each topic is a distribution over words, and each document is a mixture of these topics.\nA common topic model is Latent Dirichlet Allocation (LDA), which assumes that each document contains multiple topics in varying proportions. Topic models help uncover hidden semantic structures in text, making them useful for organizing, summarizing, and exploring large text datasets. In R, we use the {seededlda} package for LDA.\n\n# install.packages(\"seededlda\")\nlibrary(seededlda)\n\nBack to the Guardian data, corp_news. We will select only news articles published in 2016 using corpus_subset() function and the year() function from the {lubridate} package\n\ncorp_news_2016 &lt;- corpus_subset(corp_news, year(date) == 2016)\nndoc(corp_news_2016)\n\n[1] 1959\n\n\nFurther, after removal of function words and punctuation in dfm(), we will only keep the top 20% of the most frequent features (min_termfreq = 0.8) that appear in less than 10% of all documents (max_docfreq = 0.1) using dfm_trim() to focus on common but distinguishing features.\n\n# Create tokens\ntoks_news &lt;- \n  tokens(\n    corp_news_2016, \n    remove_punct = TRUE, \n    remove_numbers = TRUE, \n    remove_symbol = TRUE\n  ) |&gt;\n  tokens_remove(\n    pattern = c(stopwords(\"en\"), \"*-time\", \"updated-*\", \"gmt\", \"bst\")\n  )\n\n# Create DFM\ndfmat_news &lt;- \n  dfm(toks_news) %&gt;% \n  dfm_trim(\n    min_termfreq = 0.8, \n    termfreq_type = \"quantile\",\n    max_docfreq = 0.1, \n    docfreq_type = \"prop\"\n  )\n\nThe LDA is fitted using the code below. Note that k = 10 specifies the number of topics to be discovered. This is an important parameter and you should try a variety of values and validate the outputs of your topic models thoroughly.\n\n# Takes a while to fit!\ntmod_lda &lt;- seededlda::textmodel_lda(dfmat_news, k = 10)\n\nYou can extract the most important terms for each topic from the model using terms(). Each column (topic1, topic2, etc.) lists words that frequently co-occur in the dataset, suggesting a common theme within each topic.\n\nterms(tmod_lda, 10)\n\n      topic1     topic2       topic3         topic4       topic5    topic6    \n [1,] \"syria\"    \"labor\"      \"johnson\"      \"funding\"    \"son\"     \"officers\"\n [2,] \"refugees\" \"australia\"  \"brussels\"     \"housing\"    \"church\"  \"violence\"\n [3,] \"isis\"     \"australian\" \"talks\"        \"nhs\"        \"black\"   \"doctors\" \n [4,] \"military\" \"corbyn\"     \"boris\"        \"income\"     \"love\"    \"prison\"  \n [5,] \"syrian\"   \"turnbull\"   \"benefits\"     \"education\"  \"father\"  \"victims\" \n [6,] \"un\"       \"budget\"     \"summit\"       \"scheme\"     \"felt\"    \"sexual\"  \n [7,] \"islamic\"  \"leadership\" \"negotiations\" \"fund\"       \"parents\" \"abuse\"   \n [8,] \"forces\"   \"shadow\"     \"ireland\"      \"green\"      \"story\"   \"hospital\"\n [9,] \"turkey\"   \"senate\"     \"migrants\"     \"homes\"      \"visit\"   \"criminal\"\n[10,] \"muslim\"   \"coalition\"  \"greece\"       \"businesses\" \"read\"    \"crime\"   \n      topic7      topic8    topic9          topic10     \n [1,] \"oil\"       \"clinton\" \"climate\"       \"sales\"     \n [2,] \"markets\"   \"sanders\" \"water\"         \"apple\"     \n [3,] \"prices\"    \"cruz\"    \"energy\"        \"customers\" \n [4,] \"banks\"     \"hillary\" \"food\"          \"users\"     \n [5,] \"investors\" \"obama\"   \"gas\"           \"google\"    \n [6,] \"shares\"    \"trump's\" \"drug\"          \"technology\"\n [7,] \"trading\"   \"bernie\"  \"drugs\"         \"games\"     \n [8,] \"china\"     \"ted\"     \"environmental\" \"game\"      \n [9,] \"rates\"     \"rubio\"   \"air\"           \"iphone\"    \n[10,] \"quarter\"   \"senator\" \"emissions\"     \"app\"       \n\n\nAs an example, Topic 1 (“syria”, “refugees”, “isis”), is likely related to international conflicts, specifically around Syria and refugee crises. Topic 4 (“funding”, “housing”, “nhs”) is likely related to public services and social welfare issues, such as healthcare and housing. Each topic provides a distinct theme, derived from the words that frequently appear together in the corpus, helping to summarize and understand the main themes in the text.\nYou can then obtain the most likely topics using topics() and save them as a document-level variable.\n\n# assign topic as a new document-level variable\ndfmat_news$topic &lt;- topics(tmod_lda)\n\n# cross-table of the topic frequency\ntable(dfmat_news$topic)\n\n\n topic1  topic2  topic3  topic4  topic5  topic6  topic7  topic8  topic9 topic10 \n    203     222      85     211     249     236     180     186     196     184 \n\n\nIn the seeded LDA, you can pre-define topics in LDA using a dictionary of “seeded” words. For more information, see the {seededlda} package documentation.\n\n\nLatent semantic scaling\nLatent Semantic Scaling (LSS) is a method used to place words or documents on a latent scale that represents an underlying dimension, such as sentiment, ideology, or any other semantic axis. The key idea is to use the co-occurrence patterns of words across documents to identify and position items along this hidden dimension.\nLSS is performed using the {LSX} package. In this example, we will apply LSS to the corpus of Sputnik articles about Ukraine. First, we prepare the data set.\n\n# Read the RDS file directly from the URL\ncorp &lt;- readRDS(url(\"https://www.dropbox.com/s/abme18nlrwxgmz8/data_corpus_sputnik2022.rds?dl=1\"))\n\ntoks &lt;-\n  corp |&gt;\n  corpus_reshape(\"sentences\") |&gt;  # this is a must!\n  tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE\n  )\n\ndfmt &lt;-\n  dfm(toks) |&gt;\n  dfm_remove(pattern = stopwords(\"en\"))\n\nNow to run an LSS model, run the following command:\n\nlss &lt;- textmodel_lss(\n  dfmt, \n  seeds = as.seedwords(data_dictionary_sentiment), \n  k = 300, \n  # cache = TRUE, \n  include_data = TRUE, \n  group_data = TRUE\n)\n\nTaking the DFM and the seed words as the only inputs, textmodel_lss() computes the polarity scores of all the words in the corpus based on their semantic similarity to the seed words. You usually do not need to change the value of k (300 by default).\nLet’s look at the output of the LSS model:\n\nsummary(lss)\n\n\nCall:\ntextmodel_lss(x = dfmt, seeds = as.seedwords(data_dictionary_sentiment), \n    k = 300, include_data = TRUE, group_data = TRUE)\n\nSeeds:\n       good        nice   excellent    positive   fortunate     correct \n          1           1           1           1           1           1 \n   superior         bad       nasty        poor    negative unfortunate \n          1          -1          -1          -1          -1          -1 \n      wrong    inferior \n         -1          -1 \n\nBeta:\n(showing first 30 elements)\n          excellent            positive              gander                good \n             0.2102              0.1918              0.1883              0.1829 \n         diplomatic           staffer's              ex-kgb             correct \n             0.1802              0.1773              0.1771              0.1749 \n               mend  inter-parliamntary         workmanship      russo-american \n             0.1743              0.1719              0.1715              0.1713 \n             nicety           china-u.s               soufi good-neighborliness \n             0.1712              0.1674              0.1664              0.1644 \n       china-canada             fidgety           relations           downtrend \n             0.1616              0.1611              0.1584              0.1578 \n            cordial        canada-china            superior       understanding \n             0.1573              0.1569              0.1569              0.1561 \n   good-neighbourly           brennan's              mutual          reaffirmed \n             0.1550              0.1544              0.1538              0.1534 \n              blida          abdelkader \n             0.1533              0.1533 \n\nData Dimension:\n[1]  8063 59711\n\n\nPolarity scores in Latent Semantic Scaling (LSS) quantify how words or documents relate to a specific dimension (e.g., sentiment) based on predefined seed words. Seed words represent the extremes of this dimension (e.g., “good” vs. “bad”). LSS analyzes how other words co-occur with these seed words to assign a score.\nWe can visualize the polarity of words using textplot_terms(). If you pass a dictionary to be highlighted, words are plotted in different colors. data_dictionary_LSD2015 is a widely-used sentiment dictionary. If highlighted = NULL, words are selected randomly to highlight.\n\ntextplot_terms(lss, highlighted = data_dictionary_LSD2015[1:2])\n\n\n\n\n\n\n\n\nBased on the fitted model, we can now predict polarity scores of documents using predict(). It is best to work with the document-level data frame, which we will then add a new column for the predicted polarity scores.\n\ndat &lt;- docvars(lss$data)\ndat$lss &lt;- predict(lss)\nglimpse(dat)\n\nRows: 8,063\nColumns: 5\n$ head &lt;chr&gt; \"Biden: US Desires Diplomacy But 'Ready No Matter What Happens' I…\n$ url  &lt;chr&gt; \"https://sputniknews.com/20220131/biden-us-desires-diplomacy-but-…\n$ time &lt;dttm&gt; 2022-02-01 03:25:22, 2022-02-01 01:58:19, 2022-02-01 01:47:56, 2…\n$ date &lt;date&gt; 2022-01-31, 2022-01-31, 2022-01-31, 2022-01-31, 2022-01-31, 2022…\n$ lss  &lt;dbl&gt; 0.10093989, 0.99263241, -0.76609160, 0.15991318, 0.25420329, -1.5…\n\n\nBasically what we have is a data frame, where each row represents a single document (here, a news item from Sputnik with a timestamp). Each document also has a predicted polarity score based on the LSS model. We can visualise this easily using ggplot(). But first, we need to smooth the scores using smooth_lss() (otherwise it is too rough to interpret).\n\ndat_smooth &lt;- smooth_lss(dat, lss_var = \"lss\", date_var = \"date\")\n\nggplot(dat_smooth, aes(x = date, y = fit)) + \n  geom_line() +\n  geom_ribbon(\n    aes(ymin = fit - se.fit * 1.96, \n        ymax = fit + se.fit * 1.96), \n    alpha = 0.1\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-02-24\"), linetype = \"dotted\") +\n  scale_x_date(date_breaks = \"months\", date_labels = \"%b %Y\", name = NULL) +\n  labs(title = \"Sentiment about Ukraine\", x = \"Date\", y = \"Sentiment\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThe plot shows that the sentiment of the articles about Ukraine became more negative in March but more positive in April. Zero on the Y-axis is the overall mean of the score; the dotted vertical line indicate the beginning of the war."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "The three topics for this year’s special lectures are:\n\nWeb scraping and modelling\nGIS and spatial data\nQuantitative text analysis\n\nYou will find the lecture notes and materials for each topic by clicking on the menu button. These lectures are meant to be more “hands-on” than the previous R1-4 lectures, so be prepared to get your hands dirty with some code!\nYou may also browse the GitHub repo for the special lecture notes. In particular, you will find .qmd files, which you can open in RStudio. Best to clone the repo to your local computers and open the special-lectures.Rproj file in RStudio. You will find the source code exactly as it is displayed in the html files. You can also turn on “visual” mode in RStudio to see a friendlier version of the .qmd files."
  },
  {
    "objectID": "2-gis_data.html#introduction",
    "href": "2-gis_data.html#introduction",
    "title": "Geographical Information System (GIS) data",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nWhat we’ll learn\n\n\n\n\nTypes of GIS data and how these are handled in R.\nDifference between spatial and non-spatial data analysis.\nImportance of geocoding your data for spatial analysis.\n\n\n\nRoughly speaking, there are 4 types of GIS data.\n\nPoints\n\nHaving \\((X, Y)\\) coordinates (latitude, longitude, or projected coordinates, and are “zero-dimensional”.\nE.g. shopping malls, hospitals, outbreaks, etc.\n\nLines\n\nA collection of points that form a path or a boundary. Has length.\nE.g. roads, rivers, pipelines, etc.\n\nPolygons\n\nA closed area made up of line segments or curves.\nE.g. countries, districts, buildings, etc.\n\nRaster\n\nPixelated (or gridded) data where each pixel is associated with a geographical area and some measurement.\nE.g. satellite images, elevation data, etc.\n\n\nThe first three are usually referred to as vector data. GIS data can be stored in various formats such as .shp or .geojson. The handling of GIS data (at least vector type data) is facilitated by the {sf} package (Pebesma and Bivand 2023) which uses the simple features standard.\n\n\n\n\n\n\nNote\n\n\n\nSimple features refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects.\n\n\nIt’s helpful to think about the shape of this spatial data set. As an example, here’s a random slice of 10 kampong-level population data for Brunei:\n\nleft_join(\n  kpg_sf, \n  bn_census2021, \n  join_by(id, kampong, mukim, district)\n) |&gt;\n  select(\n    kampong, district, population, geometry\n  ) |&gt;\n  slice_sample(n = 10)\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 114.4073 ymin: 4.31476 xmax: 115.1193 ymax: 4.996524\nGeodetic CRS:  WGS 84\n# A tibble: 10 × 4\n   kampong                  district     population                     geometry\n   &lt;chr&gt;                    &lt;chr&gt;             &lt;dbl&gt;                &lt;POLYGON [°]&gt;\n 1 Kg. Ikas                 Tutong               92 ((114.803 4.861238, 114.802…\n 2 Kg. Sungai Tali          Belait             1051 ((114.4262 4.657554, 114.42…\n 3 Kg. Parit Amo            Temburong           227 ((115.1188 4.646732, 115.11…\n 4 Kg. Simpang Tiga         Belait               NA ((114.4682 4.382654, 114.46…\n 5 STKRJ Lambak Kiri        Brunei Muara       2296 ((114.944 4.995923, 114.944…\n 6 Hutan Simpan Bukit Ladan Tutong               NA ((114.7998 4.656117, 114.79…\n 7 Hutan Simpan Andulau     Tutong               NA ((114.6287 4.711252, 114.62…\n 8 Paya Pekan Tutong        Tutong               NA ((114.6965 4.800737, 114.69…\n 9 Kg. Binchaya             Tutong                6 ((114.5776 4.724263, 114.57…\n10 Kg. Lambak B             Brunei Muara       3760 ((114.9459 4.953047, 114.94…\n\n\nSpatial data analysis must have these two components:\n\nThe study variables (in the above example, this is population data).\nGIS data regarding that study variable.\n\nIf we only have 1 without 2, then it really is just a regular data analysis (stating the obvious). Adding the GIS data is a process called “geocoding” the data points.\n\n\n\n\n\n\nNote\n\n\n\nIn R, geocoding using {tidyverse} can be achieved using the dplyr::left_join() or similar xxx_join() family of functions."
  },
  {
    "objectID": "2-gis_data.html#multipoint-data",
    "href": "2-gis_data.html#multipoint-data",
    "title": "Geographical Information System (GIS) data",
    "section": "(MULTI)POINT data",
    "text": "(MULTI)POINT data\n\n\n\n\n\n\nWhat we’ll learn\n\n\n\n\nLoading data sets in R using readr::read_csv().\nIdentifying data types and their implications.\n\n\n\nUse the data from Jaafar and Sukri (2023) on the physicochemical characteristics and texture classification of soil in Bornean tropical heath forests affected by exotic Acacia mangium. There are three datasets provided.\n\nGIS data (WGS84 coordinates) of all study plots.\nSoil physicochemical property data. This contains details of soil physical, chemical, nutrient concentration of the three habits studied.\nSoil texture classification. Provides details on the classification of the soil texture in the habitats studied.\n\nWe will first load the data sets in R.\n\n# Load the data sets\nsoil_gps &lt;- read_csv(\n  \"data/8389823/GPS - Revised.csv\", \n  # IMPORTANT!!! The csv file has latin1 encoding as opposed to UTF-8\n  locale = readr::locale(encoding = \"latin1\")\n)\n  \nsoil_physico &lt;- read_csv(\"data/8389823/Soil physicochemical properties.csv\")\nsoil_texture &lt;- read_csv(\"data/8389823/Soil texture classification.csv\")\n\n\nClean up the point data\n\n\n\n\n\n\nWhat we’ll learn\n\n\n\n\nHighlighting the need for cleaning and preprocessing data.\nUsing glimpse() to peek at the data.\nUsing mutate() to change stuff in the data set.\nUsing str() to look at the structure of an R object.\n\n\n\nLet’s take a look at the point data set.\n\nglimpse(soil_gps)\n\nRows: 18\nColumns: 5\n$ Forest_type  &lt;chr&gt; \"Kerangas\", \"Kerangas\", \"Kerangas\", \"Kerangas\", \"Kerangas…\n$ Habitat_type &lt;chr&gt; \"Intact\", \"Intact\", \"Intact\", \"Intact\", \"Intact\", \"Intact…\n$ Plot_name    &lt;chr&gt; \"KU1\", \"KU2\", \"KU3\", \"KU4\", \"KU5\", \"KU6\", \"KI1\", \"KI2\", \"…\n$ Latitude     &lt;chr&gt; \"4° 35' 53.40\\\"N\", \"4° 35' 38.37\\\"N\", \"4° 35' 53.89\\\"N\", …\n$ Longitude    &lt;chr&gt; \"114° 30' 39.09\\\"E\", \"114° 31' 05.89\\\"E\", \"114° 30' 38.90…\n\n\nThe first three columns are essentially the identifiers of the plots (forest type, habitat type, and the unique identification code for the study plot). However, the latitude and longitude needs a bit of cleaning up, because it’s currently in character format. This needs to be in a formal Degree Minute Second DMS class that R can understand. For this we will use the sp::char2dms() function.\nAs an example let’s take a look at the first latitude.\n\nx &lt;- soil_gps$Latitude[1]\nx\n\n[1] \"4° 35' 53.40\\\"N\"\n\n# convert it using sp::char2dms() function\nx &lt;- sp::char2dms(x, chd = \"°\")\nx\n\n[1] 4d35'53.4\"N\n\nstr(x)\n\nFormal class 'DMS' [package \"sp\"] with 5 slots\n  ..@ WS : logi FALSE\n  ..@ deg: int 4\n  ..@ min: int 35\n  ..@ sec: num 53.4\n  ..@ NS : logi TRUE\n\n\nThis is a special class that R understands as being a latitude from Earth. To convert it to decimal, we just do as.numeric():\n\nas.numeric(x)\n\n[1] 4.598167\n\n\nNow let’s do this for all the values in the soil_gps data. We will use the dplyr::mutate() function in a pipeline.\n\nsoil_gps &lt;-\n  soil_gps |&gt;\n  mutate(\n    Latitude = as.numeric(sp::char2dms(Latitude, chd = \"°\")),\n    Longitude = as.numeric(sp::char2dms(Longitude, chd = \"°\"))\n  )\nsoil_gps\n\n# A tibble: 18 × 5\n   Forest_type Habitat_type Plot_name Latitude Longitude\n   &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 Kerangas    Intact       KU1           4.60      115.\n 2 Kerangas    Intact       KU2           4.59      115.\n 3 Kerangas    Intact       KU3           4.60      115.\n 4 Kerangas    Intact       KU4           4.63      114.\n 5 Kerangas    Intact       KU5           4.60      115.\n 6 Kerangas    Intact       KU6           4.60      115.\n 7 Kerangas    Invaded      KI1           4.59      115.\n 8 Kerangas    Invaded      KI2           4.59      115.\n 9 Kerangas    Invaded      KI3           4.59      115.\n10 Kerangas    Invaded      KI4           4.59      115.\n11 Kerangas    Invaded      KI5           4.59      115.\n12 Kerangas    Invaded      KI6           4.59      115.\n13 Kerangas    Plantation   AP1           4.59      115.\n14 Kerangas    Plantation   AP2           4.59      115.\n15 Kerangas    Plantation   AP3           4.59      115.\n16 Kerangas    Plantation   AP4           4.59      115.\n17 Kerangas    Plantation   AP5           4.59      115.\n18 Kerangas    Plantation   AP6           4.59      115.\n\n\n\n\nPreliminary plot of the data\n\n\n\n\n\n\nWhat we’ll learn\n\n\n\n\nStructure of a ggplot() (grammar of graphics).\nUsing geom_sf() to plot the GIS data, and adding points using geom_point().\n\n\n\nUsing the data contained in the {bruneimap} package, we can plot the study areas on a map of Brunei. Use either the brn_sf, dis_sf, mkm_sf or kpg_sf data sets.\n\nggplot(brn_sf) +\n  geom_sf() +\n  geom_point(data = soil_gps, aes(Longitude, Latitude)) \n\n\n\n\n\n\n\n\nWe can zoom in a bit… but we have to find out manually the correct bounding box. To do this, we can either:\n\nManually find the minimum and maximum values of the latitude and longitude.\nConvert the soil_gps data set to an sf object and use the st_bbox() function.\n\n\n# Manual way\nc(\n  xmin = min(soil_gps$Longitude), xmax = max(soil_gps$Longitude),\n  ymin = min(soil_gps$Latitude), ymax = max(soil_gps$Latitude)\n)\n\n      xmin       xmax       ymin       ymax \n114.473356 114.529297   4.592817   4.630242 \n\n# Using the sf object\nsoil_sf &lt;- st_as_sf(soil_gps, coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\nst_bbox(soil_sf)\n\n      xmin       ymin       xmax       ymax \n114.473356   4.592817 114.529297   4.630242 \n\n\nNow that we’ve found the bound box, we can plot better:\n\nggplot(mkm_sf) +\n  geom_sf() +\n  geom_sf(data = dis_sf, fill = NA, col = \"black\", linewidth = 1) +\n  geom_point(data = soil_gps, aes(Longitude, Latitude)) +\n  geom_text_repel(\n    data = soil_gps,\n    aes(Longitude, Latitude, label = Plot_name),\n    box.padding = 0.5,\n    max.overlaps = 30\n  ) +\n  coord_sf(\n    xlim = c(114.4, 114.6),\n    ylim = c(4.5, 4.7)\n  )\n\n\n\n\n\n\n\n\n\n\nMerge with the study data\n\n\n\n\n\n\nWhat we’ll learn\n\n\n\n\nUsing left_join() to merge two data sets together.\nUsing geom_jitter() to plot the study variables that are overlapping.\n\n\n\nLet’s take a look at the data set.\n\nglimpse(soil_physico)\n\nRows: 144\nColumns: 16\n$ Habitat_type              &lt;chr&gt; \"Intact\", \"Intact\", \"Intact\", \"Intact\", \"Int…\n$ Plot_name                 &lt;chr&gt; \"KU1\", \"KU1\", \"KU1\", \"KU1\", \"KU1\", \"KU1\", \"K…\n$ Subplot_name              &lt;chr&gt; \"A\", \"A\", \"B\", \"B\", \"C\", \"C\", \"D\", \"D\", \"A\",…\n$ Soil_depth                &lt;chr&gt; \"0-15\", \"30-50\", \"0-15\", \"30-50\", \"0-15\", \"3…\n$ Nitrogen                  &lt;dbl&gt; 0.617, 0.188, 0.663, 0.200, 0.465, 0.255, 0.…\n$ Phosphorus                &lt;dbl&gt; 0.248, 0.129, 0.259, 0.295, 0.172, 0.145, 0.…\n$ Magnesium                 &lt;dbl&gt; 0.000, 0.045, 0.054, 0.035, 0.079, 0.043, 0.…\n$ Calcium                   &lt;dbl&gt; 0.167, 0.187, 0.148, 0.113, 0.253, 0.229, 0.…\n$ Potassium                 &lt;dbl&gt; 0.059, 0.037, 0.054, 0.022, 0.098, 0.033, 0.…\n$ Exchangable_magnesium     &lt;dbl&gt; 0.009, 0.004, 0.007, 0.005, 0.029, 0.014, 0.…\n$ Exchangable_calcium       &lt;dbl&gt; 0.010, 0.009, 0.008, 0.009, 0.109, 0.041, 0.…\n$ Exchangable_potassium     &lt;dbl&gt; 0.101, 0.085, 0.092, 0.087, 0.101, 0.090, 0.…\n$ Available_phosphorus      &lt;dbl&gt; 0.012, 0.012, 0.013, 0.012, 0.013, 0.014, 0.…\n$ pH                        &lt;dbl&gt; 2.3, 2.7, 2.0, 2.0, 2.6, 2.5, 2.3, 2.1, 1.0,…\n$ Gravimetric_water_content &lt;dbl&gt; 5.911, 3.560, 10.860, 5.082, 6.963, 4.549, 5…\n$ Organic_matter            &lt;dbl&gt; 4.559, 1.399, 4.523, 2.309, 3.131, 2.209, 3.…\n\nglimpse(soil_texture)\n\nRows: 144\nColumns: 8\n$ Habitat_type           &lt;chr&gt; \"Intact\", \"Intact\", \"Intact\", \"Intact\", \"Intact…\n$ Plot_name              &lt;chr&gt; \"KU1\", \"KU1\", \"KU1\", \"KU1\", \"KU2\", \"KU2\", \"KU2\"…\n$ Subplot_name           &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"A\", \"B\", \"C\", \"D\", \"A\", \"B…\n$ Soil_depth             &lt;chr&gt; \"0-15\", \"0-15\", \"0-15\", \"0-15\", \"0-15\", \"0-15\",…\n$ Clay                   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 2.5, 2.5, 2.5, 0.0, 2.…\n$ Silt                   &lt;dbl&gt; 2.5, 0.0, 0.0, 2.5, 0.0, 0.0, 2.5, 2.5, 7.5, 7.…\n$ Sand                   &lt;dbl&gt; 97.5, 100.0, 100.0, 97.5, 100.0, 97.5, 95.0, 95…\n$ Texture_classification &lt;chr&gt; \"Sand\", \"Sand\", \"Sand\", \"Sand\", \"Sand\", \"Sand\",…\n\n\nThe soil_physico and soil_texture data sets contain the same columns, so we might as well merge them together. We will use the dplyr::left_join() function.\n\n# Actually I just want to merge these two together\nsoil_df &lt;- left_join(\n  soil_physico,\n  soil_texture,\n  by = join_by(Habitat_type, Plot_name, Subplot_name, Soil_depth)\n)\nsoil_df\n\n# A tibble: 144 × 20\n   Habitat_type Plot_name Subplot_name Soil_depth Nitrogen Phosphorus Magnesium\n   &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 Intact       KU1       A            0-15          0.617      0.248     0    \n 2 Intact       KU1       A            30-50         0.188      0.129     0.045\n 3 Intact       KU1       B            0-15          0.663      0.259     0.054\n 4 Intact       KU1       B            30-50         0.2        0.295     0.035\n 5 Intact       KU1       C            0-15          0.465      0.172     0.079\n 6 Intact       KU1       C            30-50         0.255      0.145     0.043\n 7 Intact       KU1       D            0-15          0.285      0.225     0.052\n 8 Intact       KU1       D            30-50         0.057      0.207     0.031\n 9 Intact       KU2       A            0-15          0.37       0.135     0.038\n10 Intact       KU2       A            30-50         0.114      0.168     0.021\n# ℹ 134 more rows\n# ℹ 13 more variables: Calcium &lt;dbl&gt;, Potassium &lt;dbl&gt;,\n#   Exchangable_magnesium &lt;dbl&gt;, Exchangable_calcium &lt;dbl&gt;,\n#   Exchangable_potassium &lt;dbl&gt;, Available_phosphorus &lt;dbl&gt;, pH &lt;dbl&gt;,\n#   Gravimetric_water_content &lt;dbl&gt;, Organic_matter &lt;dbl&gt;, Clay &lt;dbl&gt;,\n#   Silt &lt;dbl&gt;, Sand &lt;dbl&gt;, Texture_classification &lt;chr&gt;\n\n\nOnce we’ve done that, the soil_df data set (the study variables) is actually missing the spatial data. We need to geocode it with the soil_gps data set. Again, dplyr::left_join() to the rescue!\n\nsoil_df &lt;- left_join(\n  soil_df, \n  soil_gps,\n  by = join_by(Habitat_type, Plot_name)\n)\n\nNow we’re in a position to plot the study variables on the map. Note that there are only 18 plots in the soil_gps data set, and each plot has repeated measurements. That means when we plot it, it will overlap and look like a single point. So a good thing to do is to jitter the point so it’s easier to see.\n\nggplot(kpg_sf) +\n  geom_sf(fill = NA) +\n  geom_jitter(\n    data = soil_df, \n    aes(Longitude, Latitude, col = Nitrogen, size = Nitrogen, \n        shape = Habitat_type),\n    width = 0.001, height = 0.001, alpha = 0.7\n  ) +\n  coord_sf(\n    xlim = c(114.46, 114.54),\n    ylim = c(4.58, 4.64)\n  ) +\n  scale_color_viridis_c() +\n  guides(size = \"none\")"
  },
  {
    "objectID": "2-gis_data.html#line-data-multilinestring",
    "href": "2-gis_data.html#line-data-multilinestring",
    "title": "Geographical Information System (GIS) data",
    "section": "Line data ((MULTI)LINESTRING)",
    "text": "Line data ((MULTI)LINESTRING)\n\n\n\n\n\n\nWhat we’ll learn\n\n\n\n\nHow to load spatial data sets using sf::read_sf() and editing the CRS using sf::st_transform().\nHow to filter data using dplyr::filter().\nHow to plot line data using ggplot2::geom_sf().\n\n\n\nFor this example, we’ll play with the road network shape file obtained from OpenStreetMaps. The data is in geojson format, so let’s import that into R.\n\nbrd &lt;- \n  read_sf(\"data/hotosm_brn_roads_lines_geojson/hotosm_brn_roads_lines_geojson.geojson\") |&gt;\n  st_transform(4326)  # SET THE CRS!!! (WGS84)\nglimpse(brd)\n\nRows: 25,570\nColumns: 15\n$ name       &lt;chr&gt; \"Simpang 393\", \"Simpang 405\", NA, NA, NA, NA, \"Lebuhraya Tu…\n$ `name:en`  &lt;chr&gt; NA, NA, NA, NA, NA, NA, \"Tutong–Telisai Highway\", NA, NA, N…\n$ highway    &lt;chr&gt; \"residential\", \"residential\", \"service\", \"residential\", \"tr…\n$ surface    &lt;chr&gt; NA, NA, NA, NA, NA, \"asphalt\", \"asphalt\", NA, NA, NA, \"asph…\n$ smoothness &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ width      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ lanes      &lt;chr&gt; NA, NA, NA, NA, NA, \"1\", \"2\", NA, NA, NA, \"2\", NA, NA, NA, …\n$ oneway     &lt;chr&gt; NA, NA, NA, NA, NA, \"yes\", \"yes\", NA, NA, NA, \"no\", \"yes\", …\n$ bridge     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ layer      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ source     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `name:ms`  &lt;chr&gt; NA, NA, NA, NA, NA, NA, \"Lebuhraya Tutong–Telisai\", NA, NA,…\n$ osm_id     &lt;int&gt; 386886618, 481030903, 512405939, 664532755, 442044892, 6651…\n$ osm_type   &lt;chr&gt; \"ways_line\", \"ways_line\", \"ways_line\", \"ways_line\", \"ways_l…\n$ geometry   &lt;LINESTRING [°]&gt; LINESTRING (114.6236 4.7910..., LINESTRING (114.…\n\n\nThere are 25,570 features in this data set, which may be a bit too much. Let’s try to focus on the major roads only. This information seems to be contained in the highway column. What’s in it?\n\ntable(brd$highway)\n\n\n     bridleway   construction       cycleway        footway  living_street \n             1             28             73            898             10 \n      motorway  motorway_link           path     pedestrian        primary \n           116            152            140             60            865 \n  primary_link    residential           road      secondary secondary_link \n           332           9023              1            446             79 \n       service          steps       tertiary  tertiary_link          track \n          9876             53            586             59            442 \n         trunk     trunk_link   unclassified \n           460            310           1560 \n\n\nAccording to this wiki, In OpenStreetMap, the major roads of a road network are sorted on an importance scale, from motorway to quaternary road.\n\n\nbrd_mjr &lt;- \n  brd |&gt;\n  filter(highway %in% c(\"motorway\", \"trunk\", \"primary\", \"secondary\")) \nbrd_mjr\n\nSimple feature collection with 1887 features and 14 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 114.1906 ymin: 4.516642 xmax: 115.2021 ymax: 5.037115\nGeodetic CRS:  WGS 84\n# A tibble: 1,887 × 15\n   name     `name:en` highway surface smoothness width lanes oneway bridge layer\n * &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;\n 1 Lebuhra… Tutong–T… trunk   asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    &lt;NA&gt;   &lt;NA&gt; \n 2 Lebuhra… Tutong–T… trunk   asphalt &lt;NA&gt;       &lt;NA&gt;  3     yes    &lt;NA&gt;   &lt;NA&gt; \n 3 Jalan S… &lt;NA&gt;      primary asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    yes    1    \n 4 Jalan S… &lt;NA&gt;      primary asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    &lt;NA&gt;   &lt;NA&gt; \n 5 Lebuh R… Seria–Be… trunk   asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    &lt;NA&gt;   &lt;NA&gt; \n 6 &lt;NA&gt;     &lt;NA&gt;      trunk   asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    &lt;NA&gt;   &lt;NA&gt; \n 7 &lt;NA&gt;     &lt;NA&gt;      primary asphalt &lt;NA&gt;       &lt;NA&gt;  1     yes    &lt;NA&gt;   &lt;NA&gt; \n 8 Lebuh R… Seria–Be… trunk   asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    yes    1    \n 9 &lt;NA&gt;     &lt;NA&gt;      primary asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    &lt;NA&gt;   &lt;NA&gt; \n10 Lebuhra… Telisai–… trunk   asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    &lt;NA&gt;   &lt;NA&gt; \n# ℹ 1,877 more rows\n# ℹ 5 more variables: source &lt;chr&gt;, `name:ms` &lt;chr&gt;, osm_id &lt;int&gt;,\n#   osm_type &lt;chr&gt;, geometry &lt;LINESTRING [°]&gt;\n\n\nAnd now a plot of these roads.\n\nggplot() +\n  geom_sf(data = brn_sf) +\n  geom_sf(data = brd_mjr, aes(col = highway), size = 0.5) +\n  # scale_colour_viridis_d(option = \"turbo\")\n  ggsci::scale_colour_npg()\n\n\n\n\n\n\n\n\nWith this, I asked ChatGPT what kind of spatial analyses can be done on this data set. It said, when paired with appropriate data, we can do things like:\n\nNetwork Connectivity Analysis\n\nAssess reachability and identify disconnected road network components.\n\nAccessibility and Service Area Analysis\n\nDetermine service areas and catchment areas for essential services.\n\nTraffic Simulation and Management\n\nSimulate traffic flow to identify bottlenecks and suggest optimal routing.\n\nEnvironmental Impact Assessment\n\nEstimate vehicular emissions and model noise pollution from roads.\n\nUrban and Regional Planning\n\nExamine land use compatibility and assess infrastructure development needs.\n\nSafety Analysis\n\nIdentify accident hotspots and assess pedestrian safety.\n\nEconomic Analysis\n\nEvaluate economic accessibility and the impact of road projects.\n\n\nLet’s pick one of these: Calculate the distance between the centroid of several regions and the major hospital in the Belait district. This analysis guides urban and healthcare planning by pinpointing areas with inadequate access to emergency services, enabling targeted infrastructure and service improvements.\n\nRoad networks in Belait region\n\n\n\n\n\n\nWhat we’ll learn\n\n\n\n\nManipulating GIS data using sf::st_intersection() and the like. Useful for reorganising the spatial structure (without having to do this in QGIS or ArcGIS).\nSampling points from a line data set.\nCalculating distances between points and lines using {osrm} package.\n\n\n\nFirst we “crop” the road network to the Belait region.\n\nbrd_belait &lt;- st_intersection(\n  brd,\n  filter(dis_sf, name == \"Belait\")\n)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nggplot(brd_belait) +\n  geom_sf() +\n  geom_sf(data = filter(dis_sf, name == \"Belait\"), fill = NA)\n\n\n\n\n\n\n\n\nIf we were to sample random points from the Belait polygon, we might get non-sensical areas like the extremely rural areas or forest reserves. So the idea is to sample random points from the road network itself. For this, we need a function that will get us a random point on the path itself.\n\nget_random_point &lt;- function(linestring) {\n  coords &lt;- st_coordinates(linestring)\n  samp_coord &lt;- coords[sample(nrow(coords), 1), , drop = FALSE]\n  samp_coord[, 1:3]\n}\nget_random_point(brd_belait$geometry[1])\n\n         X          Y         L1 \n114.241433   4.594193   1.000000 \n\n\nOnce we have this function, we need to map() this function onto each of the linestrings in the brd_belait data set. The resulting list of points is too large! So we will just sample 100 points (you can experiment with this number).\n\nrandom_points &lt;-\n  map(brd_belait$geometry, get_random_point) |&gt;\n  bind_rows() |&gt;\n  slice_sample(n = 100)\n\nWhat we have now is a data frame of 100 random points on the road network in the Belait district. We will use the {osrm} package to calculate the distance between these points and the Suri Seri Begawan Hospital in Kuala Belait. The output will be three things: 1) The duration (minutes); 2) The distance (km); and 3) a LINESTRING object that represents the path to get to the hospital. Unfortunately the osrmRoute() function is not vectorised, i.e. we have to do it one-by-one for each of the 100 points. Luckily, we can just make a for loop and store the results in a list.\n\nsuriseri &lt;- c(114.198778, 4.583444)\n\nres &lt;- list()\nfor (i in 1:100) {\n  res[[i]] &lt;- osrmRoute(src = random_points[i, 1:2], dst = suriseri, overview = \"full\")\n}\nres &lt;- \n  bind_rows(res) |&gt;\n  as_tibble() |&gt;\n  st_as_sf()\nres\n\nSimple feature collection with 100 features and 4 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 114.1735 ymin: 4.43039 xmax: 114.7124 ymax: 4.70454\nGeodetic CRS:  WGS 84\n# A tibble: 100 × 5\n   src   dst   duration distance                                        geometry\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;                                &lt;LINESTRING [°]&gt;\n 1 1     dst      15.0     10.8  (114.2887 4.60486, 114.2887 4.60486, 114.2885 …\n 2 1     dst       6.91     5.03 (114.2385 4.59257, 114.2384 4.59313, 114.2383 …\n 3 1     dst       5.23     3.50 (114.2252 4.58684, 114.2252 4.58689, 114.2252 …\n 4 1     dst      31.2     36.5  (114.464 4.65639, 114.4641 4.65643, 114.4642 4…\n 5 1     dst      19.9     20.0  (114.324 4.60102, 114.324 4.60102, 114.3242 4.…\n 6 1     dst       6.30     4.58 (114.235 4.59267, 114.2349 4.59327, 114.2348 4…\n 7 1     dst      20.2     15.9  (114.315 4.60384, 114.3152 4.6034, 114.3137 4.…\n 8 1     dst      11.8      8.13 (114.2626 4.59433, 114.2626 4.59433, 114.2626 …\n 9 1     dst      11.8     10.7  (114.2673 4.56541, 114.2673 4.56541, 114.267 4…\n10 1     dst      33.8     37.2  (114.473 4.67402, 114.473 4.67402, 114.4734 4.…\n# ℹ 90 more rows\n\n\nSo with all that done, we can now plot the paths taken by the 100 random points to the hospital. The map gives us an indication of which areas are underserved by the hospital, and can guide urban and healthcare planning by pinpointing areas with inadequate access to emergency services, enabling targeted infrastructure and service improvements.\n\nggplot(res) +\n  # geom_point(data = random_points, aes(x = X, y = Y), col = \"red\") +\n  geom_sf(data = filter(kpg_sf, district == \"Belait\"), fill = NA) +\n  geom_sf(aes(col = duration), linewidth = 1.2, alpha = 0.7) +\n  geom_point(x = suriseri[1], y = suriseri[2], col = \"red3\", pch = \"X\", \n             size = 3) +\n  scale_colour_viridis_c() \n\n\n\n\n\n\n\n\nImproving the analysis\n\nWeight analysis by populous areas. Outcalls to hospitals can be modelled using a Poisson distribution with the population as the rate parameter.\nUse a more sophisticated routing algorithm that accounts for traffic conditions and road quality (am vs pm, weekends vs weekdays, etc.).\nSimpler to analyse at the kampong or mukim level?"
  },
  {
    "objectID": "2-gis_data.html#areal-data-multipolygons",
    "href": "2-gis_data.html#areal-data-multipolygons",
    "title": "Geographical Information System (GIS) data",
    "section": "Areal data ((MULTI)POLYGONS)",
    "text": "Areal data ((MULTI)POLYGONS)\n\n\n\n\n\n\nWhat we’ll learn\n\n\n\n\nRepresent statistical data using colour mapping symbology (choropleth)\nUse ggplot2::geom_label() or ggrepel::geom_label_repel() to add labels to the map\nUsing a binned colour scale, e.g. ggplot2::geom_scale_fill_viridis_b()\n\n\n\nWhen your study data is made up a finite number of non-overlapping areas, then you can represent them as polygons in R. This is the case for the kampong and mukim data in Brunei. As an example, let us look at the population of each kampong in Brunei. This dataset comes from the 2021 Brunei Census data (DEPS 2022)\n\nglimpse(bn_census2021)\n\nRows: 365\nColumns: 11\n$ id           &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 2…\n$ kampong      &lt;chr&gt; \"Kg. Biang\", \"Kg. Amo\", \"Kg. Sibut\", \"Kg. Sumbiling Baru\"…\n$ mukim        &lt;chr&gt; \"Mukim Amo\", \"Mukim Amo\", \"Mukim Amo\", \"Mukim Amo\", \"Muki…\n$ district     &lt;chr&gt; \"Temburong\", \"Temburong\", \"Temburong\", \"Temburong\", \"Temb…\n$ population   &lt;dbl&gt; 75, 394, 192, 91, 108, 143, 199, 123, 95, 90, 92, 2427, 4…\n$ pop_male     &lt;dbl&gt; 46, 218, 98, 48, 60, 68, 115, 65, 52, 46, 73, 1219, 252, …\n$ pop_female   &lt;dbl&gt; 29, 176, 94, 43, 48, 75, 84, 58, 43, 44, 19, 1208, 150, 2…\n$ pop_bruneian &lt;dbl&gt; 37, 280, 174, 55, 57, 64, 114, 88, 63, 35, 37, 1557, 235,…\n$ pop_pr       &lt;dbl&gt; 33, 83, 17, 24, 41, 64, 64, 28, 29, 32, 2, 179, 3, 67, 32…\n$ household    &lt;dbl&gt; 13, 83, 37, 23, 23, 23, 38, 26, 26, 23, 14, 517, 76, 691,…\n$ occ_liv_q    &lt;dbl&gt; 13, 62, 27, 16, 22, 21, 37, 22, 12, 23, 14, 492, 71, 681,…\n\n\nEach row of the data refers to a kampong-level observation. While there are unique identifiers to this (id, kampong, mukim, district), we would still need to geocode this data set so that we can do fun things like plot it on a map. Let’s use (again) left_join() to do this.\n\nbn_pop_sf &lt;- \n  left_join(\n    kpg_sf, \n    bn_census2021, \n    by = join_by(id, kampong, mukim, district)\n  )\n\nGreat. Let’s take a look at the population column. It would be very interesting to see where most of the 440,704 people of Brunei live!\n\nggplot(bn_pop_sf) +\n  geom_sf(aes(fill = population)) +\n  scale_fill_viridis_c(na.value = NA)\n\n\n\n\n\n\n\n\nAs expected, there are “hotspots” of population in the Brunei-Muara district, and to a lesser extent in the Belait district. We can make this graph a bit better by binning the population values. It seems to be dominated by a lot of these low value colours. Let’s take a look at this further by inspecting a histogram.\n\nggplot(bn_pop_sf) +\n  geom_histogram(aes(population), binwidth = 100)\n\nWarning: Removed 75 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nSo maybe we can bin the population into 4 categories: &lt; 100, 101-1000, 1001-10000, and 10000+. For this we directly use the scale_fill_viridis_b() and adjust the breaks. Otherwise we would have to cut() the population column and then use scale_fill_manual(). We also added the names of the top 10 most populous kampongs to the map using ggrepel::geom_label_repel().\n\nkpg_labels_sf &lt;-\n  bn_pop_sf |&gt;\n  arrange(desc(population)) |&gt;\n  slice_head(n = 10)\n\nbn_pop_sf |&gt;\n  # filter(population &gt; 50) |&gt;\n  ggplot() +\n  geom_sf(aes(fill = population), col = NA, alpha = 0.8) +\n  geom_sf(data = kpg_sf, fill = NA, col = \"black\") +\n  ggrepel::geom_label_repel(\n    data = kpg_labels_sf,\n    aes(label = kampong, geometry = geometry),\n    stat = \"sf_coordinates\",\n    inherit.aes = FALSE,\n    box.padding = 1,\n    size = 2,\n    max.overlaps = Inf\n  ) +\n  scale_fill_viridis_b(\n    name = \"Population\",\n    na.value = NA,\n    labels = scales::comma,\n    breaks = c(0, 100, 1000, 10000, 20000)\n    # limits = c(0, 12000)\n  ) +\n  theme_bw()"
  },
  {
    "objectID": "2-gis_data.html#openstreetmap-data",
    "href": "2-gis_data.html#openstreetmap-data",
    "title": "Geographical Information System (GIS) data",
    "section": "OpenStreetMap data",
    "text": "OpenStreetMap data\n\n\n\n\n\n\nWhat we’ll learn\n\n\n\n\nHow to scrape OpenStreetMap data using the {osmdata} package.\n\n\n\nThe {osmdata} package is a very useful tool for scraping OpenStreetMap data. It allows you to download data from OpenStreetMap and convert it into an sf object. The package is built on top of the osmdata API, which is a wrapper around the Overpass API. The Overpass API is a read-only API that allows you to query OpenStreetMap data. Conveniently, we do not need an API key.\n\nEXAMPLE: How to get all the schools in Brunei\nWhen we go to https://www.openstreetmap.org/ website, we can search for some key terms. For example, if we search for “Sekolah Rendah Kiarong”, we see the following:\n\nHighlighted in red is the polygon that represents the school. Furthermore, we have some information in the “Tags” section such as:\n\naddr:place = Kiarong\naddr:street = Jalan Datu Ratna\nalt_name = Sekolah Rendah Kiarong\nalt_name:en = Kiarong Primary School\namenity = school\netc.\n\nThe {osmdata} package allows us to query this information. To replicate this ‘GUI’ experience using code, we do the following:\n\nq &lt;-\n  opq(\"brunei\") |&gt;\n  add_osm_feature(\n    key = \"name\", \n    value = \"Sekolah Rendah Datu Ratna Haji Muhammad Jaafar\"\n  ) |&gt;\n  osmdata_sf()\nprint(q)\n\nObject of class 'osmdata' with:\n                 $bbox : 4.002508,113.017925,6.546584,115.3635623\n        $overpass_call : The call submitted to the overpass API\n                 $meta : metadata including timestamp and version numbers\n           $osm_points : 'sf' Simple Features Collection with 16 points\n            $osm_lines : NULL\n         $osm_polygons : 'sf' Simple Features Collection with 1 polygons\n       $osm_multilines : NULL\n    $osm_multipolygons : NULL\n\n\nIt has found the school. To extract the information, let’s look at the $osm_polygons entry:\n\nglimpse(q$osm_polygons)\n\nRows: 1\nColumns: 10\n$ osm_id        &lt;chr&gt; \"309023494\"\n$ name          &lt;chr&gt; \"Sekolah Rendah Datu Ratna Haji Muhammad Jaafar\"\n$ `addr:place`  &lt;chr&gt; \"Kiarong\"\n$ `addr:street` &lt;chr&gt; \"Jalan Datu Ratna\"\n$ alt_name      &lt;chr&gt; \"Sekolah Rendah Kiarong\"\n$ `alt_name:en` &lt;chr&gt; \"Kiarong Primary School\"\n$ amenity       &lt;chr&gt; \"school\"\n$ `name:en`     &lt;chr&gt; \"Datu Ratna Haji Muhammad Jaafar Primary School\"\n$ source        &lt;chr&gt; \"Bing; survey\"\n$ geometry      &lt;POLYGON [°]&gt; POLYGON ((114.9125 4.892252...\n\n\nLet’s plot it!\n\n# warning: false\nggplot(filter(kpg_sf, mukim == \"Mukim Gadong B\")) +\n  geom_sf() +\n  geom_label_repel(\n    aes(label = kampong, geometry = geometry),\n    stat = \"sf_coordinates\",\n    inherit.aes = FALSE,\n    box.padding = 1,\n    size = 3,\n    max.overlaps = Inf\n  ) +\n  geom_sf(data = q$osm_polygons, fill = \"red3\")\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\nWe can query based on amenity type as well. For example, to get all the schools in Brunei:\n\n# Bounding box for Brunei Muara\nbm_sf &lt;- filter(kpg_sf, district == \"Brunei Muara\")\nbm_bbox &lt;- st_bbox(bm_sf)\n\nq &lt;-\n  opq(bm_bbox) |&gt;\n  add_osm_feature(\n    key = \"amenity\", \n    value = \"school\"\n  ) |&gt;\n  osmdata_sf()\nprint(q)\n\nObject of class 'osmdata' with:\n                 $bbox : 4.72903834429411,114.771346735899,5.04587807206061,115.138720231749\n        $overpass_call : The call submitted to the overpass API\n                 $meta : metadata including timestamp and version numbers\n           $osm_points : 'sf' Simple Features Collection with 1321 points\n            $osm_lines : NULL\n         $osm_polygons : 'sf' Simple Features Collection with 153 polygons\n       $osm_multilines : NULL\n    $osm_multipolygons : 'sf' Simple Features Collection with 1 multipolygons\n\n\nAlmost always it is a good idea to look at the polygons, instead of the points. In any case, you can always find the centroid of the polygons if you wanted to plot point data.\n\nschools_sf &lt;-\n  q$osm_polygons |&gt;\n  as_tibble() |&gt;  # these two lines convert to tibble-like object\n  st_as_sf() |&gt; \n  select(osm_id, name) |&gt;\n  drop_na() |&gt;\n  st_centroid()  # obtains X,Y coordinates of centroids\n\nprint(schools_sf)\n\nSimple feature collection with 138 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 114.7891 ymin: 4.730341 xmax: 115.1303 ymax: 5.036068\nGeodetic CRS:  WGS 84\n# A tibble: 138 × 3\n   osm_id    name                                                 geometry\n * &lt;chr&gt;     &lt;chr&gt;                                             &lt;POINT [°]&gt;\n 1 45517438  Sekolah Rendah Haji Tarif                  (114.9321 4.88012)\n 2 45768022  Sekolah Menengah Awang Semaun             (114.9389 4.876925)\n 3 45820441  Sekolah Rendah Pengiran Anak Puteri Besar (114.9397 4.874045)\n 4 45820563  Pehin Dato Jamil Primary School           (114.9473 4.873318)\n 5 157197463 Sekolah Ugama Pengiran Muda Abdul Malik … (114.8709 4.848966)\n 6 157489516 Sekolah Rendah Dato Marsal                (114.9576 4.961157)\n 7 167974917 Chung Hwa Middle School                   (114.9445 4.894822)\n 8 167974963 Sekolah Rendah Pusar Ulak                 (114.9358 4.896647)\n 9 167974968 St. Andrew’s School                       (114.9372 4.896313)\n10 260696860 Jerudong International School             (114.8793 4.969056)\n# ℹ 128 more rows\n\nggplot() +\n  geom_sf(data = bm_sf, aes(fill = mukim), alpha = 0.3) +\n  geom_sf(data = schools_sf, size = 2) \n\n\n\n\n\n\n\n\nFrom here…\n\nVisit the OSM Wiki to see what other amenities you can query.\nClearly not limited to schools – clinics, shops, movie theatres, …\nCombine with the road data from {osrm} to calculate distances between schools and hospitals, for example."
  }
]