{
  "hash": "bfc3c210b8949370f98fba4e5c841384",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data Scraping and Linear Regression\"\n---\n\n\nFor topic 1, we will cover linear regression. But before diving into that topic, we will talk about how to scrape data from the web.\n\n- https://r4ds.hadley.nz/webscraping\n\n\nLibraries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(rvest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(polite)\n```\n:::\n\n\n## Ethics \n\nData scraping is the process of extracting data from websites. This can be done manually, but it is often done using a program. In this section, we will use the `rvest` package to scrape data from a website.\n\nWhen scraping car prices from sellers’ websites in Brunei, it’s important to consider legal and ethical aspects:\n\n1.\t**Legal Considerations**: If the data is public, non-personal, and factual, scraping is generally acceptable. However, laws vary by location. If the data is behind a login or used for commercial purposes, consulting a lawyer is advisable.\n\t\n2.\t**Terms of Service**: Many websites prohibit scraping in their terms of service. In some regions, like the US, these terms may not be binding unless you explicitly agree to them (e.g., by creating an account). In Europe, these terms are often enforceable even without explicit consent.\n\n3.\t**Personally Identifiable Information**: Avoid scraping data that includes personal information (names, contact details, etc.) due to strict privacy laws like the GDPR in Europe. Ethical concerns arise even if the data is public.\n\n4.\t**Copyright**: Data like car prices is generally not protected by copyright, as it’s factual. However, if scraping includes original content (like descriptions or images), consider copyright laws and whether “fair use” applies.\n\n## HTML basics\n\nHTML stands for \"HyperText Markup Language\" and looks like this:\n\n``` {.html}\n<html>\n<head>\n  <title>Page title</title>\n</head>\n<body>\n  <h1 id='first'>A heading</h1>\n  <p>Some text &amp; <b>some bold text.</b></p>\n  <img src='myimg.png' width='100' height='100'>\n</body>\n```\n\nHTML has a hierarchical structure formed by **elements** which consist of a start tag (e.g. `<tag>`), optional **attributes** (`id='first'`), an end tag[^1] (like `</tag>`), and **contents** (everything in between the start and end tag).\n\n[^1]: A number of tags (including `<p>` and `<li>)` don't require end tags, but I think it's best to include them because it makes seeing the structure of the HTML a little easier.\n\nSince `<` and `>` are used for start and end tags, you can't write them directly.\nInstead you have to use the HTML **escapes** `&gt;` (greater than) and `&lt;` (less than).\nAnd since those escapes use `&`, if you want a literal ampersand you have to escape it as `&amp;`.\nThere are a wide range of possible HTML escapes but you don't need to worry about them too much because rvest automatically handles them for you.\n\n### Elements\n\nAll up, there are over 100 HTML elements.\nSome of the most important are:\n\n-   Every HTML page must be in an `<html>` element, and it must have two children: `<head>`, which contains document metadata like the page title, and `<body>`, which contains the content you see in the browser.\n\n-   Block tags like `<h1>` (heading 1), `<p>` (paragraph), and `<ol>` (ordered list) form the overall structure of the page.\n\n-   Inline tags like `<b>` (bold), `<i>` (italics), and `<a>` (links) formats text inside block tags.\n\nIf you encounter a tag that you've never seen before, you can find out what it does with a little googling.\nI recommend the [MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/HTML) which are produced by Mozilla, the company that makes the Firefox web browser.\n\n### Contents\n\nMost elements can have content in between their start and end tags.\nThis content can either be text or more elements.\nFor example, the following HTML contains paragraph of text, with one word in bold.\n\n```html\n<p>\n  Hi! My <b>name</b> is Haziq.\n</p>\n```\n\nThis renders as\n\n\n```{=html}\n<p>\n  Hi! My <b>name</b> is Haziq.\n</p>\n```\n\n\nThe **children** of a node refers only to elements, so the `<p>` element above has one child, the `<b>` element.\nThe `<b>` element has no children, but it does have contents (the text \"name\").\n\nConceptually, this can be represented as follows:\n\n\n```{mermaid}\ngraph TD;\n    P[\"<p> element\"]\n    P -- \"content\" --> T1[\"'Hi! My '\"]\n    P --> B[\"&lt;b&gt; element\"]\n    P -- \"content\" --> T2[\"' is Haziq.'\"]\n    B -- \"content\" --> T3[\"'name'\"]\n```\n\n\n\nSome elements, like `<img>` can't have children.\nThese elements depend solely on attributes for their behavior.\n\n### Attributes\n\nTags can have named **attributes** which look like `name1='value1' name2='value2'`.\nTwo of the most important attributes are `id` and `class`, which are used in conjunction with CSS (Cascading Style Sheets) to control the visual appearance of the page.\nThese are often useful when scraping data off a page.\n\n## Reading HTML with rvest\n\nYou'll usually start the scraping process with `read_html()`.\nThis returns an `xml_document`[^2] object which you'll then manipulate using rvest functions:\n\n[^2]: This class comes from the [xml2](https://xml2.r-lib.org) package.\n    xml2 is a low-level package that rvest builds on top of.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- read_html(\"http://rvest.tidyverse.org/\")\nclass(html)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"xml_document\" \"xml_node\"    \n```\n\n\n:::\n:::\n\n\nFor examples and experimentation, rvest also includes a function that lets you create an `xml_document` from literal HTML:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <p>This is a paragraph<p>\n  <ul>\n    <li>This is a bulleted list</li>\n  </ul>\n\")\nhtml\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{html_document}\n<html>\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n<p>This is a paragraph</p>\\n<p>\\n  </p>\\n<ul>\\n<li>This is a bull ...\n```\n\n\n:::\n:::\n\n\nRegardless of how you get the HTML, you'll need some way to identify the elements that contain the data you care about.\nrvest provides two options: CSS selectors and XPath expressions.\nHere I'll focus on CSS selectors because they're simpler but still sufficiently powerful for most scraping tasks.\n\n## CSS selectors\n\nCSS is short for cascading style sheets, and is a tool for defining the visual styling of HTML documents.\nCSS includes a miniature language for selecting elements on a page called **CSS selectors**.\nCSS selectors define patterns for locating HTML elements, and are useful for scraping because they provide a concise way of describing which elements you want to extract.\n\nCSS selectors can be quite complex, but fortunately you only need the simplest for rvest, because you can also write R code for more complicated situations.\nThe four most important selectors are:\n\n-   `p`: selects all `<p>` elements.\n\n-   `.title`: selects all elements with `class` \"title\".\n\n-   `p.special`: selects all `<p>` elements with `class` \"special\".\n\n-   `#title`: selects the element with the `id` attribute that equals \"title\".\n    Id attributes must be unique within a document, so this will only ever select a single element.\n\nIf you want to learn more CSS selectors I recommend starting with the fun [CSS dinner](https://flukeout.github.io/) tutorial and then referring to the [MDN web docs](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors).\n\nLets try out the most important selectors with a simple example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <h1>This is a heading</h1>\n  <p id='first'>This is a paragraph</p>\n  <p class='important'>This is an important paragraph</p>\n\")\n```\n:::\n\n\nIn rvest you can extract a single element with `html_element()` or all matching elements with `html_elements()`.\nBoth functions take a document[^3] and a css selector:\n\n[^3]: Or another element, more on that shortly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> html_element(\"h1\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{html_node}\n<h1>\n```\n\n\n:::\n\n```{.r .cell-code}\nhtml |> html_elements(\"p\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{xml_nodeset (2)}\n[1] <p id=\"first\">This is a paragraph</p>\n[2] <p class=\"important\">This is an important paragraph</p>\n```\n\n\n:::\n\n```{.r .cell-code}\nhtml |> html_elements(\".important\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{xml_nodeset (1)}\n[1] <p class=\"important\">This is an important paragraph</p>\n```\n\n\n:::\n\n```{.r .cell-code}\nhtml |> html_elements(\"#first\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{xml_nodeset (1)}\n[1] <p id=\"first\">This is a paragraph</p>\n```\n\n\n:::\n:::\n\n\nSelectors can also be combined in various ways using **combinators**.\nFor example,The most important combinator is \" \", the **descendant** combination, because `p a` selects all `<a>` elements that are a child of a `<p>` element.\n\nIf you don't know exactly what selector you need, I highly recommend using [SelectorGadget](https://rvest.tidyverse.org/articles/selectorgadget.html), which lets you automatically generate the selector you need by supplying positive and negative examples in the browser.\n\n## Extracting data\n\nNow that you've got the elements you care about, you'll need to get data out of them.\nYou'll usually get the data from either the text contents or an attribute.\nBut, sometimes (if you're lucky!), the data you need will be in an HTML table.\n\n### Text\n\nUse `html_text2()` to extract the plain text contents of an HTML element:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <ol>\n    <li>apple &amp; pear</li>\n    <li>banana</li>\n    <li>pineapple</li>\n  </ol>\n\")\nhtml |> \n  html_elements(\"li\") |> \n  html_text2()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"apple & pear\" \"banana\"       \"pineapple\"   \n```\n\n\n:::\n:::\n\n\nNote that the escaped ampersand is automatically converted to `&`; you'll only ever see HTML escapes in the source HTML, not in the data returned by rvest.\n\nYou might wonder why I used `html_text2()`, since it seems to give the same result as `html_text()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> \n  html_elements(\"li\") |> \n  html_text()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"apple & pear\" \"banana\"       \"pineapple\"   \n```\n\n\n:::\n:::\n\n\nThe main difference is how the two functions handle white space.\nIn HTML, white space is largely ignored, and it's the structure of the elements that defines how text is laid out.\n`html_text2()` does its best to follow the same rules, giving you something similar to what you'd see in the browser.\nTake this example which contains a bunch of white space that HTML ignores.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"<body>\n  <p>\n  This is\n  a\n  paragraph.</p><p>This is another paragraph.\n  \n  It has two sentences.</p>\n\")\n```\n:::\n\n\n`html_text2()` gives you what you expect: two paragraphs of text separated by a blank line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> \n  html_element(\"body\") |> \n  html_text2() |> \n  cat()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThis is a paragraph.\n\nThis is another paragraph. It has two sentences.\n```\n\n\n:::\n:::\n\n\nWhereas `html_text()` returns the garbled raw underlying text:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> \n  html_element(\"body\") |> \n  html_text() |> \n  cat()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  \n  This is\n  a\n  paragraph.This is another paragraph.\n  \n  It has two sentences.\n```\n\n\n:::\n:::\n\n\n### Attributes\n\nAttributes are used to record the destination of links (the `href` attribute of `<a>` elements) and the source of images (the `src` attribute of the `<img>` element):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <p><a href='https://en.wikipedia.org/wiki/Cat'>cats</a></p>\n  <img src='https://cataas.com/cat' width='100' height='200'>\n\")\n```\n:::\n\n\nThe value of an attribute can be retrieved with `html_attr()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> \n  html_elements(\"a\") |> \n  html_attr(\"href\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"https://en.wikipedia.org/wiki/Cat\"\n```\n\n\n:::\n\n```{.r .cell-code}\nhtml |> \n  html_elements(\"img\") |> \n  html_attr(\"src\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"https://cataas.com/cat\"\n```\n\n\n:::\n:::\n\n\nNote that `html_attr()` always returns a string, so you may need to post-process with `as.integer()`/`readr::parse_integer()` or similar.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> \n  html_elements(\"img\") |> \n  html_attr(\"width\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"100\"\n```\n\n\n:::\n\n```{.r .cell-code}\nhtml |> \n  html_elements(\"img\") |> \n  html_attr(\"width\") |> \n  as.integer()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 100\n```\n\n\n:::\n:::\n\n\n### Tables\n\nHTML tables are composed four main elements: `<table>`, `<tr>` (table row), `<th>` (table heading), and `<td>` (table data).\nHere's a simple HTML table with two columns and three rows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <table>\n    <tr>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n    <tr>\n      <td>1.5</td>\n      <td>2.7</td>\n    </tr>\n    <tr>\n      <td>4.9</td>\n      <td>1.3</td>\n    </tr>\n    <tr>\n      <td>7.2</td>\n      <td>8.1</td>\n    </tr>\n  </table>\n  \")\n```\n:::\n\n\nBecause tables are a common way to store data, rvest includes the handy `html_table()` which converts a table into a data frame:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> \n  html_node(\"table\") |> \n  html_table()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n      x     y\n  <dbl> <dbl>\n1   1.5   2.7\n2   4.9   1.3\n3   7.2   8.1\n```\n\n\n:::\n:::\n\n\n## Element vs elements\n\nWhen using rvest, your eventual goal is usually to build up a data frame, and you want each row to correspond some repeated unit on the HTML page.\nIn this case, you should generally start by using `html_elements()` to select the elements that contain each observation then use `html_element()` to extract the variables from each observation.\nThis guarantees that you'll get the same number of values for each variable because `html_element()` always returns the same number of outputs as inputs.\n\nTo illustrate this problem take a look at this simple example I constructed using a few entries from `dplyr::starwars`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <ul>\n    <li><b>C-3PO</b> is a <i>droid</i> that weighs <span class='weight'>167 kg</span></li>\n    <li><b>R2-D2</b> is a <i>droid</i> that weighs <span class='weight'>96 kg</span></li>\n    <li><b>Yoda</b> weighs <span class='weight'>66 kg</span></li>\n    <li><b>R4-P17</b> is a <i>droid</i></li>\n  </ul>\n  \")\n```\n:::\n\n\nIf you try to extract name, species, and weight directly, you end up with one vector of length four and two vectors of length three, and no way to align them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> html_elements(\"b\") |> html_text2()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\n```\n\n\n:::\n\n```{.r .cell-code}\nhtml |> html_elements(\"i\") |> html_text2()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"droid\" \"droid\" \"droid\"\n```\n\n\n:::\n\n```{.r .cell-code}\nhtml |> html_elements(\".weight\") |> html_text2()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"167 kg\" \"96 kg\"  \"66 kg\" \n```\n\n\n:::\n:::\n\n\nInstead, use `html_elements()` to find a element that corresponds to each character, then use `html_element()` to extract each variable for all observations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharacters <- html |> html_elements(\"li\")\n\ncharacters |> html_element(\"b\") |> html_text2()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncharacters |> html_element(\"i\") |> html_text2()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"droid\" \"droid\" NA      \"droid\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncharacters |> html_element(\".weight\") |> html_text2()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"167 kg\" \"96 kg\"  \"66 kg\"  NA      \n```\n\n\n:::\n:::\n\n\n`html_element()` automatically fills in `NA` when no elements match, keeping all of the variables aligned and making it easy to create a data frame:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\n  name = characters |> html_element(\"b\") |> html_text2(),\n  species = characters |> html_element(\"i\") |> html_text2(),\n  weight = characters |> html_element(\".weight\") |> html_text2()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    name species weight\n1  C-3PO   droid 167 kg\n2  R2-D2   droid  96 kg\n3   Yoda    <NA>  66 kg\n4 R4-P17   droid   <NA>\n```\n\n\n:::\n:::\n\n\n## Scraping house prices\n\n(LIVE DEMO)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This is how you get read the HTML into R\nurl <- \"https://www.bruhome.com/v3/buy.asp?p_=buy&id=&district=&propose=&property=&price=&location=&mylist=128&sort=&display=&offset=&bedrooms=&bathrooms=&carpark=\"\nhtml <- read_html(url)\n\n# Extract the house prices\nprices <-\n  html |>\n  html_elements(\".property-price\") |>\n  html_text2()\n\n# Clean up\nprices <- \n  str_remove_all(prices, \"[^0-9]\") |>  # Remove non-numeric characters\n  as.integer()\n\n# Do same thing for number of beds, baths, location, and other remarks\nbeds <-\n  html |>\n  html_elements(\".property-bed\") |>\n  html_text2() |>\n  as.integer()\n\nbaths <-\n  html |>\n  html_elements(\".property-bath\") |>\n  html_text2() |>\n  as.integer()\n\nlocation <-\n  html |>\n  html_elements(\".property-address\") |>\n  html_text2()\n\nremarks <- \n  html |>\n  html_elements(\"div p .mt-3\") |>\n  html_text2()\n\nremarks <- tail(remarks, length(prices))\n\n# Put it all in a data frame\nhsp_df <- tibble(\n  price = prices,\n  beds = beds,\n  baths = baths,\n  location = location,\n  remarks = remarks\n)\n```\n:::\n\n\nSome pages require you to click a \"load more\" button to see all the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the links\nproperties <-\n  html |>\n  html_elements(\".property-link\") |>\n  html_attr(\"href\")\n\n# Suppose I have a function that can extract the info I want from a single page\nextract_info <- function(i) {\n  link <- paste0(\"https://www.bruhome.com/v3/\", properties[i])\n  html <- read_html(link)\n  out <-\n    html |>\n    html_elements(\"p\") |>\n    html_text2()\n  out[1]\n}\n\n# Now what I could do is the following:\n# res <- c()\n# for (i in seq_along(properties)) {\n#   res[i] <- extract_info(i)\n# }\n\n# A better way:\nres <- map(\n  .x = seq_along(properties),\n  .f = extract_info,\n  .progress = TRUE\n)\n```\n:::\n\n\n## Cleaning using LLM\n\nADVANCED TOPIC!!!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remotes::install_github(\"AlbertRapp/tidychatmodels\")\nlibrary(tidychatmodels)\n\nchat <-\n  create_chat(\"ollama\") |>\n  add_model(\"llama3.1\") |>\n  add_message('What is love? IN 10 WORDS.') |> \n  perform_chat() \n\nextract_chat(chat)\n\n# Try to prime it to clean the data set\nclean_desc <- function(i) {\n  create_chat(\"ollama\") |>\n  add_model(\"llama3.1\") |>\n  add_message(glue::glue(\"\n    The following is a description of house for sale in Brunei obtained from the web site of the real estate company Bruhome. I would like you to extract the following information:\n    \n    1. Built up area (usually in square feet) [NUMERIC]\n    2. Type of house (whether it is detached, semi-detached, terrace, apartment, or other) [CHARACTER]\n  \n  Please return semicolon separated values like this:\n  2500; detached\n  3000; semi-detached\n  2000; terrace\n  etc.\n  NUMBERS SHOULD NOT CONTAIN comma (,) for thousands separator\n  \n  Please only return these two values and nothing else. Do not return any other information. I only want these two values in your chat response.\n  \n  NOTE: Some of these listings may be related to LAND or COMMERCIAL properties. In this case, please return NA for built up area, and 'commercial' or 'land' for type.\n  \n  IF YOU DO NOT SEE ANY DESCRIPTION it may mean that the description is missing. In this case, return NA only.\n  \n  IF YOU SEE MULTIPLE DESCRIPTIONS, please return the first one only.\n  \n  ----------\n  \n  {res[[i]]}\n  \")) |> \n  perform_chat() |>\n  extract_chat(silent = TRUE) |>\n  filter(role == \"assistant\") |>\n  pull(message)\n}\n\n# Now map it over the descriptions!\ncleaned_descriptions <-\n  map(\n    .x = seq_along(res),\n    .f = clean_desc,\n    .progress = TRUE\n  )\n\n# Now add to the hsp_df\nhsp_df$desc <- unlist(cleaned_descriptions)\nhsp_df <-\n  hsp_df |>\n  mutate(\n    desc = unlist(res),\n    cleaned_desc = unlist(cleaned_descriptions)\n  ) |>\n  separate(cleaned_desc, into = c(\"sqft\", \"type\"), sep = \";\") |>\n  mutate(\n    sqft = as.integer(sqft),\n    type = case_when(\n      grepl(\"detached\", type, ignore.case = TRUE) ~ \"detached\",\n      grepl(\"semi-detached\", type, ignore.case = TRUE) ~ \"semi-detached\",\n      grepl(\"terrace\", type, ignore.case = TRUE) ~ \"terrace\",\n      grepl(\"apartment\", type, ignore.case = TRUE) ~ \"apartment\",\n      grepl(\"land\", type, ignore.case = TRUE) ~ \"land\",\n      grepl(\"commercial\", type, ignore.case = TRUE) ~ \"commercial\",\n      TRUE ~ NA\n    )\n  )\n# save(hsp_df, file = \"data/hsp_df.RData\")\n```\n:::\n\n\n\n\n## Linear regression\n\nIn statistical modelling, the aim is to describe the relationship between one or more **predictor variables** (usually denoted $x$) and a **response variable** (usually denoted $y$).\nMathematically, we can say\n$$\ny = f(x) + \\epsilon.\n$$\nHere $f$ is some *regression* function that we want to estimate, and $\\epsilon$ is an error term that captures the difference between the true relationship and our estimate.\n\nThe simplest type of modelling is called **linear regression**, where we assume that the relationship between $x$ and $y$ is linear.\nThat is,\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_px_p + \\epsilon.\n$$\nWhen we ask software to estimate the $\\beta$ coefficients, it will find the values that optimise a certain criterion (typically, one that yields the smallest error values).\nIn R, you need to supply two things:\n\n1. A formula that describes the relationship between the variables.\n2. The data frame that contains the variables.\n\n### Model fit\n\nHere's an example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"data/hsp_df.RData\")  # I saved this data set earlier and load it back\n\n# First clean the data a bit\nhtypes <- c(\"detached\", \"semi-detached\", \"terrace\", \"apartment\")\nhsp_mod_df <-\n  hsp_df |>\n  filter(type %in% htypes) |>\n  mutate(\n    type = factor(type, levels = htypes),\n    priceK = price / 1000,  # Price in thousands\n    sqftK = sqft / 1000  # Square feet in thousands\n  ) |>\n  select(priceK, beds, baths, sqftK, type) |>\n  drop_na()\n\nfit <- lm(priceK ~ beds + baths + sqftK + type, data = hsp_mod_df)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = priceK ~ beds + baths + sqftK + type, data = hsp_mod_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.637  -38.483   -2.603   32.274  165.839 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -178.288     47.360  -3.765 0.000387 ***\nbeds            75.147     16.837   4.463 3.70e-05 ***\nbaths           44.832     10.470   4.282 6.92e-05 ***\nsqftK           -1.654      1.894  -0.873 0.386067    \ntypeterrace    -41.525     20.878  -1.989 0.051350 .  \ntypeapartment   62.151     34.485   1.802 0.076615 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56.59 on 59 degrees of freedom\nMultiple R-squared:  0.7965,\tAdjusted R-squared:  0.7792 \nF-statistic: 46.18 on 5 and 59 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n### Interpretation\n\nHere's a breakdown of the key components to aid your understanding:\n\n1. **Residuals**: These represent the differences between the observed and predicted values of `priceK`. The five-number summary (Min, 1Q, Median, 3Q, Max) helps you understand the distribution of residuals. Ideally, the residuals should be symmetrically distributed around zero, indicating a good model fit.\n\n2. **Coefficients**: This section shows the estimated effect (Estimate) of each predictor on `priceK`:\n   - **(Intercept)**: The expected `priceK` when all predictors are zero. Here, it's -178.288, but this value often doesn't have a real-world interpretation if the predictor values can't actually be zero.\n   - **beds**: Each additional bedroom increases the expected `priceK` by about 75.15 (thousand).\n   - **baths**: Each additional bathroom increases the expected `priceK` by about 44.83 (thousand).\n   - **sqftK**: The effect of the square footage in thousands on `priceK`. Here, it's not statistically significant (p-value = 0.386), meaning it doesn't contribute much to predicting `priceK`.\n   - **type**: This is a categorical variable with three levels. The coefficients for `typeterrace` and `typeapartment` are relative to the reference category (likely another property type not shown here, such as \"detached house\"). For example, `typeterrace` lowers the expected `priceK` by 41.53 (thousand) compared to the reference category.\n\n3. **Significance Codes**: Indicators of statistical significance for each predictor:\n   - `***` highly significant (p < 0.001)\n   - `**` significant (p < 0.01)\n   - `*` moderately significant (p < 0.05)\n   - `.` marginally significant (p < 0.1)\n   - None of these symbols indicate non-significance.\n\n4. **Residual Standard Error**: This is the standard deviation of the residuals. A smaller value suggests a better fit, as it indicates that the observed values are closer to the fitted values.\n\n5. **R-squared and Adjusted R-squared**: \n   - **R-squared** (0.7965) indicates that about 79.65% of the variability in `priceK` is explained by the model.\n   - **Adjusted R-squared** (0.7792) is a modified version of R-squared that accounts for the number of predictors, providing a more accurate measure for models with multiple variables.\n\n6. **F-statistic**: This tests whether at least one predictor variable is significantly related to the dependent variable. A p-value of < 2.2e-16 indicates the model is highly significant.\n\n**Key Takeaway**: The model shows that `beds` and `baths` significantly predict `priceK`, while `sqftK` does not have a significant effect. The `type` variable shows some variation, with `typeterrace` having a marginally significant negative effect on `priceK`. Overall, the model explains a large proportion of the variation in house prices.\n\n\n### Predictions\n\nOne of the main uses of a linear regression model is to make predictions.\nThat is, given a set of predictor values (typically unseen data), we can estimate the response variable.\nIn the context of the house price data, this means we can estimate the price of a house given its number of bedrooms, bathrooms, square footage, and type.\n\nFirst, let's set up the data for a new house:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_house <- tibble(\n  beds = 3,\n  baths = 2,\n  sqftK = 2.5,\n  type = factor(\"detached\", levels = htypes)\n)\n```\n:::\n\n\nThen, to predict the price, we run the following command:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit, newdata = new_house, interval = \"prediction\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 132.6803 14.74783 250.6128\n```\n\n\n:::\n:::\n\n\nThis also gives the 95% prediction interval, which is a range of values within which we expect the true price to fall with 95% confidence. \nWhat we can see is that the model predicts the price of the new house to be around 133,000 Brunei dollars, with a 95% prediction interval of approximately [15,000,  251,000] Brunei dollars.\n\nYou might be wondering why the prediction interval is so wide.\nThis is because the model is uncertain about the price of a new house, given the limited information we have.\nGenerally, the more data you have, the narrower the prediction interval will be.\n\n::: {.callout-note}\nYou can get model predictions for the original data set by using the `predict()` without `newdata` argument. Alternatively, `fitted()` works too.\n:::\n\n## More advanced models\n\nLinear regression is a simple and powerful tool, but it has its limitations.\nIf you were more interested in predictions, then you might want to consider more advanced machine learning (ML) models.\nHere are a couple of suggestions:\n\n1. **Random Forest**: This is an ensemble learning method that builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n\n2. **Gradient Boosting Machines (GBM)**: GBM is another ensemble learning method that builds multiple decision trees sequentially, with each tree correcting the errors of the previous one.\n\n3. **Neural Networks**: These are a set of algorithms that are designed to recognize patterns, with the ability to model complex relationships between inputs and outputs.\n\n### Random forests\n\nRandom forests are popular because they are easy to use and generally provide good results.\nHere's how you can fit a random forest model to the house price data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrandomForest 4.7-1.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'randomForest'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n\n\n:::\n\n```{.r .cell-code}\nfit_rf <- randomForest(priceK ~ beds + baths + sqftK + type, data = hsp_mod_df)\n```\n:::\n\n\nWith random forests, you don't really get \"beta\" coefficients. So there's no point running `summary()`. \nInstead, it's mainly used as a black box to obtain predicted values.\n\nLet's compare the predictions from the random forest model to the linear regression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  lm = predict(fit),\n  rf = predict(fit_rf)\n) |>\n  ggplot(aes(lm, rf)) +\n  geom_point() +\n  geom_abline() +\n  labs(\n    x = \"Linear regression\",\n    y = \"Random forest\",\n    title = \"Comparison of linear regression and random forest predictions\"\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](1-scraping_linear_reg_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nTo see which model gives smaller errors, we can run the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid_lm <- hsp_mod_df$priceK - predict(fit)\nresid_rf <- hsp_mod_df$priceK - predict(fit_rf)\n\n# Residual sum of squares\nsum(resid_lm ^ 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 188934.9\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(resid_rf ^ 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 204385.7\n```\n\n\n:::\n:::\n\n\nIn this case, the linear regression model has a smaller residual sum of squares, indicating that it fits the data better.\n\nOut of curiosity, let's see the predictions for the new house using the random forest model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit_rf, newdata = new_house, )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n231.9267 \n```\n\n\n:::\n:::\n\n\nWhich seems very different to the `lm()` predictions.\n",
    "supporting": [
      "1-scraping_linear_reg_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}