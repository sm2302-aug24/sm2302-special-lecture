{
  "hash": "b8a34ee81a143a731d295b0bb2e1743b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Quantitative analysis of textual data\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n\n- https://tutorials.quanteda.io/introduction/\n\n\n## Introduction\n\nThere are several R packages used for quantitative text analysis, but we will focus specifically on the `{quanteda}` package. So, first install the package from CRAN:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"quanteda\")\n```\n:::\n\n\n\n\n\n\nSince the release of `{quanteda}` version 3.0, `textstat_*`, `textmodel_*` and `textplot_*` functions are available in separate packages. We will use several of these functions in the chapters below and strongly recommend installing these packages.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"quanteda.textmodels\")\ninstall.packages(\"quanteda.textstats\")\ninstall.packages(\"quanteda.textplots\")\n```\n:::\n\n\n\n\n\n\nWe will use the `{readtext}` package to read in different types of text data in these tutorials. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"readtext\")\n```\n:::\n\n\n\n\n\n\n\n## Quantitative data \n\nBefore beginning we need to load the libraries\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(readtext)\n```\n:::\n\n\n\n\n\n\nAnd these ones lates for the modelling section:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(seededlda)\nlibrary(LSX)\nlibrary(lubridate)\nlibrary(ggdendro)\n```\n:::\n\n\n\n\n\n\n### Pre-formatted files\n\nIf your text data is stored in a pre-formatted file where one column contains the text and additional columns might store document-level variables (e.g. year, author, or language), you can import this into R using `read_csv()`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath_data <- system.file(\"extdata/\", package = \"readtext\")\ndat_inaug <- read_csv(paste0(path_data, \"/csv/inaugCorpus.csv\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 5 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (3): texts, President, FirstName\ndbl (1): Year\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(dat_inaug)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 5\nColumns: 4\n$ texts     <chr> \"Fellow-Citizens of the Senate and of the House of Represent~\n$ Year      <dbl> 1789, 1793, 1797, 1801, 1805\n$ President <chr> \"Washington\", \"Washington\", \"Adams\", \"Jefferson\", \"Jefferson\"\n$ FirstName <chr> \"George\", \"George\", \"John\", \"Thomas\", \"Thomas\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe data set is about the inaugural speeches of the US presidents. So as we can see the data set is arranged in tabular form, with 5 rows and 4 columns. The columns are `texts`, `Year`, `President`, and `FirstName`. \n\nAlternatively, you can use the `{readtext}` package to import character (comma- or tab-separated) values. `{readtext}` reads files containing text, along with any associated document-level variables. As an example, consider the following tsv file:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsv_file <- paste0(path_data, \"/tsv/dailsample.tsv\")\ncat(readLines(tsv_file, n = 4), sep = \"\\n\")  # first 3 lines\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nspeechID\tmemberID\tpartyID\tconstID\ttitle\tdate\tmember_name\tparty_name\tconst_name\tspeech\n1\t977\t22\t158\t1. CEANN COMHAIRLE I gCOIR AN LAE.\t1919-01-21\tCount George Noble, Count Plunkett\tSinn Féin\tRoscommon North\tMolaimse don Dáil Cathal Brugha, an Teachta ó Dhéisibh Phortláirge do bheith mar Cheann Comhairle againn indiu.\n2\t1603\t22\t103\t1. CEANN COMHAIRLE I gCOIR AN LAE.\t1919-01-21\tMr. Pádraic Ó Máille\tSinn Féin\tGalway Connemara\tIs bród mór damhsa cur leis an dtairgsin sin. Air sin do ghaibh CATHAL BRUGHA ceannus na Dála agus adubhairt:-\n3\t116\t22\t178\t1. CEANN COMHAIRLE I gCOIR AN LAE.\t1919-01-21\tMr. Cathal Brugha\tSinn Féin\tWaterford County\t' A cháirde, tá obair thábhachtach le déanamh annso indiu, an obair is tábhachtaighe do rinneadh in Éirinn ón lá tháinic na Gaedhil go hÉirinn, agus is naomhtha an obair í. Daoine go bhfuil dóchas aca as Dia iseadh sinn go léir, daoine a chuireann suim I ndlighthibh Dé, agus dá bhrigh sin budh chóir dúinn congnamh d'iarraidh ar Dhia I gcóir na hoibre atá againn le déanamh. Iarrfad anois ar an sagart is dúthrachtaighe dár mhair riamh I nÉirinn, an tAthair Micheál Ó Flannagáin, guidhe chum an Spiorad Naomh dúinn chum sinn do stiúradh ar ár leas ar an mbóthar atá againn le gabháil. ' ' Agus, a cháirde, pé cineál creidimh atá ag éinne annso, iarrfad ar gach n-aon paidir do chur suas chum Dé, ó íochtar a chroidhe chum cabhair do thabhairt dúinn indiu. Glaodhaim anois ar an Athair Micheál Ó Flannagáin. ' Do tháinig an tATHAIR MICHEÁL Ó FLANNAGÁIN I láthair na Dála agus do léigh an phaidir seo I n-ár ndiaidh: 'Tair, A Spioraid Naomh, ath-líon croidhthe t'fhíoraon, agus adhain ionnta teine do ghrádha. ' ' Cuir chugainn do Spioraid agus cruthóchfar iad agus athnuadhfaidh Tú aghaidh na talmhan.' ' Guidhmís: ' ' A Dhia, do theagaisc croidhthe sa bhfíoraon le lonnradh an Spioraid Naoimh, tabhair dúinn, san Spiorad cheudna, go mblaisfimíd an ceart agus go mbéidh síorgháirdeachais orainn de bhárr a shóláis sin. Tré Íosa Críost ár dTighearna. Ámén'\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe document itself in raw format is arranged in tabular form, separated by tabs. Each row contains a \"document\" (in this case, a speech) and the columns contain **document-level** variables. The column that contains the actual speech is named `speech`. To import this using `{readtext}`, you can use the following code:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_dail <- readtext(tsv_file, text_field = \"speech\")\nglimpse(dat_dail)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 33\nColumns: 11\n$ doc_id      <chr> \"dailsample.tsv.1\", \"dailsample.tsv.2\", \"dailsample.tsv.3\"~\n$ text        <chr> \"Molaimse don Dáil Cathal Brugha, an Teachta ó Dhéisibh Ph~\n$ speechID    <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,~\n$ memberID    <int> 977, 1603, 116, 116, 116, 116, 496, 116, 116, 2095, 116, 1~\n$ partyID     <int> 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22~\n$ constID     <int> 158, 103, 178, 178, 178, 178, 46, 178, 178, 139, 178, 178,~\n$ title       <chr> \"1. CEANN COMHAIRLE I gCOIR AN LAE.\", \"1. CEANN COMHAIRLE ~\n$ date        <chr> \"1919-01-21\", \"1919-01-21\", \"1919-01-21\", \"1919-01-21\", \"1~\n$ member_name <chr> \"Count George Noble, Count Plunkett\", \"Mr. Pádraic Ó Máill~\n$ party_name  <chr> \"Sinn Féin\", \"Sinn Féin\", \"Sinn Féin\", \"Sinn Féin\", \"Sinn ~\n$ const_name  <chr> \"Roscommon North\", \"Galway Connemara\", \"Waterford County\",~\n```\n\n\n:::\n:::\n\n\n\n\n\n\n### Multiple text files\n\nA second option to import data is to load multiple text files at once that are stored in the same folder or subfolders. Again, `path_data` is the location of sample files on your computer. Unlike the pre-formatted files, individual text files usually do not contain document-level variables. However, you can create document-level variables using the `{readtext}` package.\n\nThe directory `/txt/UDHR` contains text files (\".txt\") of the Universal Declaration of Human Rights in 13 languages. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath_udhr <- paste0(path_data, \"/txt/UDHR\")\nlist.files(path_udhr)  # list the files in this folder\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"UDHR_chinese.txt\"    \"UDHR_czech.txt\"      \"UDHR_danish.txt\"    \n [4] \"UDHR_english.txt\"    \"UDHR_french.txt\"     \"UDHR_georgian.txt\"  \n [7] \"UDHR_greek.txt\"      \"UDHR_hungarian.txt\"  \"UDHR_icelandic.txt\" \n[10] \"UDHR_irish.txt\"      \"UDHR_japanese.txt\"   \"UDHR_russian.txt\"   \n[13] \"UDHR_vietnamese.txt\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\nEach one of these txt files contains the text of the UDHR in the specific language.\nFor instance, to inspect what each one of these files contain, we do the following:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# just first 5 lines\ncat(readLines(file.path(path_udhr, \"UDHR_chinese.txt\"), n = 5), sep = \"\\n\")  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n世界人权宣言\n联合国大会一九四八年十二月十日第217A(III)号决议通过并颁布 1948 年 12 月 10 日， 联 合 国 大 会 通 过 并 颁 布《 世 界 人 权 宣 言》。 这 一 具 有 历 史 意 义 的《 宣 言》 颁 布 后， 大 会 要 求 所 有 会 员 国 广 为 宣 传， 并 且“ 不 分 国 家 或 领 土 的 政 治 地 位 , 主 要 在 各 级 学 校 和 其 他 教 育 机 构 加 以 传 播、 展 示、 阅 读 和 阐 述。” 《 宣 言 》 全 文 如 下： 序言 鉴于对人类家庭所有成员的固有尊严及其平等的和不移的权利的 承 认, 乃 是 世 界 自 由、 正 义 与 和 平 的 基 础, 鉴 于 对 人 权 的 无 视 和 侮 蔑 已 发 展 为 野 蛮 暴 行, 这 些 暴 行 玷 污 了 人 类 的 良 心, 而 一 个 人 人 享 有 言 论 和 信 仰 自 由 并 免 予 恐 惧 和 匮 乏 的 世 界 的 来 临, 已 被 宣 布 为 普 通 人 民 的 最 高 愿 望, 鉴 于 为 使 人 类 不 致 迫 不 得 已 铤 而 走 险 对 暴 政 和 压 迫 进 行 反 叛, 有 必 要 使 人 权 受 法 治 的 保 护, 鉴 于 有 必 要 促 进 各 国 间 友 好 关 系 的 发 展, 鉴于各联合国国家的人民已在联合国宪章中重申他们对基本人 权、 人 格 尊 严 和 价 值 以 及 男 女 平 等 权 利 的 信 念, 并 决 心 促 成 较 大 自 由 中 的 社 会 进 步 和 生 活 水 平 的 改 善, 鉴于各会员国业已誓愿同联合国合作以促进对人权和基本自由的 普 遍 尊 重 和 遵 行, 鉴于对这些权利和自由的普遍了解对于这个誓愿的充分实现具有 很 大 的 重 要 性, 因 此 现 在, 大 会, 发 布 这 一 世 界 人 权 宣 言 , 作 为 所 有 人 民 和 所 有 国 家 努 力 实 现 的 共 同 标 准, 以 期 每 一 个 人 和 社 会 机 构 经 常 铭 念 本 宣 言, 努 力 通 过 教 诲 和 教 育 促 进 对 权 利 和 自 由 的 尊 重, 并 通 过 国 家 的 和 国 际 的 渐 进 措 施, 使 这 些 权 利 和 自 由 在 各 会 员 国 本 身 人 民 及 在 其 管 辖 下 领 土 的 人 民 中 得 到 普 遍 和 有 效 的 承 认 和 遵 行; 第一条\n\n\f人 人 生 而 自 由, 在 尊 严 和 权 利 上 一 律 平 等。 他 们 赋 有 理 性 和 良 心, 并 应 以 兄 弟 关 系 的 精 神 相 对 待。 第二条 人 人 有 资 格 享 有 本 宣 言 所 载 的 一 切 权 利 和 自 由, 不 分 种 族、 肤 色、 性 别、 语 言、 宗 教、 政 治 或 其 他 见 解、 国 籍 或 社 会 出 身、 财 产、 出 生 或 其 他 身 分 等 任 何 区 别。 并 且 不 得 因 一 人 所 属 的 国 家 或 领 土 的 政 治 的、 行 政 的 或 者 国 际 的 地 位 之 不 同 而 有 所 区 别, 无 论 该 领 土 是 独 立 领 土、 托 管 领 土、 非 自 治 领 土 或 者 处 于 其 他 任 何 主 权 受 限 制 的 情 况 之 下。 第三条 人 人 有 权 享 有 生 命、 自 由 和 人 身 安 全。 第四条 任 何 人 不 得 使 为 奴 隶 或 奴 役; 一 切 形 式 的 奴 隶 制 度 和 奴 隶 买 卖, 均 应 予 以 禁 止。 第五条 任 何 人 不 得 加 以 酷 刑, 或 施 以 残 忍 的、 不 人 道 的 或 侮 辱 性 的 待 遇 或 刑 罚。 第六条 人 人 在 任 何 地 方 有 权 被 承 认 在 法 律 前 的 人 格。 第七条\n```\n\n\n:::\n:::\n\n\n\n\n\n\nTo import these files, you can use the following code:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_udhr <- readtext(path_udhr)\nglimpse(dat_udhr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 13\nColumns: 2\n$ doc_id <chr> \"UDHR_chinese.txt\", \"UDHR_czech.txt\", \"UDHR_danish.txt\", \"UDHR_~\n$ text   <chr> \"世界人权宣言\\n联合国大会一九四八年十二月十日第217A(III)号决议~\n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: {.callout-note}\nIf you are using Windows, you need might need to specify the encoding of the file by adding `encoding = \"utf-8\"`. In this case, imported texts might appear like `<U+4E16><U+754C><U+4EBA><U+6743>` but they indicate that Unicode charactes are imported correctly.\n:::\n\nHere's another example of multiple text files. The directory `/txt/EU_manifestos` contains text files (\".txt\") of the European Union manifestos in different languages. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath_eu <- paste0(path_data, \"/txt/EU_manifestos/\")\nlist.files(path_eu)  # list the files in this folder\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"EU_euro_2004_de_PSE.txt\" \"EU_euro_2004_de_V.txt\"  \n [3] \"EU_euro_2004_en_PSE.txt\" \"EU_euro_2004_en_V.txt\"  \n [5] \"EU_euro_2004_es_PSE.txt\" \"EU_euro_2004_es_V.txt\"  \n [7] \"EU_euro_2004_fi_V.txt\"   \"EU_euro_2004_fr_PSE.txt\"\n [9] \"EU_euro_2004_fr_V.txt\"   \"EU_euro_2004_gr_V.txt\"  \n[11] \"EU_euro_2004_hu_V.txt\"   \"EU_euro_2004_it_PSE.txt\"\n[13] \"EU_euro_2004_lv_V.txt\"   \"EU_euro_2004_nl_V.txt\"  \n[15] \"EU_euro_2004_pl_V.txt\"   \"EU_euro_2004_se_V.txt\"  \n[17] \"EU_euro_2004_si_V.txt\"  \n```\n\n\n:::\n:::\n\n\n\n\n\n\nYou can generate document-level variables based on the file names using the `docvarnames` and `docvarsfrom` argument. `dvsep = \"_\"` specifies the value separator in the filenames. `encoding = \"ISO-8859-1\"` determines character encodings of the texts. Notice how the document variables are nicely generated from the file names.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_eu <- readtext(\n  file = path_eu,\n  docvarsfrom = \"filenames\", \n  docvarnames = c(\"unit\", \"context\", \"year\", \"language\", \"party\"),\n  dvsep = \"_\", \n  encoding = \"ISO-8859-1\"\n)\nglimpse(dat_eu)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 17\nColumns: 7\n$ doc_id   <chr> \"EU_euro_2004_de_PSE.txt\", \"EU_euro_2004_de_V.txt\", \"EU_euro_~\n$ text     <chr> \"PES · PSE · SPE European Parliament rue Wiertz B 1047 Brusse~\n$ unit     <chr> \"EU\", \"EU\", \"EU\", \"EU\", \"EU\", \"EU\", \"EU\", \"EU\", \"EU\", \"EU\", \"~\n$ context  <chr> \"euro\", \"euro\", \"euro\", \"euro\", \"euro\", \"euro\", \"euro\", \"euro~\n$ year     <int> 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2~\n$ language <chr> \"de\", \"de\", \"en\", \"en\", \"es\", \"es\", \"fi\", \"fr\", \"fr\", \"gr\", \"~\n$ party    <chr> \"PSE\", \"V\", \"PSE\", \"V\", \"PSE\", \"V\", \"V\", \"PSE\", \"V\", \"V\", \"V\"~\n```\n\n\n:::\n:::\n\n\n\n\n\n\n### JSON\n\nYou can also read JSON files (.json) downloaded from the Twititer stream API. [twitter.json](https://raw.githubusercontent.com/quanteda/tutorials.quanteda.io/master/content/data/twitter.json) is located in data directory of this tutorial package.\n\nThe JSON file looks something like this\n\n```\n{\"created_at\":\"Wed Jun 07 23:30:01 +0000 2017\",\"id\":872596537142116352,\"id_str\":\"872596537142116352\",\"text\":\"@EFC_Jayy UKIP\",\"display_text_range\":[10,14],\n\"source\":\"\\u003ca href=\\\"http:\\/\\/twitter.com\\/download\\/iphone\\\" rel=\\\"nofollow\\\"\\u003eTwitter for iPhone\\u003c\\/a\\u003e\",\"truncated\":false,\"in_reply_to_status_id\":872596176834572288,\n\"in_reply_to_status_id_str\":\"872596176834572288\",\"in_reply_to_user_id\":4556760676,\"in_reply_to_user_id_str\":\"4556760676\",\"in_reply_to_screen_name\":\"EFC_Jayy\",\"user\":{\"id\":863929468984995840,\"id_str\":\"863929468984995840\",\"name\":\"\\u30b8\\u30e7\\u30fc\\u30b8\",\"screen_name\":\"CoysJoji\",\"location\":\"Japan\",\"url\":null,\"description\":null,\"protected\":false,\n\"verified\":false,\"followers_count\":367,\"friends_count\":304,\"listed_count\":1,\"favourites_count\":1260,\"statuses_count\":2930,\"created_at\":\"Mon May 15 01:30:11 +0000 2017\",\"utc_offset\":null,\"time_zone\":null,\"geo_enabled\":false,\"lang\":\"en\",\"contributors_enabled\":false,\"is_translator\":false,\"profile_background_color\":\"F5F8FA\",\"profile_background_image_url\":\"\",\"profile_background_image_url_https\":\"\",\"profile_background_tile\":false,\n\"profile_link_color\":\"1DA1F2\",\"profile_sidebar_border_color\":\"C0DEED\",\"profile_sidebar_fill_color\":\"DDEEF6\",\"profile_text_color\":\"333333\",\"profile_use_background_image\":true,\"profile_image_url\":\"http:\\/\\/pbs.twimg.com\\/profile_images\\/870447188400365568\\/RiR1hbCe_normal.jpg\",\n\"profile_image_url_https\":\"https:\\/\\/pbs.twimg.com\\/profile_images\\/870447188400365568\\/RiR1hbCe_normal.jpg\",\"profile_banner_url\":\"https:\\/\\/pbs.twimg.com\\/profile_banners\\/863929468984995840\\/1494897624\",\"default_profile\":true,\"default_profile_image\":false,\"following\":null,\"follow_request_sent\":null,\"notifications\":null},\"geo\":null,\"coordinates\":null,\n\"place\":null,\"contributors\":null,\"is_quote_status\":false,\"retweet_count\":0,\"favorite_count\":0,\"entities\":{\"hashtags\":[],\"urls\":[],\"user_mentions\":[{\"screen_name\":\"EFC_Jayy\",\"name\":\"\\u274c\\u274c\\u274c\",\"id\":4556760676,\"id_str\":\"4556760676\",\"indices\":[0,9]}],\"symbols\":[]},\"favorited\":false,\"retweeted\":false,\"filter_level\":\"low\",\"lang\":\"en\",\"timestamp_ms\":\"1496878201171\"}\n```\n\nIt's a little hard to parse, but luckily we just leave it to the `{readtext}` package to do the job for us.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_twitter <- readtext(\"../data/twitter.json\", source = \"twitter\")\n```\n:::\n\n\n\n\n\n\nThe file comes with several metadata for each tweet, such as the number of retweets and likes, the username, time and time zone. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(names(dat_twitter))\n```\n:::\n\n\n\n\n\n\n```\n## [1] \"doc_id\"         \"text\"           \"retweet_count\"  \"favorite_count\"\n## [5] \"favorited\"      \"truncated\"\n```\n\n### PDF\n\n`readtext()` can also convert and read PDF (\".pdf\") files. The directory `/pdf/UDHR` contains PDF files of the Universal Declaration of Human Rights in 13 languages. Each file looks like this:\n\n![](figures/udhr_sample.png)\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_udhr <- readtext(\n  paste0(path_data, \"/pdf/UDHR/*.pdf\"), \n  docvarsfrom = \"filenames\", \n  docvarnames = c(\"document\", \"language\"),\n  sep = \"_\"\n)\nprint(dat_udhr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nreadtext object consisting of 11 documents and 2 docvars.\n# A data frame: 11 x 4\n  doc_id           text                          document language\n  <chr>            <chr>                         <chr>    <chr>   \n1 UDHR_chinese.pdf \"\\\"世界人权宣言\\n\\n联合\\\"...\" UDHR     chinese \n2 UDHR_czech.pdf   \"\\\"VŠEOBECNÁ \\\"...\"           UDHR     czech   \n3 UDHR_danish.pdf  \"\\\"Den 10. de\\\"...\"           UDHR     danish  \n4 UDHR_english.pdf \"\\\"Universal \\\"...\"           UDHR     english \n5 UDHR_french.pdf  \"\\\"Déclaratio\\\"...\"           UDHR     french  \n6 UDHR_greek.pdf   \"\\\"ΟΙΚΟΥΜΕΝΙΚ\\\"...\"           UDHR     greek   \n# i 5 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n\n### Microsoft Word\n\nFinally, `readtext()` can import Microsoft Word (\".doc\" and \".docx\") files.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_word <- readtext(paste0(path_data, \"/word/*.docx\"))\nprint(dat_udhr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nreadtext object consisting of 11 documents and 2 docvars.\n# A data frame: 11 x 4\n  doc_id           text                          document language\n  <chr>            <chr>                         <chr>    <chr>   \n1 UDHR_chinese.pdf \"\\\"世界人权宣言\\n\\n联合\\\"...\" UDHR     chinese \n2 UDHR_czech.pdf   \"\\\"VŠEOBECNÁ \\\"...\"           UDHR     czech   \n3 UDHR_danish.pdf  \"\\\"Den 10. de\\\"...\"           UDHR     danish  \n4 UDHR_english.pdf \"\\\"Universal \\\"...\"           UDHR     english \n5 UDHR_french.pdf  \"\\\"Déclaratio\\\"...\"           UDHR     french  \n6 UDHR_greek.pdf   \"\\\"ΟΙΚΟΥΜΕΝΙΚ\\\"...\"           UDHR     greek   \n# i 5 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Workflow\n\n`{quanteda}` has three basic types of objects:\n\n1.  Corpus\n    \n    * Saves character strings and variables in a data frame\n    * Combines texts with document-level variables\n\n2.  Tokens\n    \n    * Stores tokens in a list of vectors\n    * More efficient than character strings, but preserves positions of words \n    * Positional (string-of-words) analysis is performed using `textstat_collocations()`, `tokens_ngrams()` and `tokens_select()` or `fcm()` with `window` option\n\n3.  Document-feature matrix (DFM)\n\n    * Represents frequencies of features in documents in a matrix\n    * The most efficient structure, but it does not have information on positions of words \n    * Non-positional (bag-of-words) analysis are profrmed using many of the `textstat_*` and `textmodel_*` functions \n\nText analysis with `{quanteda}` goes through all those three types of objects either explicitly or implicitly.\n\n\n\n\n\n\n```{mermaid}\n    graph TD\n    D[Text files]\n    V[Document-level variables]\n    C(Corpus)\n    T(Tokens)\n    AP[\"Positional analysis (string-of-words)\"]\n    AN[\"Non-positional analysis (bag-of-words)\"]\n    M(DFM)\n    style C stroke-width:4px\n    style T stroke-width:4px\n    style M stroke-width:4px\n    D --> C\n    V --> C \n    C --> T \n    T --> M\n    T -.-> AP\n    M -.-> AN\n```\n\n\n\n\n\n\nFor example, if character vectors are given to `dfm()`, it internally constructs corpus and tokens objects before creating a DFM. \n\n### Corpus\n\nYou can create a corpus from various available sources:\n\n1. A character vector consisting of one document per element\n\n2. A data frame consisting of a character vector for documents, and additional vectors for document-level variables\n\n\n\n#### Character vector\n\n`data_char_ukimmig2010` is a named character vector and consists of sections of British election manifestos on immigration and asylum.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(data_char_ukimmig2010)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Named chr [1:9] \"IMMIGRATION: AN UNPARALLELED CRISIS WHICH ONLY THE BNP CAN SOLVE. \\n\\n- At current immigration and birth rates,\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:9] \"BNP\" \"Coalition\" \"Conservative\" \"Greens\" ...\n```\n\n\n:::\n\n```{.r .cell-code}\ncorp_immig <- corpus(\n  data_char_ukimmig2010, \n  docvars = data.frame(party = names(data_char_ukimmig2010))\n)\nprint(corp_immig)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 9 documents and 1 docvar.\nBNP :\n\"IMMIGRATION: AN UNPARALLELED CRISIS WHICH ONLY THE BNP CAN S...\"\n\nCoalition :\n\"IMMIGRATION.  The Government believes that immigration has e...\"\n\nConservative :\n\"Attract the brightest and best to our country. Immigration h...\"\n\nGreens :\n\"Immigration. Migration is a fact of life.  People have alway...\"\n\nLabour :\n\"Crime and immigration The challenge for Britain We will cont...\"\n\nLibDem :\n\"firm but fair immigration system Britain has always been an ...\"\n\n[ reached max_ndoc ... 3 more documents ]\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(corp_immig)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 9 documents, showing 9 documents:\n\n         Text Types Tokens Sentences        party\n          BNP  1125   3280        88          BNP\n    Coalition   142    260         4    Coalition\n Conservative   251    499        15 Conservative\n       Greens   322    679        21       Greens\n       Labour   298    683        29       Labour\n       LibDem   251    483        14       LibDem\n           PC    77    114         5           PC\n          SNP    88    134         4          SNP\n         UKIP   346    723        26         UKIP\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Data frame\n\nUsing `read_csv()`, load an example file from `path_data` as a data frame called `dat_inaug`. Note that your file does not need to be formatted as `.csv`. You can build a `{quanteda}` corpus from any file format that R can import as a data frame (see, for instance, the [**rio**](https://cran.r-project.org/web/packages/rio/index.html) package for importing various files as data frames into R).\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set path\npath_data <- system.file(\"extdata/\", package = \"readtext\")\n\n# import csv file\ndat_inaug <- read.csv(paste0(path_data, \"/csv/inaugCorpus.csv\"))\nnames(dat_inaug)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"texts\"     \"Year\"      \"President\" \"FirstName\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\nConstruct a corpus from the \"texts\" column in `dat_inaug`.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorp_inaug <- corpus(dat_inaug, text_field = \"texts\")\nprint(corp_inaug)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 5 documents and 3 docvars.\ntext1 :\n\"Fellow-Citizens of the Senate and of the House of Representa...\"\n\ntext2 :\n\"Fellow citizens, I am again called upon by the voice of my c...\"\n\ntext3 :\n\"When it was first perceived, in early times, that no middle ...\"\n\ntext4 :\n\"Friends and Fellow Citizens: Called upon to undertake the du...\"\n\ntext5 :\n\"Proceeding, fellow citizens, to that qualification which the...\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Document-level variables\n\n`{quanteda}`'s objects keep information associated with documents. They are called \"document-level variables\", or \"docvars\", and are accessed using `docvars()`.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorp <- data_corpus_inaugural\nhead(docvars(corp))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Year  President FirstName                 Party\n1 1789 Washington    George                  none\n2 1793 Washington    George                  none\n3 1797      Adams      John            Federalist\n4 1801  Jefferson    Thomas Democratic-Republican\n5 1805  Jefferson    Thomas Democratic-Republican\n6 1809    Madison     James Democratic-Republican\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIf you want to extract individual elements of document variables, you can specify `field`. Or you could just subset it as you normally would a data.frame.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndocvars(corp, field = \"Year\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1789 1793 1797 1801 1805 1809 1813 1817 1821 1825 1829 1833 1837 1841 1845\n[16] 1849 1853 1857 1861 1865 1869 1873 1877 1881 1885 1889 1893 1897 1901 1905\n[31] 1909 1913 1917 1921 1925 1929 1933 1937 1941 1945 1949 1953 1957 1961 1965\n[46] 1969 1973 1977 1981 1985 1989 1993 1997 2001 2005 2009 2013 2017 2021\n```\n\n\n:::\n\n```{.r .cell-code}\ncorp$Year\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1789 1793 1797 1801 1805 1809 1813 1817 1821 1825 1829 1833 1837 1841 1845\n[16] 1849 1853 1857 1861 1865 1869 1873 1877 1881 1885 1889 1893 1897 1901 1905\n[31] 1909 1913 1917 1921 1925 1929 1933 1937 1941 1945 1949 1953 1957 1961 1965\n[46] 1969 1973 1977 1981 1985 1989 1993 1997 2001 2005 2009 2013 2017 2021\n```\n\n\n:::\n:::\n\n\n\n\n\n\nSo that means assignments to *change* document-level variables will work as usual in R. For example, you can change the `Year` variable to a factor (if you wished). And since the output of a `docvars()` function is a data.frame, you could subset or filter as you would a data.frame.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndocvars(corp) |>\n  filter(Year >= 1990)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Year President FirstName      Party\n1 1993   Clinton      Bill Democratic\n2 1997   Clinton      Bill Democratic\n3 2001      Bush George W. Republican\n4 2005      Bush George W. Republican\n5 2009     Obama    Barack Democratic\n6 2013     Obama    Barack Democratic\n7 2017     Trump Donald J. Republican\n8 2021     Biden Joseph R. Democratic\n```\n\n\n:::\n\n```{.r .cell-code}\n# {quanteda} also provides corpus_subset() function, but since we learnt about\n# dplyr, we can use it here.\n```\n:::\n\n\n\n\n\n\nAnother useful feature is the ability to change the unit of texts. For example, the UK Immigration 2010 data set is a corpus of 9 documents, where each document is a speech by the political party.  \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorp <- corpus(data_char_ukimmig2010)\nprint(corp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 9 documents.\nBNP :\n\"IMMIGRATION: AN UNPARALLELED CRISIS WHICH ONLY THE BNP CAN S...\"\n\nCoalition :\n\"IMMIGRATION.  The Government believes that immigration has e...\"\n\nConservative :\n\"Attract the brightest and best to our country. Immigration h...\"\n\nGreens :\n\"Immigration. Migration is a fact of life.  People have alway...\"\n\nLabour :\n\"Crime and immigration The challenge for Britain We will cont...\"\n\nLibDem :\n\"firm but fair immigration system Britain has always been an ...\"\n\n[ reached max_ndoc ... 3 more documents ]\n```\n\n\n:::\n:::\n\n\n\n\n\n\nWe can use `corpus_reshape()` to change the unit of texts. For example, we can change the unit of texts to sentences using the command below.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorp_sent <- corpus_reshape(corp, to = \"sentences\")\nprint(corp_sent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 206 documents.\nBNP.1 :\n\"IMMIGRATION: AN UNPARALLELED CRISIS WHICH ONLY THE BNP CAN S...\"\n\nBNP.2 :\n\"The Scale of the Crisis  Britain's existence is in grave per...\"\n\nBNP.3 :\n\"In the absence of urgent action, we, the indigenous British ...\"\n\nBNP.4 :\n\"We, alone of all the political parties, have a decades-long ...\"\n\nBNP.5 :\n\"British People Set to be a Minority within 30 - 50 Years: Th...\"\n\nBNP.6 :\n\"Figures released by the ONS in January 2009 revealed that th...\"\n\n[ reached max_ndoc ... 200 more documents ]\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe following code restores it back to the document level.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorp_doc <- corpus_reshape(corp_sent, to = \"documents\")\nprint(corp_doc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 9 documents.\nBNP :\n\"IMMIGRATION: AN UNPARALLELED CRISIS WHICH ONLY THE BNP CAN S...\"\n\nCoalition :\n\"IMMIGRATION.  The Government believes that immigration has e...\"\n\nConservative :\n\"Attract the brightest and best to our country.  Immigration ...\"\n\nGreens :\n\"Immigration.  Migration is a fact of life.  People have alwa...\"\n\nLabour :\n\"Crime and immigration  The challenge for Britain  We will co...\"\n\nLibDem :\n\"firm but fair immigration system  Britain has always been an...\"\n\n[ reached max_ndoc ... 3 more documents ]\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### Tokens\n\n`tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries. \nBy default, tokens() only removes separators (typically white spaces), but you can also remove punctuation and numbers.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks <- tokens(corp_immig)\nprint(toks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTokens consisting of 9 documents and 1 docvar.\nBNP :\n [1] \"IMMIGRATION\"  \":\"            \"AN\"           \"UNPARALLELED\" \"CRISIS\"      \n [6] \"WHICH\"        \"ONLY\"         \"THE\"          \"BNP\"          \"CAN\"         \n[11] \"SOLVE\"        \".\"           \n[ ... and 3,268 more ]\n\nCoalition :\n [1] \"IMMIGRATION\" \".\"           \"The\"         \"Government\"  \"believes\"   \n [6] \"that\"        \"immigration\" \"has\"         \"enriched\"    \"our\"        \n[11] \"culture\"     \"and\"        \n[ ... and 248 more ]\n\nConservative :\n [1] \"Attract\"     \"the\"         \"brightest\"   \"and\"         \"best\"       \n [6] \"to\"          \"our\"         \"country\"     \".\"           \"Immigration\"\n[11] \"has\"         \"enriched\"   \n[ ... and 487 more ]\n\nGreens :\n [1] \"Immigration\" \".\"           \"Migration\"   \"is\"          \"a\"          \n [6] \"fact\"        \"of\"          \"life\"        \".\"           \"People\"     \n[11] \"have\"        \"always\"     \n[ ... and 667 more ]\n\nLabour :\n [1] \"Crime\"       \"and\"         \"immigration\" \"The\"         \"challenge\"  \n [6] \"for\"         \"Britain\"     \"We\"          \"will\"        \"control\"    \n[11] \"immigration\" \"with\"       \n[ ... and 671 more ]\n\nLibDem :\n [1] \"firm\"        \"but\"         \"fair\"        \"immigration\" \"system\"     \n [6] \"Britain\"     \"has\"         \"always\"      \"been\"        \"an\"         \n[11] \"open\"        \",\"          \n[ ... and 471 more ]\n\n[ reached max_ndoc ... 3 more documents ]\n```\n\n\n:::\n\n```{.r .cell-code}\ntoks_nopunct <- tokens(data_char_ukimmig2010, remove_punct = TRUE)\nprint(toks_nopunct)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTokens consisting of 9 documents.\nBNP :\n [1] \"IMMIGRATION\"  \"AN\"           \"UNPARALLELED\" \"CRISIS\"       \"WHICH\"       \n [6] \"ONLY\"         \"THE\"          \"BNP\"          \"CAN\"          \"SOLVE\"       \n[11] \"At\"           \"current\"     \n[ ... and 2,839 more ]\n\nCoalition :\n [1] \"IMMIGRATION\"  \"The\"          \"Government\"   \"believes\"     \"that\"        \n [6] \"immigration\"  \"has\"          \"enriched\"     \"our\"          \"culture\"     \n[11] \"and\"          \"strengthened\"\n[ ... and 219 more ]\n\nConservative :\n [1] \"Attract\"     \"the\"         \"brightest\"   \"and\"         \"best\"       \n [6] \"to\"          \"our\"         \"country\"     \"Immigration\" \"has\"        \n[11] \"enriched\"    \"our\"        \n[ ... and 440 more ]\n\nGreens :\n [1] \"Immigration\" \"Migration\"   \"is\"          \"a\"           \"fact\"       \n [6] \"of\"          \"life\"        \"People\"      \"have\"        \"always\"     \n[11] \"moved\"       \"from\"       \n[ ... and 598 more ]\n\nLabour :\n [1] \"Crime\"       \"and\"         \"immigration\" \"The\"         \"challenge\"  \n [6] \"for\"         \"Britain\"     \"We\"          \"will\"        \"control\"    \n[11] \"immigration\" \"with\"       \n[ ... and 608 more ]\n\nLibDem :\n [1] \"firm\"        \"but\"         \"fair\"        \"immigration\" \"system\"     \n [6] \"Britain\"     \"has\"         \"always\"      \"been\"        \"an\"         \n[11] \"open\"        \"welcoming\"  \n[ ... and 423 more ]\n\n[ reached max_ndoc ... 3 more documents ]\n```\n\n\n:::\n:::\n\n\n\n\n\n\nYou can see how keywords are used in the actual contexts in a concordance view produced by `kwic()`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkw <- kwic(toks, pattern =  \"immig*\")\nhead(kw, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKeyword-in-context with 10 matches.                                                                 \n   [BNP, 1]                                       | IMMIGRATION |\n  [BNP, 16]                   SOLVE. - At current | immigration |\n  [BNP, 78]                 a halt to all further | immigration |\n  [BNP, 85]        the deportation of all illegal | immigrants  |\n [BNP, 169]          Britain, regardless of their | immigration |\n [BNP, 197] admission that they orchestrated mass | immigration |\n [BNP, 272]            grave peril, threatened by | immigration |\n [BNP, 374]                  ), legal Third World | immigrants  |\n [BNP, 531]        to second and third generation |  immigrant  |\n [BNP, 661]                     are added in, the |  immigrant  |\n                                          \n : AN UNPARALLELED CRISIS WHICH           \n and birth rates, indigenous              \n , the deportation of all                 \n , a halt to the                          \n status. - The BNP                        \n to change forcibly Britain's demographics\n and multiculturalism. In the             \n made up 14.7 percent (                   \n mothers. Figures released by             \n birth rate is estimated to               \n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: {.callout-note}\n\n1. If you want to find multi-word expressions, separate words by white space and wrap the character vector by `phrase()`, as follows:\n\n    ```r\n    kw_asylum <- kwic(toks, pattern = phrase(\"asylum seeker*\"))\n    ```\n\n\n\n2. Texts do not always appear nicely in your R console, so you can use `View()` to see the keywords-in-context in an interactive HTML table.\n\n:::\n\nYou can remove tokens that you are not interested in using `tokens_select()`. Usually we remove function words (grammatical words) that have little or no substantive meaning in pre-processing. `stopwords()` returns a pre-defined list of function words.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_nostop <- tokens_select(\n  toks, \n  pattern = stopwords(\"en\"), \n  selection = \"remove\"  # keep or remove\n)\nprint(toks_nostop)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTokens consisting of 9 documents and 1 docvar.\nBNP :\n [1] \"IMMIGRATION\"  \":\"            \"UNPARALLELED\" \"CRISIS\"       \"BNP\"         \n [6] \"CAN\"          \"SOLVE\"        \".\"            \"-\"            \"current\"     \n[11] \"immigration\"  \"birth\"       \n[ ... and 2,109 more ]\n\nCoalition :\n [1] \"IMMIGRATION\"  \".\"            \"Government\"   \"believes\"     \"immigration\" \n [6] \"enriched\"     \"culture\"      \"strengthened\" \"economy\"      \",\"           \n[11] \"must\"         \"controlled\"  \n[ ... and 146 more ]\n\nConservative :\n [1] \"Attract\"     \"brightest\"   \"best\"        \"country\"     \".\"          \n [6] \"Immigration\" \"enriched\"    \"nation\"      \"years\"       \"want\"       \n[11] \"attract\"     \"brightest\"  \n[ ... and 277 more ]\n\nGreens :\n [1] \"Immigration\" \".\"           \"Migration\"   \"fact\"        \"life\"       \n [6] \".\"           \"People\"      \"always\"      \"moved\"       \"one\"        \n[11] \"country\"     \"another\"    \n[ ... and 377 more ]\n\nLabour :\n [1] \"Crime\"            \"immigration\"      \"challenge\"        \"Britain\"         \n [5] \"control\"          \"immigration\"      \"new\"              \"Australian-style\"\n [9] \"points-based\"     \"system\"           \"-\"                \"unlike\"          \n[ ... and 391 more ]\n\nLibDem :\n [1] \"firm\"        \"fair\"        \"immigration\" \"system\"      \"Britain\"    \n [6] \"always\"      \"open\"        \",\"           \"welcoming\"   \"country\"    \n[11] \",\"           \"thousands\"  \n[ ... and 285 more ]\n\n[ reached max_ndoc ... 3 more documents ]\n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: {.callout-note}\n\nThe `stopwords()` function returns character vectors of stopwords for different languages, using the ISO-639-1 language codes. For **Malay**, use `stopwords(\"ms\", source = \"stopwords-iso\")`. For Bruneian specific context, you may need to amend the stopwords yourselves.\n\n:::\n\nYou can generate n-grams in any lengths from a tokens using `tokens_ngrams()`. N-grams are a contiguous sequence of n tokens from already tokenized text objects. So for example, in the phrase \"natural language processing\":\n\n- Unigram (1-gram): \"natural\", \"language\", \"processing\"\n- Bigram (2-gram): \"natural language\", \"language processing\"\n- Trigram (3-gram): \"natural language processing\"\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokens_ngrams() also supports skip to generate skip-grams.\ntoks_ngram <- tokens_ngrams(toks_nopunct, n = 3, skip = 0)\nhead(toks_ngram[[1]], 20)  # the first political party's trigrams\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"IMMIGRATION_AN_UNPARALLELED\" \"AN_UNPARALLELED_CRISIS\"     \n [3] \"UNPARALLELED_CRISIS_WHICH\"   \"CRISIS_WHICH_ONLY\"          \n [5] \"WHICH_ONLY_THE\"              \"ONLY_THE_BNP\"               \n [7] \"THE_BNP_CAN\"                 \"BNP_CAN_SOLVE\"              \n [9] \"CAN_SOLVE_At\"                \"SOLVE_At_current\"           \n[11] \"At_current_immigration\"      \"current_immigration_and\"    \n[13] \"immigration_and_birth\"       \"and_birth_rates\"            \n[15] \"birth_rates_indigenous\"      \"rates_indigenous_British\"   \n[17] \"indigenous_British_people\"   \"British_people_are\"         \n[19] \"people_are_set\"              \"are_set_to\"                 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n### Document feature matrix\n\n`dfm()` constructs a document-feature matrix (DFM) from a tokens object.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_inaug <- tokens(data_corpus_inaugural, remove_punct = TRUE)\ndfmat_inaug <- dfm(toks_inaug)\nprint(dfmat_inaug)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDocument-feature matrix of: 59 documents, 9,419 features (91.89% sparse) and 4 docvars.\n                 features\ndocs              fellow-citizens  of the senate and house representatives\n  1789-Washington               1  71 116      1  48     2               2\n  1793-Washington               0  11  13      0   2     0               0\n  1797-Adams                    3 140 163      1 130     0               2\n  1801-Jefferson                2 104 130      0  81     0               0\n  1805-Jefferson                0 101 143      0  93     0               0\n  1809-Madison                  1  69 104      0  43     0               0\n                 features\ndocs              among vicissitudes incident\n  1789-Washington     1            1        1\n  1793-Washington     0            0        0\n  1797-Adams          4            0        0\n  1801-Jefferson      1            0        0\n  1805-Jefferson      7            0        0\n  1809-Madison        0            0        0\n[ reached max_ndoc ... 53 more documents, reached max_nfeat ... 9,409 more features ]\n```\n\n\n:::\n:::\n\n\n\n\n\n\nSome useful functions to operate on DFMs are:\n\n1. `ndoc()`: returns the number of documents\n2. `nfeat()`: returns the number of features\n3. `docnames()`: returns the document names\n4. `featnames()`: returns the feature (column) names\n5. `topfeatures()`: returns the most frequent features\n6. `docvars()`: returns the document-level variables\n\nDFMs sometimes behaves like normal matrices too, so you can use `rowSums()` and `colSums()` to calculate marginals. \n\nMost commonly perhaps, is you want to select some columns (i.e. features) from the DFM that satisfy a pattern. For instance,\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfm_select(dfmat_inaug, pattern = \"freedom\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDocument-feature matrix of: 59 documents, 1 feature (38.98% sparse) and 4 docvars.\n                 features\ndocs              freedom\n  1789-Washington       0\n  1793-Washington       0\n  1797-Adams            0\n  1801-Jefferson        4\n  1805-Jefferson        2\n  1809-Madison          1\n[ reached max_ndoc ... 53 more documents ]\n```\n\n\n:::\n\n```{.r .cell-code}\ndfm_keep(dfmat_inaug, min_nchar = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDocument-feature matrix of: 59 documents, 8,560 features (93.04% sparse) and 4 docvars.\n                 features\ndocs              fellow-citizens senate house representatives among\n  1789-Washington               1      1     2               2     1\n  1793-Washington               0      0     0               0     0\n  1797-Adams                    3      1     0               2     4\n  1801-Jefferson                2      0     0               0     1\n  1805-Jefferson                0      0     0               0     7\n  1809-Madison                  1      0     0               0     0\n                 features\ndocs              vicissitudes incident event could filled\n  1789-Washington            1        1     2     3      1\n  1793-Washington            0        0     0     0      0\n  1797-Adams                 0        0     0     1      0\n  1801-Jefferson             0        0     0     0      0\n  1805-Jefferson             0        0     0     2      0\n  1809-Madison               0        0     0     1      1\n[ reached max_ndoc ... 53 more documents, reached max_nfeat ... 8,550 more features ]\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThere is also `dfm_trim()` to remove features that are too frequent or too rare, based on frequencies.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Trim DFM containing features that occur less than 10 times in the corpus\ndfm_trim(dfmat_inaug, min_termfreq = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDocument-feature matrix of: 59 documents, 1,524 features (68.93% sparse) and 4 docvars.\n                 features\ndocs              fellow-citizens  of the senate and house representatives\n  1789-Washington               1  71 116      1  48     2               2\n  1793-Washington               0  11  13      0   2     0               0\n  1797-Adams                    3 140 163      1 130     0               2\n  1801-Jefferson                2 104 130      0  81     0               0\n  1805-Jefferson                0 101 143      0  93     0               0\n  1809-Madison                  1  69 104      0  43     0               0\n                 features\ndocs              among to life\n  1789-Washington     1 48    1\n  1793-Washington     0  5    0\n  1797-Adams          4 72    2\n  1801-Jefferson      1 61    1\n  1805-Jefferson      7 83    2\n  1809-Madison        0 61    1\n[ reached max_ndoc ... 53 more documents, reached max_nfeat ... 1,514 more features ]\n```\n\n\n:::\n\n```{.r .cell-code}\n# Trim DFM containing features that occur in more than 10% of the documents\ndfm_trim(dfmat_inaug, max_docfreq = 0.1, docfreq_type = \"prop\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDocument-feature matrix of: 59 documents, 7,388 features (96.87% sparse) and 4 docvars.\n                 features\ndocs              vicissitudes filled anxieties notification transmitted 14th\n  1789-Washington            1      1         1            1           1    1\n  1793-Washington            0      0         0            0           0    0\n  1797-Adams                 0      0         0            0           0    0\n  1801-Jefferson             0      0         0            0           0    0\n  1805-Jefferson             0      0         0            0           0    0\n  1809-Madison               0      1         0            0           0    0\n                 features\ndocs              month summoned veneration fondest\n  1789-Washington     1        1          1       1\n  1793-Washington     0        0          0       0\n  1797-Adams          0        0          2       0\n  1801-Jefferson      0        0          0       0\n  1805-Jefferson      0        0          0       0\n  1809-Madison        0        0          0       0\n[ reached max_ndoc ... 53 more documents, reached max_nfeat ... 7,378 more features ]\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n## Statistical analysis\n\nNote: If you have not installed `{quanteda.corpora}`, do so by running\n\n```r\nremotes::install_github(\"quanteda/quanteda.corpora\")\n```\n\n### Simple frequency analysis\n\nUnlike `topfeatures()`, `textstat_frequency()` shows both term and document frequencies. You can also use the function to find the most frequent features within groups. Using the `download()` function from `{quanteda.corpora}`, you can retrieve a text corpus of tweets.\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorp_tweets <- quanteda.corpora::download(url = \"https://www.dropbox.com/s/846skn1i5elbnd2/data_corpus_sampletweets.rds?dl=1\")\n```\n:::\n\n\n\n\n\n\nWe can analyse the most frequent hashtags by applying `tokens_keep(pattern = \"#*\")` before creating a DFM.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_tweets <- \n  tokens(corp_tweets, remove_punct = TRUE) |>\n  tokens_keep(pattern = \"#*\")\ndfmat_tweets <- dfm(toks_tweets)\n\ntstat_freq <- textstat_frequency(dfmat_tweets, n = 5, groups = lang)\nhead(tstat_freq, 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             feature frequency rank docfreq     group\n1           #twitter         1    1       1    Basque\n2     #canviemeuropa         1    1       1    Basque\n3             #prest         1    1       1    Basque\n4           #psifizo         1    1       1    Basque\n5     #ekloges2014gr         1    1       1    Basque\n6            #ep2014         1    1       1 Bulgarian\n7         #yourvoice         1    1       1 Bulgarian\n8      #eudebate2014         1    1       1 Bulgarian\n9            #велико         1    1       1 Bulgarian\n10           #ep2014         1    1       1  Croatian\n11 #savedonbaspeople         1    1       1  Croatian\n12   #vitoriagasteiz         1    1       1  Croatian\n13           #ep14dk        32    1      32    Danish\n14            #dkpol        18    2      18    Danish\n15            #eupol         7    3       7    Danish\n16        #vindtilep         7    3       7    Danish\n17    #patentdomstol         4    5       4    Danish\n18           #ep2014        35    1      35     Dutch\n19              #vvd        11    2      11     Dutch\n20               #eu         9    3       7     Dutch\n```\n\n\n:::\n:::\n\n\n\n\n\n\nYou can also plot the Twitter hashtag frequencies easily using `ggplot()`.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfmat_tweets |>\n  textstat_frequency(n = 15) |>\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_point() +\n  coord_flip() +\n  labs(x = NULL, y = \"Frequency\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](3-quanteda_files/figure-pdf/unnamed-chunk-38-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nAlternatively, you can create a word cloud of the 100 most common hashtags.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(132)\ntextplot_wordcloud(dfmat_tweets, max_words = 100)\n```\n\n::: {.cell-output-display}\n![](3-quanteda_files/figure-pdf/unnamed-chunk-39-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nFinally, it is possible to compare different groups within one Wordcloud. We must first create a dummy variable that indicates whether a tweet was posted in English or a different language. Afterwards, we can compare the most frequent hashtags of English and non-English tweets.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create document-level variable indicating whether tweet was in English or\n# other language\ncorp_tweets$dummy_english <- \n  factor(ifelse(corp_tweets$lang == \"English\", \"English\", \"Not English\"))\n\n# tokenize texts\ntoks_tweets <- tokens(corp_tweets)\n\n# create a grouped dfm and compare groups\ndfmat_corp_language <- \n  dfm(toks_tweets) |>\n  dfm_keep(pattern = \"#*\") |>\n  dfm_group(groups = dummy_english)\n\n# create wordcloud\nset.seed(132) # set seed for reproducibility\ntextplot_wordcloud(dfmat_corp_language, comparison = TRUE, max_words = 200)\n```\n\n::: {.cell-output-display}\n![](3-quanteda_files/figure-pdf/unnamed-chunk-40-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n### Lexical diversity\n\nLexical diversity is a measure of how varied the vocabulary in a text or speech is. It indicates the richness of language use by comparing the number of unique words (types) to the total number of words (tokens) in the text.  It is useful, for instance, for analysing speakers’ or writers’ linguistic skills, or the complexity of ideas expressed in documents.\n\nA common metric for lexical diversity is the Type-Token Ratio (TTR), calculated as:\n$$\nTTR = \\frac{N_{\\text{types}}}{N_{\\text{tokens}}}\n$$\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_inaug <- tokens(data_corpus_inaugural)\ndfmat_inaug <- \n  dfm(toks_inaug) |>\n  dfm_remove(pattern = stopwords(\"en\"))  # similar to dfm_select()\n\ntstat_lexdiv <- textstat_lexdiv(dfmat_inaug)\ntail(tstat_lexdiv, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     document       TTR\n55  2005-Bush 0.6176753\n56 2009-Obama 0.6828645\n57 2013-Obama 0.6605238\n58 2017-Trump 0.6409537\n59 2021-Biden 0.5572316\n```\n\n\n:::\n:::\n\n\n\n\n\n\nWe can prepare a plot using `ggplot()` as follows:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_df <-\n  tstat_lexdiv |>\n  mutate(id = row_number())\n\nggplot(plot_df, aes(id, TTR)) +\n  geom_line() +\n  scale_x_continuous(\n    breaks = plot_df$id,\n    labels = plot_df$document,\n    name = NULL\n  ) +\n  scale_y_continuous(labels = scales::percent) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](3-quanteda_files/figure-pdf/unnamed-chunk-42-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n### Document/Feature similarity\n\nDocument/feature similarity is a measure of how alike two documents or sets of features are based on their content. It quantifies the degree to which documents share similar terms, topics, or characteristics.\n\n`textstat_dist()` calculates similarities of documents or features for various measures. The output is compatible with R’s `dist()`, so hierarchical clustering can be performed without any transformation.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_inaug <- tokens(data_corpus_inaugural)\ndfmat_inaug <- \n  dfm(toks_inaug) |>\n  dfm_remove(pattern = stopwords(\"en\"))  # similar to dfm_select()\n\n# Calculate document similarity\ndist_mat <- textstat_dist(dfmat_inaug)  # using Euclidean distance\ndist_mat[1:3, 1:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3 x 3 Matrix of class \"dspMatrix\"\n                1789-Washington 1793-Washington 1797-Adams\n1789-Washington         0.00000        76.13803   141.4072\n1793-Washington        76.13803         0.00000   206.6954\n1797-Adams            141.40721       206.69543     0.0000\n```\n\n\n:::\n:::\n\n\n\n\n\n\nTo plot this using `ggplot()`, we rely on the `{ggdendro}` package. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclust <- hclust(as.dist(dist_mat))  # hierarchical clustering\n\nlibrary(ggdendro)\ndendr <- dendro_data(clust)\nggdendrogram(dendr, rotate = TRUE) \n```\n\n::: {.cell-output-display}\n![](3-quanteda_files/figure-pdf/unnamed-chunk-44-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n### Feature co-occurence matrix\n\nA feature co-occurrence matrix (FCM) is a square matrix that counts the number of times two features co-occur in the same context, such as within the same document, sentence, or window of text. This is a special object in `{quanteda}`, but behaves similarly to a DFM. As an example, consider the following:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  doc_id = 1:2,\n  text = c(\"I love Mathematics.\", \"Mathematics is awesome.\")\n) |>\n  corpus() |>\n  tokens(remove_punct = TRUE) |>\n  fcm(context = \"document\")  # by default\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFeature co-occurrence matrix of: 5 by 5 features.\n             features\nfeatures      I love Mathematics is awesome\n  I           0    1           1  0       0\n  love        0    0           1  0       0\n  Mathematics 0    0           0  1       1\n  is          0    0           0  0       1\n  awesome     0    0           0  0       0\n```\n\n\n:::\n:::\n\n\n\n\n\n\nLet's download the `data_corpus_guardian` corpus from the `{quanteda.corpora}` package. \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorp_news <- quanteda.corpora::download(\"data_corpus_guardian\")\n```\n:::\n\n\n\n\n\n\nWhen a corpus is large, you have to select features of a DFM before constructing a FCM. In the example below, we clean up as follows:\n\n1. Remove all stopwords and punctuation characters. \n2. Remove certain patterns that usually describe the publication time and date of articles. \n3. Keep only terms that occur at least 100 times in the document-feature matrix.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_news <- tokens(corp_news, remove_punct = TRUE)\ndfmat_news <- \n  dfm(toks_news) |>\n  dfm_remove(pattern = c(stopwords(\"en\"), \"*-time\", \"updated-*\", \"gmt\", \"bst\", \"|\")) |>\n  dfm_trim(min_termfreq = 100)\n\ntopfeatures(dfmat_news)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      said     people        one        new       also         us        can \n     28413      11169       9884       8024       7901       7091       6972 \ngovernment       year       last \n      6821       6570       6335 \n```\n\n\n:::\n\n```{.r .cell-code}\nnfeat(dfmat_news)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4211\n```\n\n\n:::\n:::\n\n\n\n\n\n\nTo construct an FCM from a DFM (or a tokens object), use `fcm()`. You can visualise the FCM using a `textplot_network()` graph as follows:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfcmat_news <- fcm(dfmat_news, context = \"document\")\nfeat <- names(topfeatures(dfmat_news, 30))  # Top 30 features\nfcmat_news_select <- fcm_select(fcmat_news, pattern = feat, selection = \"keep\")\ndim(fcmat_news_select)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 30 30\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(123)\nquanteda.textplots::textplot_network(fcmat_news_select)\n```\n\n::: {.cell-output-display}\n![](3-quanteda_files/figure-pdf/unnamed-chunk-49-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n## Scaling and classification\n\nIn this section we apply mainly unsupervised learning models to textual data. \nScaling and classification aim to uncover hidden structures, relationships, and patterns within textual data by placing texts or words on latent scales (scaling) and grouping them into meaningful categories or themes (classification). This process transforms complex, high-dimensional text into more interpretable and actionable insights.\n\n### Wordfish\n\nWordfish is a Poisson scaling model of one-dimensional document positions [@slapin2008scaling]. This model is used primarily for scaling political texts to position documents (like speeches or manifestos) on a latent dimension, often reflecting ideological or policy positions. The main objective is to identify the relative positioning of texts on a scale (e.g., left-right political spectrum) based on word frequencies.\n\nLet $y_{ij}$ be the count of word $j$ in document $i$. Then assume \n\n\\begin{align}\ny_{ij} &\\sim \\operatorname{Poi}(\\lambda_{ij}) \\\\\n\\log (\\lambda_{ij}) &= \\psi_j +\\beta_j \\theta_i\n\\end{align}\n\nIn this example, we will show how to apply Wordfish to the Irish budget speeches from 2010. First, we will create a document-feature matrix. Afterwards, we will run Wordfish.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_irish <- tokens(data_corpus_irishbudget2010, remove_punct = TRUE)\ndfmat_irish <- dfm(toks_irish)\n\n# Run Wordfish model\ntmod_wf <- textmodel_wordfish(dfmat_irish, dir = c(6, 5))\nsummary(tmod_wf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\ntextmodel_wordfish.dfm(x = dfmat_irish, dir = c(6, 5))\n\nEstimated Document Positions:\n                             theta      se\nLenihan, Brian (FF)        1.79403 0.02007\nBruton, Richard (FG)      -0.62160 0.02823\nBurton, Joan (LAB)        -1.13503 0.01568\nMorgan, Arthur (SF)       -0.07841 0.02896\nCowen, Brian (FF)          1.77846 0.02330\nKenny, Enda (FG)          -0.75350 0.02635\nODonnell, Kieran (FG)     -0.47615 0.04309\nGilmore, Eamon (LAB)      -0.58406 0.02994\nHiggins, Michael (LAB)    -1.00383 0.03964\nQuinn, Ruairi (LAB)       -0.92648 0.04183\nGormley, John (Green)      1.18361 0.07224\nRyan, Eamon (Green)        0.14738 0.06321\nCuffe, Ciaran (Green)      0.71541 0.07291\nOCaolain, Caoimhghin (SF) -0.03982 0.03878\n\nEstimated Feature Scores:\n        when      i presented    the supplementary  budget     to   this  house\nbeta -0.1594 0.3177    0.3603 0.1933         1.077 0.03537 0.3077 0.2473 0.1399\npsi   1.6241 2.7239   -1.7958 5.3308        -1.134 2.70993 4.5190 3.4603 1.0396\n       last   april    said     we   could   work    our   way through  period\nbeta 0.2419 -0.1565 -0.8339 0.4156 -0.6138 0.5221 0.6892 0.275  0.6115  0.4985\npsi  0.9853 -0.5725 -0.4514 3.5125  1.0858 1.1151 2.5278 1.419  1.1603 -0.1779\n         of severe economic distress   today   can  report    that\nbeta 0.2777  1.229   0.4237    1.799 0.09141 0.304  0.6196 0.01506\npsi  4.4656 -2.013   1.5714   -4.456 0.83875 1.564 -0.2466 3.83785\n     notwithstanding difficulties   past\nbeta           1.799        1.175 0.4746\npsi           -4.456       -1.357 0.9321\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe R output shows the results of the Wordfish model applied to Irish political texts, estimating the ideological positions of various politicians. Each politician is assigned a “theta” value, representing their placement on a latent scale; positive values indicate one end of the spectrum, while negative values indicate the opposite. \n\nFor example, Brian Lenihan (FF) has a high positive theta, suggesting a strong position on one side, while Joan Burton (LAB) has a negative theta, placing her on the other side. The model also provides feature scores for words (beta values), indicating their importance in distinguishing between these positions. Words with higher absolute beta values, such as “supplementary,” are key in differentiating the ideological content of the texts, while psi values reflect word frequency variance, contributing to the model’s differentiation of document positions.\n\nWe can plot the results of a fitted scaling model using `textplot_scale1d()`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntextplot_scale1d(tmod_wf)\n```\n\n::: {.cell-output-display}\n![](3-quanteda_files/figure-pdf/unnamed-chunk-51-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n::: {.callout-note}\nThe value of 0 for theta in the Wordfish model is not a true zero in an absolute sense. Instead, it serves as a relative reference point on the latent scale. In Wordfish, theta values are relative, meaning they indicate positions along a spectrum where the direction (positive or negative) is determined by the model’s scaling based on the data and specified parameters.\n:::\n\nThe function also allows you to plot scores by a grouping variable, in this case the party affiliation of the speakers.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntextplot_scale1d(tmod_wf, groups = dfmat_irish$party)\n```\n\n::: {.cell-output-display}\n![](3-quanteda_files/figure-pdf/unnamed-chunk-52-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nFinally, we can plot the estimated word positions and highlight certain features.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntextplot_scale1d(\n  tmod_wf, \n  margin = \"features\", \n  highlighted = c(\"government\", \"global\", \"children\", \n                  \"bank\", \"economy\", \"the\", \"citizenship\",\n                  \"productivity\", \"deficit\")\n)\n```\n\n::: {.cell-output-display}\n![](3-quanteda_files/figure-pdf/unnamed-chunk-53-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nBeta (x-axis) Reflects how strongly a word is associated with the latent dimension (e.g., ideological position). Words with high absolute beta values are more influential in distinguishing between different positions; positive beta values indicate words more associated with one end of the scale, while negative values indicate the opposite.\n\nPsi (y-axis) Represents the variance in word frequency. Higher psi values suggest that the word occurs with varying frequency across documents, while lower values indicate more consistent usage.\n\nTherefore, words in the upper right (high beta, high psi) are influential and variably used, indicating key terms that may strongly differentiate between document positions. Words in the lower left (low beta, low psi) are less influential and consistently used, likely serving as common or neutral terms.\n\nThe plot also helps identify which words are driving the distinctions in the latent scale and how their usage varies across documents.\n\n### Topic models\n\nTopic models are statistical models used to identify the underlying themes or topics within a large collection of documents. They analyze word co-occurrences across documents to group words into topics, where each topic is a distribution over words, and each document is a mixture of these topics.\n\nA common topic model is Latent Dirichlet Allocation (LDA), which assumes that each document contains multiple topics in varying proportions. Topic models help uncover hidden semantic structures in text, making them useful for organizing, summarizing, and exploring large text datasets. In R, we use the `{seededlda}` package for LDA.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"seededlda\")\nlibrary(seededlda)\n```\n:::\n\n\n\n\n\n\nBack to the Guardian data, `corp_news`.\nWe will select only news articles published in 2016 using `corpus_subset()` function and the `year()` function from the `{lubridate}` package \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorp_news_2016 <- corpus_subset(corp_news, year(date) == 2016)\nndoc(corp_news_2016)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1959\n```\n\n\n:::\n:::\n\n\n\n\n\n\nFurther, after removal of function words and punctuation in `dfm()`, we will only keep the top 20% of the most frequent features (`min_termfreq = 0.8`) that appear in less than 10% of all documents (`max_docfreq = 0.1`) using `dfm_trim()` to focus on common but distinguishing features.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create tokens\ntoks_news <- \n  tokens(\n    corp_news_2016, \n    remove_punct = TRUE, \n    remove_numbers = TRUE, \n    remove_symbol = TRUE\n  ) |>\n  tokens_remove(\n    pattern = c(stopwords(\"en\"), \"*-time\", \"updated-*\", \"gmt\", \"bst\")\n  )\n\n# Create DFM\ndfmat_news <- \n  dfm(toks_news) %>% \n  dfm_trim(\n    min_termfreq = 0.8, \n    termfreq_type = \"quantile\",\n    max_docfreq = 0.1, \n    docfreq_type = \"prop\"\n  )\n```\n:::\n\n\n\n\n\n\nThe LDA is fitted using the code below. Note that `k = 10` specifies the number of topics to be discovered. This is an important parameter and you should try a variety of values and validate the outputs of your topic models thoroughly.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Takes a while to fit!\ntmod_lda <- seededlda::textmodel_lda(dfmat_news, k = 10)\n```\n:::\n\n\n\n\n\n\nYou can extract the most important terms for each topic from the model using `terms()`. Each column (`topic1`, `topic2`, etc.) lists words that frequently co-occur in the dataset, suggesting a common theme within each topic.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nterms(tmod_lda, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      topic1     topic2       topic3         topic4       topic5    topic6    \n [1,] \"syria\"    \"labor\"      \"johnson\"      \"funding\"    \"son\"     \"officers\"\n [2,] \"refugees\" \"australia\"  \"brussels\"     \"housing\"    \"church\"  \"violence\"\n [3,] \"isis\"     \"australian\" \"talks\"        \"nhs\"        \"black\"   \"doctors\" \n [4,] \"military\" \"corbyn\"     \"boris\"        \"income\"     \"love\"    \"prison\"  \n [5,] \"syrian\"   \"turnbull\"   \"benefits\"     \"education\"  \"father\"  \"victims\" \n [6,] \"un\"       \"budget\"     \"summit\"       \"scheme\"     \"felt\"    \"sexual\"  \n [7,] \"islamic\"  \"leadership\" \"negotiations\" \"fund\"       \"parents\" \"abuse\"   \n [8,] \"forces\"   \"shadow\"     \"ireland\"      \"green\"      \"story\"   \"hospital\"\n [9,] \"turkey\"   \"senate\"     \"migrants\"     \"homes\"      \"visit\"   \"criminal\"\n[10,] \"muslim\"   \"coalition\"  \"greece\"       \"businesses\" \"read\"    \"crime\"   \n      topic7      topic8    topic9          topic10     \n [1,] \"oil\"       \"clinton\" \"climate\"       \"sales\"     \n [2,] \"markets\"   \"sanders\" \"water\"         \"apple\"     \n [3,] \"prices\"    \"cruz\"    \"energy\"        \"customers\" \n [4,] \"banks\"     \"hillary\" \"food\"          \"users\"     \n [5,] \"investors\" \"obama\"   \"gas\"           \"google\"    \n [6,] \"shares\"    \"trump's\" \"drug\"          \"technology\"\n [7,] \"trading\"   \"bernie\"  \"drugs\"         \"games\"     \n [8,] \"china\"     \"ted\"     \"environmental\" \"game\"      \n [9,] \"rates\"     \"rubio\"   \"air\"           \"iphone\"    \n[10,] \"quarter\"   \"senator\" \"emissions\"     \"app\"       \n```\n\n\n:::\n:::\n\n\n\n\n\n\nAs an example, Topic 1 (\"syria\", \"refugees\", \"isis\"), is likely related to international conflicts, specifically around Syria and refugee crises. Topic 4 (\"funding\", \"housing\", \"nhs\") is likely related to public services and social welfare issues, such as healthcare and housing.\nEach topic provides a distinct theme, derived from the words that frequently appear together in the corpus, helping to summarize and understand the main themes in the text.\n\nYou can then obtain the most likely topics using topics() and save them as a document-level variable.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# assign topic as a new document-level variable\ndfmat_news$topic <- topics(tmod_lda)\n\n# cross-table of the topic frequency\ntable(dfmat_news$topic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n topic1  topic2  topic3  topic4  topic5  topic6  topic7  topic8  topic9 topic10 \n    203     222      85     211     249     236     180     186     196     184 \n```\n\n\n:::\n:::\n\n\n\n\n\n\nIn the seeded LDA, you can pre-define topics in LDA using a dictionary of \"seeded\" words. For more information, see the `{seededlda}` package documentation.\n\n### Latent semantic scaling\n\nLatent Semantic Scaling (LSS) is a method used to place words or documents on a latent scale that represents an underlying dimension, such as sentiment, ideology, or any other semantic axis. The key idea is to use the co-occurrence patterns of words across documents to identify and position items along this hidden dimension.\n\nLSS is performed using the `{LSX}` package. In this example, we will apply LSS to the corpus of Sputnik articles about Ukraine. First, we prepare the data set.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read the RDS file directly from the URL\ncorp <- readRDS(url(\"https://www.dropbox.com/s/abme18nlrwxgmz8/data_corpus_sputnik2022.rds?dl=1\"))\n\ntoks <-\n  corp |>\n  corpus_reshape(\"sentences\") |>  # this is a must!\n  tokens(\n    remove_punct = TRUE, \n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE\n  )\n\ndfmt <-\n  dfm(toks) |>\n  dfm_remove(pattern = stopwords(\"en\"))\n```\n:::\n\n\n\n\n\n\nNow to run an LSS model, run the following command:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlss <- textmodel_lss(\n  dfmt, \n  seeds = as.seedwords(data_dictionary_sentiment), \n  k = 300, \n  # cache = TRUE, \n  include_data = TRUE, \n  group_data = TRUE\n)\n```\n:::\n\n\n\n\n\n\nTaking the DFM and the seed words as the only inputs, `textmodel_lss()` computes the *polarity* scores of all the words in the corpus based on their semantic similarity to the seed words. You usually do not need to change the value of `k` (300 by default).\n\nLet's look at the output of the LSS model:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\ntextmodel_lss(x = dfmt, seeds = as.seedwords(data_dictionary_sentiment), \n    k = 300, include_data = TRUE, group_data = TRUE)\n\nSeeds:\n       good        nice   excellent    positive   fortunate     correct \n          1           1           1           1           1           1 \n   superior         bad       nasty        poor    negative unfortunate \n          1          -1          -1          -1          -1          -1 \n      wrong    inferior \n         -1          -1 \n\nBeta:\n(showing first 30 elements)\n          excellent            positive              gander                good \n             0.2102              0.1918              0.1883              0.1829 \n         diplomatic           staffer's              ex-kgb             correct \n             0.1802              0.1773              0.1771              0.1749 \n               mend  inter-parliamntary         workmanship      russo-american \n             0.1743              0.1719              0.1715              0.1713 \n             nicety           china-u.s               soufi good-neighborliness \n             0.1712              0.1674              0.1664              0.1644 \n       china-canada             fidgety           relations           downtrend \n             0.1616              0.1611              0.1584              0.1578 \n            cordial        canada-china            superior       understanding \n             0.1573              0.1569              0.1569              0.1561 \n   good-neighbourly           brennan's              mutual          reaffirmed \n             0.1550              0.1544              0.1538              0.1534 \n              blida          abdelkader \n             0.1533              0.1533 \n\nData Dimension:\n[1]  8063 59711\n```\n\n\n:::\n:::\n\n\n\n\n\n\nPolarity scores in Latent Semantic Scaling (LSS) quantify how words or documents relate to a specific dimension (e.g., sentiment) based on predefined seed words. Seed words represent the extremes of this dimension (e.g., \"good\" vs. \"bad\"). LSS analyzes how other words co-occur with these seed words to assign a score.\n\nWe can visualize the polarity of words using `textplot_terms()`. If you pass a dictionary to be highlighted, words are plotted in different colors. `data_dictionary_LSD2015` is a widely-used sentiment dictionary. If `highlighted = NULL`, words are selected randomly to highlight.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntextplot_terms(lss, highlighted = data_dictionary_LSD2015[1:2])\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: ggrepel: 8 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](3-quanteda_files/figure-pdf/unnamed-chunk-63-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nBased on the fitted model, we can now predict polarity scores of documents using `predict()`. It is best to work with the document-level data frame, which we will then add a new column for the predicted polarity scores.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- docvars(lss$data)\ndat$lss <- predict(lss)\nglimpse(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 8,063\nColumns: 5\n$ head <chr> \"Biden: US Desires Diplomacy But 'Ready No Matter What Happens' I~\n$ url  <chr> \"https://sputniknews.com/20220131/biden-us-desires-diplomacy-but-~\n$ time <dttm> 2022-02-01 03:25:22, 2022-02-01 01:58:19, 2022-02-01 01:47:56, 2~\n$ date <date> 2022-01-31, 2022-01-31, 2022-01-31, 2022-01-31, 2022-01-31, 2022~\n$ lss  <dbl> 0.10093989, 0.99263241, -0.76609160, 0.15991318, 0.25420329, -1.5~\n```\n\n\n:::\n:::\n\n\n\n\n\n\nBasically what we have is a data frame, where each row represents a single document (here, a news item from Sputnik with a timestamp). Each document also has a predicted polarity score based on the LSS model. \nWe can visualise this easily using `ggplot()`.\nBut first, we need to smooth the scores using `smooth_lss()` (otherwise it is too rough to interpret).\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_smooth <- smooth_lss(dat, lss_var = \"lss\", date_var = \"date\")\n\nggplot(dat_smooth, aes(x = date, y = fit)) + \n  geom_line() +\n  geom_ribbon(\n    aes(ymin = fit - se.fit * 1.96, \n        ymax = fit + se.fit * 1.96), \n    alpha = 0.1\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-02-24\"), linetype = \"dotted\") +\n  scale_x_date(date_breaks = \"months\", date_labels = \"%b %Y\", name = NULL) +\n  labs(title = \"Sentiment about Ukraine\", x = \"Date\", y = \"Sentiment\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](3-quanteda_files/figure-pdf/unnamed-chunk-65-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nThe plot shows that the sentiment of the articles about Ukraine became more negative in March but more positive in April. Zero on the Y-axis is the overall mean of the score; the dotted vertical line indicate the beginning of the war.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}